This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix. The content has been processed where empty lines have been removed.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: .specstory/**/*.md, .venv/**, _private/**, CLEANUP.txt, **/*.json, *.lock
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Empty lines have been removed from all files

Additional Info:
----------------

================================================================
Directory Structure
================================================================
.cursor/
  rules/
    0project.mdc
    cleanup.mdc
    filetree.mdc
.github/
  workflows/
    push.yml
    release.yml
src/
  twat_search/
    web/
      engines/
        __init__.py
        base.py
        bing_scraper.py
        brave.py
        critique.py
        duckduckgo.py
        google_scraper.py
        hasdata.py
        pplx.py
        searchit.py
        serpapi.py
        tavily.py
        you.py
      __init__.py
      api.py
      cli.py
      config.py
      engine_constants.py
      exceptions.py
      models.py
      utils.py
    __init__.py
    __main__.py
tests/
  unit/
    web/
      engines/
        __init__.py
        test_base.py
      __init__.py
      test_api.py
      test_config.py
      test_exceptions.py
      test_models.py
      test_utils.py
    __init__.py
    mock_engine.py
  web/
    test_bing_scraper.py
  conftest.py
  test_twat_search.py
.gitignore
.pre-commit-config.yaml
CHANGELOG.md
cleanup.py
LICENSE
NEXTENGINES.md
PROGRESS.md
pyproject.toml
README.md
TODO.md
VERSION.txt

================================================================
Files
================================================================

================
File: .cursor/rules/0project.mdc
================
---
description: About this project
globs: 
alwaysApply: false
---
# About this project

`twat-search` is a multi-provider search 

## Development Notes
- Uses `uv` for Python package management
- Quality tools: ruff, mypy, pytest
- Clear provider protocol for adding new search backends
- Strong typing and runtime checks throughout

================
File: .cursor/rules/cleanup.mdc
================
---
description: Run `cleanup.py` script before and after changes
globs: 
alwaysApply: false
---
Before you do any changes or if I say "cleanup", run the `cleanup.py update` script in the main folder. Analyze the results, describe recent changes in [PROGRESS.md](mdc:PROGRESS.md) and edit @TODO.md to update priorities and plan next changes. PERFORM THE CHANGES, then run the `cleanup.py status` script and react to the results.

When you edit @TODO.md, lead in lines with empty GFM checkboxes if things aren't done (`- [ ] `) vs. filled (`- [x] `) if done.

================
File: .cursor/rules/filetree.mdc
================
---
description: File tree of the project
globs: 
---
[1.0K]  .
â”œâ”€â”€ [  64]  .benchmarks
â”œâ”€â”€ [  96]  .cursor
â”‚Â Â  â””â”€â”€ [ 192]  rules
â”‚Â Â      â”œâ”€â”€ [ 334]  0project.mdc
â”‚Â Â      â”œâ”€â”€ [ 558]  cleanup.mdc
â”‚Â Â      â””â”€â”€ [5.7K]  filetree.mdc
â”œâ”€â”€ [  96]  .github
â”‚Â Â  â””â”€â”€ [ 128]  workflows
â”‚Â Â      â”œâ”€â”€ [2.7K]  push.yml
â”‚Â Â      â””â”€â”€ [1.4K]  release.yml
â”œâ”€â”€ [3.5K]  .gitignore
â”œâ”€â”€ [1.5K]  .pre-commit-config.yaml
â”œâ”€â”€ [ 128]  .specstory
â”‚Â Â  â”œâ”€â”€ [ 992]  history
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [2.0K]  .what-is-this.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [ 52K]  2025-02-25_01-58-creating-and-tracking-project-tasks.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [7.4K]  2025-02-25_02-17-project-task-continuation-and-progress-update.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [ 11K]  2025-02-25_02-24-planning-tests-for-twat-search-web-package.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [196K]  2025-02-25_02-27-implementing-tests-for-twat-search-package.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [ 46K]  2025-02-25_02-58-transforming-python-script-into-cli-tool.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [ 93K]  2025-02-25_03-09-generating-a-name-for-the-chat.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [5.5K]  2025-02-25_03-33-untitled.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [ 57K]  2025-02-25_03-54-integrating-search-engines-into-twat-search.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [ 72K]  2025-02-25_04-05-consolidating-you-py-and-youcom-py.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [6.1K]  2025-02-25_04-13-missing-env-api-key-names-in-pplx-py.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [118K]  2025-02-25_04-16-implementing-functions-for-brave-search-engines.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [286K]  2025-02-25_04-48-unifying-search-engine-parameters-in-twat-search.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [ 83K]  2025-02-25_05-36-implementing-duckduckgo-search-engine.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [194K]  2025-02-25_05-43-implementing-the-webscout-search-engine.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [ 23K]  2025-02-25_06-07-implementing-bing-scraper-engine.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [ 15K]  2025-02-25_06-12-continuing-bing-scraper-engine-implementation.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [121K]  2025-02-25_06-34-implementing-safe-import-patterns-in-modules.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [9.9K]  2025-02-25_07-09-refactoring-plan-and-progress-update.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [ 40K]  2025-02-25_07-17-implementing-phase-1-from-todo-md.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [292K]  2025-02-25_07-34-integrating-hasdata-google-serp-apis.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [142K]  2025-02-25_08-19-implementing-search-engines-from-nextengines-md.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [175K]  2025-02-26_09-54-implementing-plain-option-for-search-commands.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [264K]  2025-02-26_10-55-standardizing-engine-naming-conventions.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [4.3K]  2025-02-26_11-30-untitled.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [102K]  2025-02-26_12-11-update-config-py-to-use-engine-constants.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [278K]  2025-02-26_12-18-update-engine-imports-and-exports-in-init-py.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [268K]  2025-02-26_13-40-search-engine-initialization-errors.md
â”‚Â Â  â”‚Â Â  â””â”€â”€ [ 54K]  2025-02-26_14-15-untitled.md
â”‚Â Â  â””â”€â”€ [2.2M]  history.txt
â”œâ”€â”€ [3.2K]  CHANGELOG.md
â”œâ”€â”€ [ 499]  CLEANUP.txt
â”œâ”€â”€ [1.0K]  LICENSE
â”œâ”€â”€ [ 30K]  NEXTENGINES.md
â”œâ”€â”€ [1.2K]  PROGRESS.md
â”œâ”€â”€ [ 21K]  README.md
â”œâ”€â”€ [5.0K]  TODO.md
â”œâ”€â”€ [   7]  VERSION.txt
â”œâ”€â”€ [ 12K]  cleanup.py
â”œâ”€â”€ [ 192]  dist
â”œâ”€â”€ [1.5K]  fix_name_attribute.py
â”œâ”€â”€ [ 10K]  pyproject.toml
â”œâ”€â”€ [ 128]  src
â”‚Â Â  â””â”€â”€ [ 256]  twat_search
â”‚Â Â      â”œâ”€â”€ [ 577]  __init__.py
â”‚Â Â      â”œâ”€â”€ [2.4K]  __main__.py
â”‚Â Â      â””â”€â”€ [ 416]  web
â”‚Â Â          â”œâ”€â”€ [1.7K]  __init__.py
â”‚Â Â          â”œâ”€â”€ [5.8K]  api.py
â”‚Â Â          â”œâ”€â”€ [ 45K]  cli.py
â”‚Â Â          â”œâ”€â”€ [ 13K]  config.py
â”‚Â Â          â”œâ”€â”€ [2.6K]  engine_constants.py
â”‚Â Â          â”œâ”€â”€ [ 576]  engines
â”‚Â Â          â”‚Â Â  â”œâ”€â”€ [8.5K]  __init__.py
â”‚Â Â          â”‚Â Â  â”œâ”€â”€ [ 25K]  anywebsearch.py
â”‚Â Â          â”‚Â Â  â”œâ”€â”€ [4.4K]  base.py
â”‚Â Â          â”‚Â Â  â”œâ”€â”€ [ 11K]  bing_scraper.py
â”‚Â Â          â”‚Â Â  â”œâ”€â”€ [7.9K]  brave.py
â”‚Â Â          â”‚Â Â  â”œâ”€â”€ [8.5K]  critique.py
â”‚Â Â          â”‚Â Â  â”œâ”€â”€ [6.9K]  duckduckgo.py
â”‚Â Â          â”‚Â Â  â”œâ”€â”€ [ 12K]  google_scraper.py
â”‚Â Â          â”‚Â Â  â”œâ”€â”€ [7.5K]  hasdata.py
â”‚Â Â          â”‚Â Â  â”œâ”€â”€ [5.1K]  pplx.py
â”‚Â Â          â”‚Â Â  â”œâ”€â”€ [ 26K]  searchit.py
â”‚Â Â          â”‚Â Â  â”œâ”€â”€ [7.2K]  serpapi.py
â”‚Â Â          â”‚Â Â  â”œâ”€â”€ [7.6K]  tavily.py
â”‚Â Â          â”‚Â Â  â””â”€â”€ [7.5K]  you.py
â”‚Â Â          â”œâ”€â”€ [1.0K]  exceptions.py
â”‚Â Â          â”œâ”€â”€ [1.3K]  models.py
â”‚Â Â          â””â”€â”€ [1.5K]  utils.py
â”œâ”€â”€ [ 256]  tests
â”‚Â Â  â”œâ”€â”€ [  64]  .benchmarks
â”‚Â Â  â”œâ”€â”€ [2.0K]  conftest.py
â”‚Â Â  â”œâ”€â”€ [ 157]  test_twat_search.py
â”‚Â Â  â”œâ”€â”€ [ 192]  unit
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [  42]  __init__.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [1.5K]  mock_engine.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ [ 320]  web
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ [  46]  __init__.py
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ [ 160]  engines
â”‚Â Â  â”‚Â Â      â”‚Â Â  â”œâ”€â”€ [  37]  __init__.py
â”‚Â Â  â”‚Â Â      â”‚Â Â  â””â”€â”€ [4.3K]  test_base.py
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ [5.1K]  test_api.py
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ [2.7K]  test_config.py
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ [2.0K]  test_exceptions.py
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ [4.5K]  test_models.py
â”‚Â Â  â”‚Â Â      â””â”€â”€ [3.5K]  test_utils.py
â”‚Â Â  â””â”€â”€ [ 160]  web
â”‚Â Â      â””â”€â”€ [ 10K]  test_bing_scraper.py
â”œâ”€â”€ [158K]  twat_search.txt
â””â”€â”€ [267K]  uv.lock

19 directories, 87 files

================
File: .github/workflows/push.yml
================
name: Build & Test
on:
  push:
    branches: [main]
    tags-ignore: ["v*"]
  pull_request:
    branches: [main]
  workflow_dispatch:
permissions:
  contents: write
  id-token: write
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true
jobs:
  quality:
    name: Code Quality
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Run Ruff lint
        uses: astral-sh/ruff-action@v3
        with:
          version: "latest"
          args: "check --output-format=github"
      - name: Run Ruff Format
        uses: astral-sh/ruff-action@v3
        with:
          version: "latest"
          args: "format --check --respect-gitignore"
  test:
    name: Run Tests
    needs: quality
    strategy:
      matrix:
        python-version: ["3.10", "3.11", "3.12"]
        os: [ubuntu-latest]
      fail-fast: true
    runs-on: ${{ matrix.os }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: ${{ matrix.python-version }}
          enable-cache: true
          cache-suffix: ${{ matrix.os }}-${{ matrix.python-version }}
      - name: Install test dependencies
        run: |
          uv pip install --system --upgrade pip
          uv pip install --system ".[test]"
      - name: Run tests with Pytest
        run: uv run pytest -n auto --maxfail=1 --disable-warnings --cov-report=xml --cov-config=pyproject.toml --cov=src/twat_search --cov=tests tests/
      - name: Upload coverage report
        uses: actions/upload-artifact@v4
        with:
          name: coverage-${{ matrix.python-version }}-${{ matrix.os }}
          path: coverage.xml
  build:
    name: Build Distribution
    needs: test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: "3.12"
          enable-cache: true
      - name: Install build tools
        run: uv pip install build hatchling hatch-vcs
      - name: Build distributions
        run: uv run python -m build --outdir dist
      - name: Upload distribution artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dist-files
          path: dist/
          retention-days: 5

================
File: .github/workflows/release.yml
================
name: Release
on:
  push:
    tags: ["v*"]
permissions:
  contents: write
  id-token: write
jobs:
  release:
    name: Release to PyPI
    runs-on: ubuntu-latest
    environment:
      name: pypi
      url: https://pypi.org/p/twat-search
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: "3.12"
          enable-cache: true
      - name: Install build tools
        run: uv pip install build hatchling hatch-vcs
      - name: Build distributions
        run: uv run python -m build --outdir dist
      - name: Verify distribution files
        run: |
          ls -la dist/
          test -n "$(find dist -name '*.whl')" || (echo "Wheel file missing" && exit 1)
          test -n "$(find dist -name '*.tar.gz')" || (echo "Source distribution missing" && exit 1)
      - name: Publish to PyPI
        uses: pypa/gh-action-pypi-publish@release/v1
        with:
          password: ${{ secrets.PYPI_TOKEN }}
      - name: Create GitHub Release
        uses: softprops/action-gh-release@v1
        with:
          files: dist/*
          generate_release_notes: true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

================
File: src/twat_search/web/engines/__init__.py
================
    __all__.extend(
    __all__.extend(["SerpApiSearchEngine", "google_serpapi"])
    __all__.extend(["TavilySearchEngine", "tavily"])
    __all__.extend(["PerplexitySearchEngine", "pplx"])
    __all__.extend(["CritiqueSearchEngine", "critique"])
    __all__.extend(["DuckDuckGoSearchEngine", "duckduckgo"])
    __all__.extend(["BingScraperSearchEngine", "bing_scraper"])
    logger.warning(f"Failed to import bing_scraper: {e}")
    __all__.extend(["GoogleScraperEngine", "google_scraper"])
    logger.warning(f"Failed to import google_scraper module: {e}")
    logger.warning(f"Failed to import searchit module: {e}")
def is_engine_available(engine_name: str) -> bool:
    standardized_name = standardize_engine_name(engine_name)
def get_engine_function(
    return available_engine_functions.get(standardized_name)
def get_available_engines() -> list[str]:
    return list(available_engine_functions.keys())

================
File: src/twat_search/web/engines/base.py
================
logger = logging.getLogger(__name__)
class SearchEngine(abc.ABC):
    def __init__(self, config: EngineConfig, **kwargs: Any) -> None:
        self.num_results = kwargs.get("num_results", 5)
        self.country = kwargs.get("country", None)
        self.language = kwargs.get("language", None)
        self.safe_search = kwargs.get("safe_search", True)
        self.time_frame = kwargs.get("time_frame", None)
        self.timeout = kwargs.get("timeout", 10)
        self.retries = kwargs.get("retries", 2)
        self.retry_delay = kwargs.get("retry_delay", 1.0)
        self.use_random_user_agent = kwargs.get("use_random_user_agent", True)
            raise EngineError(self.engine_code, msg)
                    f"Please set it via one of these env vars: {', '.join(self.env_api_key_names)}"
    def _get_num_results(self, param_name: str = "num_results", min_value: int = 1) -> int:
        value = self.kwargs.get(param_name)
                return max(min_value, int(value))
                logger.warning(f"Invalid value for '{param_name}' ({value!r}) in {self.engine_code}, using default.")
        default = self.config.default_params.get(param_name) or self.config.default_params.get("num_results")
                return max(min_value, int(default))
                logger.warning(
    def max_results(self) -> int:
        return self._get_num_results(param_name="num_results", min_value=1)
    def limit_results(self, results: list[SearchResult]) -> list[SearchResult]:
        if max_results > 0 and len(results) > max_results:
    async def make_http_request(
        if self.use_random_user_agent and "user-agent" not in {k.lower() for k in headers}:
            headers["User-Agent"] = random.choice(USER_AGENTS)
        for attempt in range(1, actual_retries + 2):  # +2 because we want actual_retries+1 attempts
                async with httpx.AsyncClient(timeout=actual_timeout) as client:
                    response = await client.request(
                    response.raise_for_status()
                jitter = random.uniform(0.5, 1.5)
                await asyncio.sleep(actual_delay)
            last_error = httpx.RequestError("Unknown error occurred during HTTP request")
        raise EngineError(self.engine_code, msg) from last_error
    async def search(self, query: str) -> list[SearchResult]:
    def _get_api_key(self) -> str:
            raise EngineError(
                f"API key is required. Set it via one of these env vars: {', '.join(self.env_api_key_names)}",
def register_engine(engine_class: type[SearchEngine]) -> type[SearchEngine]:
        if not hasattr(engine_class, "engine_code") or not engine_class.engine_code:
            raise AttributeError(error_msg)
        if not hasattr(engine_class, "env_api_key_names"):
                engine_class.friendly_engine_name = ENGINE_FRIENDLY_NAMES.get(
        logger.error(f"Failed to register engine {engine_class.__name__}: {e}")
def get_engine(engine_name: str, config: EngineConfig, **kwargs: Any) -> SearchEngine:
    engine_class = _engine_registry.get(engine_name)
        available_engines = ", ".join(sorted(_engine_registry.keys()))
        raise EngineError(engine_name, msg)
        return engine_class(config, **kwargs)
        raise EngineError(engine_name, msg) from e
def get_registered_engines() -> dict[str, type[SearchEngine]]:
    return _engine_registry.copy()

================
File: src/twat_search/web/engines/bing_scraper.py
================
logger = logging.getLogger(__name__)
    class BingScraper:  # type: ignore
        def __init__(
        def search(self, query: str, num_results: int = 10) -> list[Any]:
class BingScraperResult(BaseModel):
class BingScraperSearchEngine(SearchEngine):
        super().__init__(config, **kwargs)
        self.max_results = num_results if num_results is not None else self.config.default_params.get("max_results", 5)
        self.max_retries: int = kwargs.get(
        ) or self.config.default_params.get("max_retries", 3)
        self.delay_between_requests: float = kwargs.get(
        ) or self.config.default_params.get("delay_between_requests", 1.0)
            unused_params.append(f"country='{country}'")
            unused_params.append(f"language='{language}'")
            unused_params.append(f"safe_search={safe_search}")
            unused_params.append(f"time_frame='{time_frame}'")
            logger.debug(
                f"Parameters {', '.join(unused_params)} set but not used by Bing Scraper",
    def _convert_result(self, result: Any) -> SearchResult | None:
            logger.warning("Empty result received from Bing Scraper")
        if not hasattr(result, "title") or not hasattr(result, "url"):
            logger.warning(f"Invalid result format: {result}")
            validated = BingScraperResult(
                description=result.description if hasattr(result, "description") else None,
            return SearchResult(
                    "url": str(result.url),
                    "description": result.description if hasattr(result, "description") else None,
            logger.warning(f"Validation error for result: {exc}")
            logger.warning(f"Unexpected error converting result: {exc}")
    async def search(self, query: str) -> list[SearchResult]:
            raise EngineError(self.engine_code, "Search query cannot be empty")
        logger.info(f"Searching Bing with query: '{query}'")
            scraper = BingScraper(
            raw_results = scraper.search(query, num_results=self.max_results)
                logger.info("No results returned from Bing Scraper")
                f"Received {len(raw_results)} raw results from Bing Scraper",
                search_result = self._convert_result(result)
                    results.append(search_result)
                    if len(results) >= self.max_results:
            logger.info(
                f"Returning {len(results)} validated results from Bing Scraper",
            logger.error(error_msg)
            raise EngineError(self.engine_code, error_msg) from exc
async def bing_scraper(
    config = EngineConfig(enabled=True)
    engine = BingScraperSearchEngine(
    return await engine.search(query)

================
File: src/twat_search/web/engines/brave.py
================
class BraveResult(BaseModel):
class BraveNewsResult(BaseModel):
class BaseBraveEngine(SearchEngine):
    def __init__(
        super().__init__(config, **kwargs)
        self.num_results = self._get_num_results(param_name="num_results", min_value=1)
        self.country = country or kwargs.get("country") or self.config.default_params.get("country", None)
        search_lang = kwargs.get("search_lang", language)
        self.search_lang = search_lang or self.config.default_params.get(
        ui_lang = kwargs.get("ui_lang", language)
        self.ui_lang = ui_lang or self.config.default_params.get(
        safe = kwargs.get("safe_search", safe_search)
        if isinstance(safe, bool):
        self.safe_search = safe or self.config.default_params.get(
        freshness = kwargs.get("freshness", time_frame)
        self.freshness = freshness or self.config.default_params.get(
            raise EngineError(
                f"Brave API key is required. Set it via one of these env vars: {', '.join(self.env_api_key_names)}",
    async def search(self, query: str) -> list[SearchResult]:
        async with httpx.AsyncClient() as client:
                response = await client.get(
                response.raise_for_status()
                data = response.json()
                section = data.get(self.response_key, {})
                if section.get("results"):
                            parsed = self.result_model(**result)
                            results.append(self.convert_result(parsed, result))
    def convert_result(self, parsed: BaseModel, raw: dict[str, Any]) -> SearchResult:
            publisher = getattr(parsed, "publisher", None)
            published_time = getattr(parsed, "published_time", None)
        return SearchResult(
    async def _make_api_call(self, query: str) -> dict[str, Any]:
            raise EngineError(self.engine_code, "Search query cannot be empty")
        if hasattr(self, "freshness") and self.freshness:
        response = await self.make_http_request(
class BraveSearchEngine(BaseBraveEngine):
        data = await self._make_api_call(query)
            web_result = data.get("web", {})
            items = web_result.get("results", [])
            for idx, item in enumerate(items, start=1):
                title = item.get("title", "")
                url = item.get("url", "")
                description = item.get("description", "")
                results.append(
                    SearchResult(
            return self.limit_results(results)
class BraveNewsSearchEngine(BaseBraveEngine):
            news_results = data.get("news", {})
            items = news_results.get("results", [])
                if item.get("age") or item.get("source", {}).get("name"):
                    source_name = item.get("source", {}).get("name", "")
                    age = item.get("age", "")
async def brave(
    config = EngineConfig(api_key=api_key, enabled=True)
    engine = BraveSearchEngine(
    return await engine.search(query)
async def brave_news(
    engine = BraveNewsSearchEngine(

================
File: src/twat_search/web/engines/critique.py
================
class CritiqueResult(BaseModel):
    url: str = Field(default="")  # URL of the result source
    title: str = Field(default="")  # Title of the result
    summary: str = Field(default="")  # Summary or snippet from the result
    source: str = Field(default="")  # Source of the result
class CritiqueResponse(BaseModel):
    results: list[CritiqueResult] = Field(default_factory=list)
class CritiqueSearchEngine(SearchEngine):
    def __init__(
        super().__init__(config)
        self.image_url = image_url or kwargs.get("image_url")
        self.image_base64 = image_base64 or kwargs.get("image_base64")
        self.source_whitelist = source_whitelist or kwargs.get(
        self.source_blacklist = source_blacklist or kwargs.get(
        self.output_format = output_format or kwargs.get("output_format")
            raise EngineError(
                f"Critique Labs API key is required. Set it via one of these env vars: {', '.join(self.env_api_key_names)}",
    async def _convert_image_url_to_base64(self, image_url: str) -> str:
            async with httpx.AsyncClient() as client:
                response = await client.get(image_url, timeout=30)
                response.raise_for_status()
                encoded = base64.b64encode(response.content).decode("utf-8")
            raise EngineError(self.engine_code, f"Error processing image: {e}")
    async def _build_payload(self, query: str) -> dict[str, Any]:
            payload["image"] = await self._convert_image_url_to_base64(self.image_url)
    def _build_result(self, item: CritiqueResult, rank: int) -> SearchResult:
                HttpUrl(item.url)
                else HttpUrl(
            url_obj = HttpUrl("https://critique-labs.ai")
        return SearchResult(
            raw=item.model_dump(),
    def _parse_results(self, data: dict[str, Any]) -> list[SearchResult]:
        critique_data = CritiqueResponse(
            results=data.get("results", []),
            response=data.get("response"),
            structured_output=data.get("structured_output"),
            results.append(
                SearchResult(
                    url=HttpUrl("https://critique-labs.ai"),
        for idx, item in enumerate(critique_data.results, 1):
                results.append(self._build_result(item, idx))
    async def search(self, query: str) -> list[SearchResult]:
        payload = await self._build_payload(query)
                response = await client.post(
                data = response.json()
                return self._parse_results(data)
async def critique(
    config = EngineConfig(api_key=api_key, enabled=True)
    engine = CritiqueSearchEngine(
    return await engine.search(query)

================
File: src/twat_search/web/engines/duckduckgo.py
================
logger = logging.getLogger(__name__)
class DuckDuckGoResult(BaseModel):
class DuckDuckGoSearchEngine(SearchEngine):
    def __init__(
        super().__init__(config, **kwargs)
        ) = self._map_init_params(
            logger.debug(
    def _map_init_params(
        max_results = kwargs.get(
        ) or config.default_params.get("max_results", 10)
        region = kwargs.get("region", country) or config.default_params.get(
        lang = language or config.default_params.get("language", None)
        timelimit = kwargs.get("timelimit", time_frame) or config.default_params.get(
        if timelimit and not kwargs.get("timelimit"):
            timelimit = time_mapping.get(timelimit.lower(), timelimit)
        safesearch = kwargs.get("safesearch", safe_search)
        if isinstance(safesearch, str):
                if safesearch.lower()
        proxy = kwargs.get("proxy") or config.default_params.get("proxy", None)
        timeout = kwargs.get(
        ) or config.default_params.get("timeout", 10)
    def _convert_result(self, raw: dict[str, Any]) -> SearchResult | None:
            ddg_result = DuckDuckGoResult(
            return SearchResult(
            logger.warning(f"Validation error for result: {exc}")
    async def search(self, query: str) -> list[SearchResult]:
            ddgs = DDGS(proxy=self.proxy, timeout=self.timeout)
            raw_results = ddgs.text(
                converted = self._convert_result(raw)
                    results.append(converted)
                    if len(results) >= self.max_results:
            raise EngineError(
async def duckduckgo(
    config = EngineConfig(enabled=True)
    engine = DuckDuckGoSearchEngine(
    return await engine.search(query)

================
File: src/twat_search/web/engines/google_scraper.py
================
    class GoogleSearchResult:  # type: ignore
        def __init__(self, url: str, title: str, description: str):
    def google_search(*args, **kwargs):  # type: ignore
logger = logging.getLogger(__name__)
class GoogleScraperResult(BaseModel):
    @field_validator("title", "description")
    def validate_non_empty(cls, v: str) -> str:
class GoogleScraperEngine(SearchEngine):
    def __init__(
        super().__init__(config, **kwargs)
        self.max_results: int = num_results or self.config.default_params.get(
        self.language: str = language or self.config.default_params.get(
        self.region: str | None = country or self.config.default_params.get(
            else self.config.default_params.get("safe", "active")
        self.sleep_interval: float = kwargs.get(
        ) or self.config.default_params.get("sleep_interval", 0.0)
        self.ssl_verify: bool | None = kwargs.get(
        ) or self.config.default_params.get("ssl_verify", None)
        self.proxy: str | None = kwargs.get("proxy") or self.config.default_params.get(
        self.unique: bool = kwargs.get("unique") or self.config.default_params.get(
            unused_params.append(f"time_frame='{time_frame}'")
            logger.debug(
                f"Parameters {', '.join(unused_params)} set but not used by Google Scraper",
    def _convert_result(self, result: GoogleSearchResult) -> SearchResult | None:
            logger.warning("Empty result received from Google Scraper")
            validated = GoogleScraperResult(
                url=HttpUrl(result.url),
                description=result.description if hasattr(result, "description") else "",
            return SearchResult(
                    "url": str(result.url),
                    "description": result.description if hasattr(result, "description") else "",
            logger.warning(f"Validation error for result: {exc}")
            logger.warning(f"Unexpected error converting result: {exc}")
    async def search(self, query: str) -> list[SearchResult]:
            raise EngineError(self.engine_code, "Search query cannot be empty")
        logger.info(f"Searching Google with query: '{query}'")
            raw_results = list(
                google_search(
                logger.info("No results returned from Google Scraper")
                f"Received {len(raw_results)} raw results from Google Scraper",
            logger.error(error_msg)
            raise EngineError(self.engine_code, error_msg) from exc
            for search_result in (self._convert_result(cast(GoogleSearchResult, result)) for result in raw_results)
        logger.info(
            f"Returning {len(results)} validated results from Google Scraper",
async def google_scraper(
    return await search(

================
File: src/twat_search/web/engines/hasdata.py
================
class HasDataGoogleResult(BaseModel):
    def from_api_result(cls, result: dict[str, Any]) -> HasDataGoogleResult:
        return cls(
            title=result.get("title", ""),
            url=HttpUrl(result.get("link", "")),
            snippet=result.get("snippet", ""),
class HasDataBaseEngine(SearchEngine):
    def __init__(
        super().__init__(config)
        self.location = location or kwargs.get("location") or self.config.default_params.get("location")
            device_type or kwargs.get("device_type") or self.config.default_params.get("device_type", "desktop")
            raise EngineError(
                f"HasData API key is required. Set it via one of these env vars: {', '.join(self.env_api_key_names)}",
    async def search(self, query: str) -> list[SearchResult]:
        async with httpx.AsyncClient() as client:
                response = await client.get(
                response.raise_for_status()
                data = response.json()
                organic_results = data.get("organicResults", [])
                for i, result in enumerate(organic_results):
                        parsed = HasDataGoogleResult.from_api_result(result)
                        results.append(
                            SearchResult(
class HasDataGoogleEngine(HasDataBaseEngine):
class HasDataGoogleLightEngine(HasDataBaseEngine):
async def hasdata_google_full(
    config = EngineConfig(api_key=api_key, enabled=True)
    engine = HasDataGoogleEngine(
    return await engine.search(query)
async def hasdata_google(
    engine = HasDataGoogleLightEngine(

================
File: src/twat_search/web/engines/pplx.py
================
logger = logging.getLogger(__name__)
class PerplexityResult(BaseModel):
    answer: str = Field(default="")
    url: str = Field(default="https://perplexity.ai")
    title: str = Field(default="Perplexity AI Response")  # Default title
class PerplexitySearchEngine(SearchEngine):
    def __init__(
        super().__init__(config, **kwargs)
        self.model = kwargs.get("model") or self.config.default_params.get("model", "pplx-70b-online")
        self.max_results = self._get_num_results()
    async def search(self, query: str) -> list[SearchResult]:
            raise EngineError(self.engine_code, "Search query cannot be empty")
            response = await self.make_http_request(
            data = response.json()
            raise EngineError(
                f"API request failed: {str(e)!s}",
                f"Unexpected error: {str(e)!s}",
        for choice in data.get("choices", []):
            answer = choice.get("message", {}).get("content", "")
                pr = PerplexityResult(answer=answer, url=url, title=title)
                url_obj = HttpUrl(pr.url)  # Validate URL format
                results.append(
                    SearchResult(
                if len(results) >= self.max_results:
                logger.warning(f"Invalid result from Perplexity: {e}")
async def pplx(
    config = EngineConfig(
    engine = PerplexitySearchEngine(
    return await engine.search(query)

================
File: src/twat_search/web/engines/searchit.py
================
    class SearchitResult:  # type: ignore
        def __init__(self, rank: int, url: str, title: str, description: str):
    class ScrapeRequest:  # type: ignore
        def __init__(
    class GoogleScraper:  # type: ignore
        def __init__(self, max_results_per_page: int = 100):
        async def scrape(request: ScrapeRequest) -> list[SearchitResult]:
    class YandexScraper:  # type: ignore
        def __init__(self, max_results_per_page: int = 10):
    class QwantScraper:  # type: ignore
    class BingScraper:  # type: ignore
logger = logging.getLogger(__name__)
class SearchitScraperResult(BaseModel):
    @field_validator("title", "description")
    def validate_non_empty(cls, v: str) -> str:
class SearchitEngine(SearchEngine):
        super().__init__(config, **kwargs)
        self.max_results: int = num_results or self.config.default_params.get(
        self.language: str = language or self.config.default_params.get(
        self.domain: str | None = country or self.config.default_params.get(
        self.geo: str | None = country or self.config.default_params.get(
        self.sleep_interval: int = kwargs.get(
        ) or self.config.default_params.get("sleep_interval", 0)
        self.proxy: str | None = kwargs.get("proxy") or self.config.default_params.get(
            unused_params.append(f"safe_search={safe_search}")
            unused_params.append(f"time_frame='{time_frame}'")
            logger.debug(
                f"Parameters {', '.join(unused_params)} set but not used by {self.engine_code}",
    def _convert_result(self, result: SearchitResult) -> SearchResult | None:
            logger.warning(f"Empty result received from {self.engine_code}")
            validated = SearchitScraperResult(
                url=HttpUrl(result.url),
                description=result.description if hasattr(result, "description") else "",
            return SearchResult(
                    "url": str(result.url),
                    "description": result.description if hasattr(result, "description") else "",
            logger.warning(f"Validation error for result: {exc}")
            logger.warning(f"Unexpected error converting result: {exc}")
    async def _run_scraper(
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(
                lambda: asyncio.run(scraper.scrape(request)),
            logger.error(f"Error running searchit scraper: {exc}")
            raise EngineError(
class GoogleSearchitEngine(SearchitEngine):
    async def search(self, query: str) -> list[SearchResult]:
            raise EngineError(self.engine_code, "Search query cannot be empty")
        logger.info(f"Searching Google (searchit) with query: '{query}'")
            request = ScrapeRequest(
            scraper = GoogleScraper(
                max_results_per_page=min(100, self.max_results),
            raw_results = await self._run_scraper(scraper, request)
                logger.info("No results returned from Google (searchit)")
                f"Received {len(raw_results)} raw results from Google (searchit)",
                for search_result in (self._convert_result(result) for result in raw_results)
            logger.info(
                f"Returning {len(results)} validated results from Google (searchit)",
            logger.error(error_msg)
            raise EngineError(self.engine_code, error_msg) from exc
class YandexSearchitEngine(SearchitEngine):
        logger.info(f"Searching Yandex (searchit) with query: '{query}'")
            scraper = YandexScraper(
                max_results_per_page=min(10, self.max_results),
                logger.info("No results returned from Yandex (searchit)")
                f"Received {len(raw_results)} raw results from Yandex (searchit)",
                f"Returning {len(results)} validated results from Yandex (searchit)",
class QwantSearchitEngine(SearchitEngine):
        logger.info(f"Searching Qwant (searchit) with query: '{query}'")
            scraper = QwantScraper(
                logger.info("No results returned from Qwant (searchit)")
                f"Received {len(raw_results)} raw results from Qwant (searchit)",
                f"Returning {len(results)} validated results from Qwant (searchit)",
class BingSearchitEngine(SearchitEngine):
        logger.info(f"Searching Bing (searchit) with query: '{query}'")
            scraper = BingScraper(
                max_results_per_page=min(30, self.max_results),
                logger.info("No results returned from Bing (searchit)")
                f"Received {len(raw_results)} raw results from Bing (searchit)",
                f"Returning {len(results)} validated results from Bing (searchit)",
async def google_searchit(
    return await search(
async def yandex_searchit(
async def qwant_searchit(
async def bing_searchit(

================
File: src/twat_search/web/engines/serpapi.py
================
class SerpApiResult(BaseModel):
class SerpApiResponse(BaseModel):
class SerpApiSearchEngine(SearchEngine):
    def __init__(
        super().__init__(config)
            "num": kwargs.get("num", num_results) or self.config.default_params.get("num", 10),
            "google_domain": kwargs.get("google_domain")
            or self.config.default_params.get("google_domain", "google.com"),
            "gl": kwargs.get("gl", country) or self.config.default_params.get("gl"),
            "hl": kwargs.get("hl", language) or self.config.default_params.get("hl"),
            "safe": _convert_safe(kwargs.get("safe", safe_search)) or self.config.default_params.get("safe"),
            "time_period": kwargs.get("time_period", time_frame) or self.config.default_params.get("time_period"),
            raise EngineError(
                f"SerpApi API key is required. Set it via one of these env vars: {', '.join(self.env_api_key_names)}",
    async def search(self, query: str) -> list[SearchResult]:
        params.update({k: v for k, v in self._params.items() if v is not None})
        async with httpx.AsyncClient() as client:
                response = await client.get(
                response.raise_for_status()
                data = response.json()
                serpapi_response = SerpApiResponse(**data)
                        results.append(
                            SearchResult(
                                raw=result.model_dump(),  # Include raw result for debugging
                return self.limit_results(results)
def _convert_safe(safe: bool | str | None) -> str | None:
    if isinstance(safe, bool):
async def google_serpapi(
    config = EngineConfig(
    engine = SerpApiSearchEngine(
    return await engine.search(query)

================
File: src/twat_search/web/engines/tavily.py
================
class TavilySearchResult(BaseModel):
class TavilySearchResponse(BaseModel):
class TavilySearchEngine(SearchEngine):
    def __init__(
        super().__init__(config, **kwargs)
            self.max_results = kwargs.get("max_results") or self.config.default_params.get("max_results", 5)
        self.search_depth = search_depth or self.config.default_params.get("search_depth", "basic")
        self.include_domains = include_domains or self.config.default_params.get("include_domains", None)
        self.exclude_domains = exclude_domains or self.config.default_params.get("exclude_domains", None)
        self.include_answer = include_answer or self.config.default_params.get("include_answer", False)
        self.max_tokens = max_tokens or self.config.default_params.get("max_tokens", None)
        self.search_type = search_type or self.config.default_params.get("search_type", "search")
            raise EngineError(
                f"Tavily API key is required. Set it via one of these env vars: {', '.join(self.env_api_key_names)}",
    def _build_payload(self, query: str) -> dict:
    def _convert_result(self, item: dict, rank: int) -> SearchResult | None:
            validated_url = HttpUrl(item.get("url", ""))
            return SearchResult(
                title=item.get("title", ""),
                snippet=textwrap.shorten(
                    item.get("content", "").strip(),
    async def search(self, query: str) -> list[SearchResult]:
        payload = self._build_payload(query)
        async with httpx.AsyncClient() as client:
                response = await client.post(
                response.raise_for_status()
                data = response.json()
                raise EngineError(self.engine_code, f"HTTP error: {e}")
                raise EngineError(self.engine_code, f"Request error: {e}")
                raise EngineError(self.engine_code, f"Error: {e!s}")
            parsed_response = TavilySearchResponse.model_validate(data)
            items = [item.model_dump() for item in parsed_response.results]
            items = data.get("results", [])
        for idx, item in enumerate(items, start=1):
            converted = self._convert_result(item, idx)
                results.append(converted)
async def tavily(
    config = EngineConfig(
    engine = TavilySearchEngine(
    return await engine.search(query)

================
File: src/twat_search/web/engines/you.py
================
logger = logging.getLogger(__name__)
class YouSearchHit(BaseModel):
    snippet: str = Field(alias="description")
class YouSearchResponse(BaseModel):
    search_id: str | None = Field(None, alias="searchId")
class YouNewsArticle(BaseModel):
class YouNewsResponse(BaseModel):
class YouBaseEngine(SearchEngine):
    def __init__(
        super().__init__(config, **kwargs)
        self.max_results = self._get_num_results(self.num_results_param)
        self.country_code = kwargs.get("country") or self.config.default_params.get(
    async def _make_api_call(self, query: str) -> Any:
            params["safe_search"] = str(self.safe_search).lower()
        logger.debug(f"Making You.com API request to {self.base_url} with params: {params}")
            response = await self.make_http_request(
            return response.json()
            raise EngineError(self.engine_code, f"API request failed: {str(e)!s}")
            raise EngineError(self.engine_code, f"Unexpected error: {str(e)!s}")
class YouSearchEngine(YouBaseEngine):
    async def search(self, query: str) -> list[SearchResult]:
            raise EngineError(self.engine_code, "Search query cannot be empty")
        data = await self._make_api_call(query)
            you_response = YouSearchResponse(**data)
                    results.append(
                        SearchResult(
                            raw=hit.model_dump(by_alias=True),
                    if len(results) >= self.max_results:
                    logger.warning(f"Invalid result from You.com: {e}")
            raise EngineError(
class YouNewsSearchEngine(YouBaseEngine):
            you_response = YouNewsResponse(**data)
                            raw=article.model_dump(by_alias=True),
                    logger.warning(f"Invalid news result from You.com: {e}")
async def you(
    config = EngineConfig(
    engine = YouSearchEngine(
    return await engine.search(query)
async def you_news(
    engine = YouNewsSearchEngine(

================
File: src/twat_search/web/__init__.py
================
    __all__.extend(["Config", "EngineConfig", "SearchResult", "search"])
    __all__.extend(["brave", "brave_news"])
    __all__.extend(["pplx"])
    __all__.extend(["serpapi"])
    __all__.extend(["tavily"])
    __all__.extend(["you", "you_news"])
    __all__.extend(["critique"])
    __all__.extend(["duckduckgo"])
    __all__.extend(["bing_scraper"])

================
File: src/twat_search/web/api.py
================
logger = logging.getLogger(__name__)
def get_engine_params(
    std_engine_name = standardize_engine_name(engine_name)
    for k, v in kwargs.items():
        if k.startswith(std_engine_name + "_"):
            engine_specific[k[len(std_engine_name) + 1 :]] = v
        elif k.startswith(engine_name + "_"):  # For backward compatibility
            engine_specific[k[len(engine_name) + 1 :]] = v
    std_engines = [standardize_engine_name(e) for e in engines]
    non_prefixed = {k: v for k, v in kwargs.items() if not any(k.startswith(e + "_") for e in std_engines + engines)}
def init_engine_task(
    engine_config = config.engines.get(std_engine_name)
        engine_config = config.engines.get(engine_name)
            logger.warning(f"Engine '{engine_name}' not configured.")
            engine_config = EngineConfig(enabled=True)
    num_results = kwargs.get("num_results")
        logger.debug(f"Initializing engine '{engine_name}' with num_results={num_results}")
        engine_params = get_engine_params(
            engines=kwargs.get("engines", []),
            common_params=kwargs.get("common_params", {}),
        engine_instance: SearchEngine = get_engine(
        logger.info(f"ðŸ” Querying engine: {engine_name}")
        return engine_name, engine_instance.search(query)
        logger.warning(
            f"Failed to initialize engine '{engine_name}': {type(e).__name__}",
        logger.error(f"Error initializing engine '{engine_name}': {e}")
async def search(
        config = config or Config()
        explicit_engines_requested = engines is not None and len(engines) > 0
        engines_to_try = engines or list(config.engines.keys())
            logger.error(msg)
            raise SearchError(msg)
        logger.debug(f"Search requested with num_results={num_results}")
        engines_to_try = [standardize_engine_name(e) for e in engines_to_try]
                task = init_engine_task(
                    engine_names.append(task[0])
                    tasks.append(task[1])
                logger.warning(f"Failed to initialize engine '{engine_name}': {e}")
                failed_engines.append(engine_name)
            failed_engines_str = ", ".join(failed_engines)
            logger.warning(f"Failed to initialize engines: {failed_engines_str}")
                msg = f"No search engines could be initialized from requested engines: {', '.join(engines_to_try)}"
            logger.warning("Falling back to any available search engine...")
            for engine_name in get_registered_engines():
                    logger.debug(f"Failed to initialize engine {engine_name}: {e}")
        results = await asyncio.gather(*tasks, return_exceptions=True)
        for engine_name, result in zip(engine_names, results, strict=False):
            if isinstance(result, Exception):
                logger.error(
            elif isinstance(result, list):
                logger.info(
                    f"âœ… Engine '{engine_name}' returned {len(result)} results",
                flattened_results.extend(result)
                    f"âš ï¸ Engine '{engine_name}' returned no results or unexpected type: {type(result)}",
        logger.error(f"Search failed: {e}")

================
File: src/twat_search/web/cli.py
================
class CustomJSONEncoder(json_lib.JSONEncoder):
    def default(self, o: Any) -> Any:
            return json_lib.JSONEncoder.default(self, o)
            return str(o)
console = Console()
class SearchCLI:
    def __init__(self) -> None:
        self.logger = logging.getLogger("twat_search.cli")
        self.log_handler = RichHandler(rich_tracebacks=True)
        self._configure_logging()
        self.console = Console()
        available_engines = get_available_engines()
            self.logger.warning(
                f"{', '.join(missing_engines)}. "
    def _configure_logging(self, verbose: bool = False) -> None:
        logging.basicConfig(
        self.logger.setLevel(level)
        logging.getLogger("twat_search.web.api").setLevel(level)
        logging.getLogger("twat_search.web.engines").setLevel(level)
        logging.getLogger("httpx").setLevel(level)
    def _parse_engines(self, engines_arg: Any) -> list[str] | None:
        if isinstance(engines_arg, str):
            if engines_arg.strip().lower() == "free":
                self.logger.info(f"Using 'free' engines: {', '.join(engines)}")
            elif engines_arg.strip().lower() == "best":
                self.logger.info(f"Using 'best' engines: {', '.join(engines)}")
            elif engines_arg.strip().lower() == "all":
                engines = get_available_engines()
                self.logger.info(
                    f"Using 'all' available engines: {', '.join(engines)}",
            engines = [e.strip() for e in engines_arg.split(",") if e.strip()]
            return [standardize_engine_name(e) for e in engines]
        if isinstance(engines_arg, list | tuple):
            engines = [str(e).strip() for e in engines_arg if str(e).strip()]
            f"Unexpected engines type: {type(engines_arg)}. Using all available engines.",
    async def _run_search(
                if engine == "all" or is_engine_available(engine):
                    available.append(engine)
            self.logger.debug(f"Attempting to search with engines: {engines}")
            results = await search(query=query, engines=engines, **kwargs)
            return _process_results(results)
            self.logger.error(f"Search failed: {e}")
            _display_errors([str(e)])
    async def _search_engine(
            engine_func = get_engine_function(engine)
                self.logger.warning(error_msg)
                _display_errors([error_msg])
            registered_engines = get_registered_engines()
            engine_class = registered_engines.get(engine)
            friendly = engine_class.friendly_engine_name if engine_class else ENGINE_FRIENDLY_NAMES.get(engine, engine)
            friendly = ENGINE_FRIENDLY_NAMES.get(engine, engine)
            self.console.print(f"[bold]Searching {friendly}[/bold]: {query}")
            results = await engine_func(query=query, **params)
            processed_results = _process_results(results)
                _display_json_results(processed_results)
                _display_results(processed_results, verbose, plain)
            self.logger.error(f"{friendly} search failed: {e}")
    def q(
        self._configure_logging(verbose)
        self.logger.debug(f"Using num_results={num_results}")
            self.logger.debug(
        engine_list = self._parse_engines(engines)
        common_params = {k: v for k, v in common_params.items() if v is not None}
            results = asyncio.run(
                self._run_search(
            with self.console.status(
            _display_json_results(results)
            _display_results(results, verbose, plain)
    def info(
            config = Config()
                self._display_engines_json(engine, config)
                self._display_engines_plain(engine, config)
                self._list_all_engines(config)
                self._show_engine_details(engine, config)
                self.logger.error(
    def _display_engines_plain(self, engine: str | None, config: Config) -> None:
                self.console.print(engine)
            for engine_name in sorted(config.engines.keys()):
                self.console.print(engine_name)
    def _list_all_engines(self, config: Config) -> None:
        table = Table(title="ðŸ”Ž Available Search Engines")
        table.add_column("Engine", style="cyan", no_wrap=True)
        table.add_column("Enabled", style="magenta")
        table.add_column("API Key Required", style="yellow")
        sorted_engines = sorted(config.engines.items(), key=lambda x: x[0])
                hasattr(
                if engine_class and hasattr(engine_class, "env_api_key_names"):
                    api_key_required = bool(engine_class.env_api_key_names)
            table.add_row(
        self.console.print(table)
        self.console.print(
    def _show_engine_details(self, engine_name: str, config: Config) -> None:
            self.console.print("\nAvailable engines:")
                self.console.print(f"- {name}")
        api_key_required = hasattr(engine_config, "api_key") and engine_config.api_key is not None
            engine_class = registered_engines.get(engine_name)
            if not api_key_required and engine_class and hasattr(engine_class, "env_api_key_names"):
                if engine_class and hasattr(engine_class, "friendly_name")
                else ENGINE_FRIENDLY_NAMES.get(engine_name, engine_name)
                    value_status = "âœ…" if os.environ.get(env_name) else "âŒ"
                    self.console.print(f"  {env_name}: {value_status}")
            self.console.print("\n[bold]Default Parameters:[/bold]")
                for param, value in engine_config.default_params.items():
                    self.console.print(f"  {param}: {value}")
                self.console.print("  No default parameters specified")
                base_engine = engine_name.split("-")[0]
                engine_module = importlib.import_module(module_name)
                function_name = engine_name.replace("-", "_")
                if hasattr(engine_module, function_name):
                    func = getattr(engine_module, function_name)
                    self.console.print("\n[bold]Function Interface:[/bold]")
                        f"  [green]{function_name}()[/green] - {func.__doc__.strip().split('\\n')[0]}",
                    self.console.print("\n[bold]Example Usage:[/bold]")
            self.console.print("\n[bold]Basic Configuration:[/bold]")
    def _display_engines_json(self, engine: str | None, config: Config) -> None:
            result[engine] = _get_engine_info(
            for engine_name, engine_config in sorted(config.engines.items()):
                result[engine_name] = _get_engine_info(
        print(json.dumps(result, indent=2))
    async def critique(
            params["source_whitelist"] = [domain.strip() for domain in source_whitelist.split(",")]
            params["source_blacklist"] = [domain.strip() for domain in source_blacklist.split(",")]
        params.update(kwargs)
        return await self._search_engine(
    async def brave(
        params = {k: v for k, v in params.items() if v is not None}
        return await self._search_engine("brave", query, params, json, verbose, plain)
    async def brave_news(
    async def serpapi(
        return await self._search_engine("serpapi", query, params, json, verbose, plain)
    async def tavily(
            params["include_domains"] = [s.strip() for s in include_domains.split(",") if s.strip()]
            params["exclude_domains"] = [s.strip() for s in exclude_domains.split(",") if s.strip()]
        return await self._search_engine("tavily", query, params, json, verbose, plain)
    async def pplx(
        return await self._search_engine("pplx", query, params, json, verbose, plain)
    async def you(
        return await self._search_engine("you", query, params, json, verbose, plain)
    async def you_news(
    async def duckduckgo(
    async def hasdata_google(
    async def hasdata_google_light(
def _check_engine_availability(engine_name: str) -> bool:
    return is_engine_available(engine_name)
def _get_engine_info(
    if hasattr(engine_config, "api_key") and engine_config.api_key is not None:
                {"name": env_name, "set": bool(os.environ.get(env_name))} for env_name in engine_class.env_api_key_names
        if hasattr(
        "enabled": engine_config.enabled if hasattr(engine_config, "enabled") else False,
def _process_results(results: list) -> list[dict[str, Any]]:
        engine_name = getattr(result, "source", None) or "unknown"
        engine_results.setdefault(engine_name, []).append(result)
    for engine, engine_results_list in engine_results.items():
            processed.append(
        for idx, result in enumerate(engine_results_list):
            url = str(result.url)
                    "snippet": result.snippet[:100] + "..." if len(result.snippet) > 100 else result.snippet,
                    "raw_result": getattr(result, "raw", None),
def _display_results(
        console.print("[bold red]No results found![/bold red]")
        urls = set()
                urls.add(result["url"])
        for url in sorted(urls):
            console.print(url)
    table = Table()  # Remove show_lines=True to eliminate row separator lines
        table.add_column("Status", style="magenta")
        table.add_column("Title", style="green")
        table.add_column("URL", style="blue", overflow="fold")
        table.add_column("URL", style="blue", overflow="fold", max_width=70)
            table.add_row(result["engine"], result["url"])
    console.print(table)
                console.print(result)
def _display_errors(error_messages: list[str]) -> None:
    table = Table(title="âŒ Search Errors")
    table.add_column("Error", style="red")
        table.add_row(error)
def _display_json_results(processed_results: list[dict[str, Any]]) -> None:
        results_by_engine[engine]["results"].append(
                "snippet": result.get("snippet") if result.get("snippet") != "N/A" else None,
                "raw": result.get("raw_result"),
    print(json.dumps(results_by_engine, indent=2))
def main() -> None:
    fire.core.Display = lambda lines, out: console.print(*lines)
    fire.Fire(SearchCLI)
    main()

================
File: src/twat_search/web/config.py
================
load_dotenv()
logger = logging.getLogger(__name__)
class EngineConfig(BaseModel):
    default_params: dict[str, Any] = Field(default_factory=dict)
    @field_validator("api_key")
    def validate_api_key(cls, v: str | None, info: Any) -> str | None:
        engine_code = info.data.get("engine_code")
                engine_class = get_registered_engines().get(engine_code)
                if engine_class and hasattr(engine_class, "env_api_key_names") and engine_class.env_api_key_names:
                        env_value = os.environ.get(env_var)
                    logger.warning(
                        f"Please set one of these environment variables: {', '.join(engine_class.env_api_key_names)}"
                logger.debug(f"Couldn't check API key requirements for {engine_code}: {e}")
class Config(BaseModel):
    engines: dict[str, EngineConfig] = Field(default_factory=dict)
    def get_config_path(cls) -> Path:
            return Path(cls.config_path)
        env_path = os.environ.get("TWAT_SEARCH_CONFIG_PATH")
            return Path(env_path)
        home_config = Path.home() / ".twat" / "search_config.json"
        if home_config.exists():
        xdg_config = Path.home() / ".config" / "twat" / "search_config.json"
        if xdg_config.exists():
    def __init__(self, **data: Any) -> None:
        config_data = json.loads(json.dumps(DEFAULT_CONFIG))  # Deep copy
        config_path = self.get_config_path()
        if config_path.exists():
                with open(config_path, encoding="utf-8") as f:
                    file_config = json.load(f)
                self._merge_config(config_data, file_config)
                logger.info(f"Loaded configuration from {config_path}")
                logger.error(f"Error loading configuration from {config_path}: {e}")
        _apply_env_overrides(config_data)
            self._merge_config(config_data, data)
        super().__init__(**config_data)
    def _merge_config(self, target: dict[str, Any], source: dict[str, Any]) -> None:
        for key, value in source.items():
            if key in target and isinstance(target[key], dict) and isinstance(value, dict):
                self._merge_config(target[key], value)
    def add_engine(
            engine_config.default_params.update(default_params)
            self.engines[engine_name] = EngineConfig(
def _apply_env_overrides(config_data: dict[str, Any]) -> None:
    for env_var, config_path in ENV_VAR_MAP.items():
            value = _parse_env_value(env_value)
            if isinstance(config_path, str):
                _set_nested_value(config_data, config_path, value)
                _set_nested_value(config_data, path, value)
def _parse_env_value(env_value: str) -> Any:
    if env_value.lower() in ("true", "yes", "1"):
    elif env_value.lower() in ("false", "no", "0"):
    elif env_value.isdigit():
        return int(env_value)
    elif env_value.replace(".", "", 1).isdigit():
        return float(env_value)
def _set_nested_value(config_data: dict[str, Any], path: list[str], value: Any) -> None:
    for i, path_part in enumerate(path):
        if i == len(path) - 1:
            if path_part not in current or not isinstance(current[path_part], dict):

================
File: src/twat_search/web/engine_constants.py
================
def standardize_engine_name(engine_name: str) -> str:
    return engine_name.replace("-", "_")

================
File: src/twat_search/web/exceptions.py
================
class SearchError(Exception):
    def __init__(self, message: str) -> None:
        super().__init__(message)
class EngineError(SearchError):
    def __init__(self, engine_name: str, message: str) -> None:
        super().__init__(f"Engine '{engine_name}': {message}")

================
File: src/twat_search/web/models.py
================
class SearchResult(BaseModel):
    @field_validator("title", "snippet", "source")
    def validate_non_empty(cls, v: str) -> str:
        if not v or not v.strip():
            raise ValueError(msg)
        return v.strip()

================
File: src/twat_search/web/utils.py
================
logger = logging.getLogger(__name__)
def load_environment(force_reload: bool = False) -> None:
    if not force_reload and os.environ.get(loaded_flag) == "1":
        logger.debug("Environment variables already loaded")
    load_dotenv()
    if logger.isEnabledFor(logging.DEBUG):  # Only in debug mode
        for key, value in os.environ.items():
                logger.debug(
        logger.info("Environment variables loaded")
class RateLimiter:
    def __init__(self, calls_per_second: float) -> None:
    async def wait(self) -> None:
        current_time = time.time()
            logger.debug(f"Rate limiter sleeping for {delay:.4f} seconds")
            time.sleep(delay)
        self.last_call_time = time.time()
    def wait_if_needed(self) -> None:
        if len(self.call_timestamps) >= self.calls_per_second:
            oldest_timestamp = min(self.call_timestamps)
        self.call_timestamps.append(time.time())
def extract_domain(url: str) -> str:
    parsed = urlparse(url)
        parsed = urlparse(f"https://{url}")
def extract_query_param(url: str, param: str) -> str | None:
    params = parse_qs(parsed.query)
    return params.get(param, [None])[0]

================
File: src/twat_search/__init__.py
================
    __all__.append("__version__")
    __all__.append("web")

================
File: src/twat_search/__main__.py
================
logging.basicConfig(
    handlers=[RichHandler(rich_tracebacks=True)],
logger = logging.getLogger(__name__)
console = Console()
SearchCLIType = TypeVar("SearchCLIType")
class TwatSearchCLI:
    def __init__(self) -> None:
            self.web: Any = web_cli.SearchCLI()
            logger.error(f"Web CLI not available: {e!s}")
            logger.error(
    def _cli_error(*args: Any, **kwargs: Any) -> int:
        console.print(
    def version() -> str:
def main() -> None:
    install(show_locals=True)
    ansi_decoder = AnsiDecoder()
    console = Console(theme=Theme({"prompt": "cyan", "question": "bold cyan"}))
    def display(lines, out):
        console.print(Group(*map(ansi_decoder.decode_line, lines)))
    fire.Fire(TwatSearchCLI, name="twat-search")
    main()

================
File: tests/unit/web/engines/__init__.py
================


================
File: tests/unit/web/engines/test_base.py
================
class TestSearchEngine(SearchEngine):
    async def search(self, query: str) -> list[SearchResult]:
            SearchResult(
                url=HttpUrl("https://example.com/test"),
register_engine(TestSearchEngine)
class DisabledTestSearchEngine(SearchEngine):
        raise NotImplementedError(msg)
register_engine(DisabledTestSearchEngine)
def test_search_engine_is_abstract() -> None:
    assert hasattr(SearchEngine, "__abstractmethods__")
    with pytest.raises(TypeError):
        SearchEngine(EngineConfig())  # type: ignore
def test_search_engine_name_class_var() -> None:
    assert hasattr(SearchEngine, "engine_code")
def test_engine_registration() -> None:
    class NewEngine(SearchEngine):
    returned_class = register_engine(NewEngine)
    engine_instance = get_engine("new_engine", EngineConfig())
    assert isinstance(engine_instance, NewEngine)
def test_get_engine_with_invalid_name() -> None:
    with pytest.raises(SearchError, match="Unknown search engine"):
        get_engine("nonexistent_engine", EngineConfig())
def test_get_engine_with_disabled_engine() -> None:
    config = EngineConfig(enabled=False)
    with pytest.raises(SearchError, match="is disabled"):
        get_engine("disabled_engine", config)
def test_get_engine_with_config() -> None:
    config = EngineConfig(
    engine = get_engine("test_engine", config)
def test_get_engine_with_kwargs() -> None:
    engine = get_engine("test_engine", EngineConfig(), **kwargs)

================
File: tests/unit/web/__init__.py
================


================
File: tests/unit/web/test_api.py
================
logging.basicConfig(level=logging.DEBUG)
T = TypeVar("T")
class MockSearchEngine(SearchEngine):
    def __init__(self, config: EngineConfig, **kwargs: Any) -> None:
        super().__init__(config, **kwargs)
        self.should_fail = kwargs.get("should_fail", False)
    async def search(self, query: str) -> list[SearchResult]:
            raise Exception(msg)
        result_count = self.kwargs.get("result_count", 1)
            SearchResult(
                url=HttpUrl(f"https://example.com/{i + 1}"),
            for i in range(result_count)
register_engine(MockSearchEngine)
def mock_config() -> Config:
    config = Config()
        "mock": EngineConfig(
async def setup_teardown() -> AsyncGenerator[None]:
    tasks = [t for t in asyncio.all_tasks() if t is not asyncio.current_task()]
    with contextlib.suppress(asyncio.CancelledError):
        await asyncio.gather(*tasks)
async def test_search_with_mock_engine(
    results = await search("test query", engines=["mock"], config=mock_config)
    assert len(results) == 2
    assert all(isinstance(result, SearchResult) for result in results)
    assert all(result.source == "mock" for result in results)
async def test_search_with_additional_params(
    results = await search(
    assert len(results) == 3
async def test_search_with_engine_specific_params(
    assert len(results) == 4
async def test_search_with_no_engines(setup_teardown: None) -> None:
    with pytest.raises(SearchError, match="No search engines configured"):
        await search("test query", engines=[])
async def test_search_with_failing_engine(
    assert len(results) == 0
async def test_search_with_nonexistent_engine(
    with pytest.raises(SearchError, match="No search engines could be initialized"):
        await search("test query", engines=["nonexistent"], config=mock_config)
async def test_search_with_disabled_engine(
        await search("test query", engines=["mock"], config=mock_config)

================
File: tests/unit/web/test_config.py
================
def test_engine_config_defaults() -> None:
    config = EngineConfig()
def test_engine_config_values() -> None:
    config = EngineConfig(
def test_config_defaults(isolate_env_vars: None) -> None:
    config = Config()
    assert isinstance(config.engines, dict)
    assert len(config.engines) == 0
def test_config_with_env_vars(monkeypatch: MonkeyPatch, env_vars_for_brave: None) -> None:
def test_config_with_direct_initialization() -> None:
    custom_config = Config(
        engines={"test_engine": EngineConfig(api_key="direct_key", enabled=True, default_params={"count": 5})},
def test_config_env_vars_override_direct_config(monkeypatch: MonkeyPatch) -> None:
    monkeypatch.setenv("BRAVE_API_KEY", "env_key")
        engines={"brave": EngineConfig(api_key="direct_key", enabled=True, default_params={"count": 5})},

================
File: tests/unit/web/test_exceptions.py
================
def test_search_error() -> None:
    exception = SearchError(error_message)
    assert str(exception) == error_message
    assert isinstance(exception, Exception)
def test_engine_error() -> None:
    exception = EngineError(engine_name, error_message)
    assert str(exception) == f"Engine '{engine_name}': {error_message}"
    assert isinstance(exception, SearchError)
def test_engine_error_inheritance() -> None:
        raise EngineError(msg, "Test error")
        if isinstance(e, EngineError):
def test_search_error_as_base_class() -> None:
        raise SearchError(msg)
        exceptions.append(e)
        raise EngineError(msg, "API key missing")
    assert len(exceptions) == 2
    assert isinstance(exceptions[0], SearchError)
    assert isinstance(exceptions[1], EngineError)
    assert "General search error" in str(exceptions[0])
    assert "Engine 'brave': API key missing" in str(exceptions[1])

================
File: tests/unit/web/test_models.py
================
def test_search_result_valid_data() -> None:
    url = HttpUrl("https://example.com")
    result = SearchResult(
    assert str(result.url) == "https://example.com/"
def test_search_result_with_optional_fields() -> None:
def test_search_result_invalid_url() -> None:
    with pytest.raises(ValidationError):
        SearchResult.model_validate(
def test_search_result_empty_fields() -> None:
                "url": str(url),
def test_search_result_serialization() -> None:
    result_dict = result.model_dump()
    assert str(result_dict["url"]) == "https://example.com/"
    result_json = result.model_dump_json()
    assert isinstance(result_json, str)
def test_search_result_deserialization() -> None:
    result = SearchResult.model_validate(data)

================
File: tests/unit/web/test_utils.py
================
def rate_limiter() -> RateLimiter:
    return RateLimiter(calls_per_second=5)
def test_rate_limiter_init() -> None:
    limiter = RateLimiter(calls_per_second=10)
def test_rate_limiter_wait_when_not_needed(rate_limiter: RateLimiter) -> None:
    with patch("time.sleep") as mock_sleep:
        rate_limiter.wait_if_needed()
        mock_sleep.assert_not_called()
        for _ in range(3):  # 4 total calls including the one above
def test_rate_limiter_wait_when_needed(rate_limiter: RateLimiter) -> None:
    now = time.time()
    rate_limiter.call_timestamps = [now - 0.01 * i for i in range(rate_limiter.calls_per_second)]
    with patch("time.sleep") as mock_sleep, patch("time.time", return_value=now):
        mock_sleep.assert_called_once()
def test_rate_limiter_cleans_old_timestamps(rate_limiter: RateLimiter) -> None:
    with patch("time.time", return_value=now):
    assert len(rate_limiter.call_timestamps) == len(recent_stamps) + 1  # +1 for the new call
@pytest.mark.parametrize("calls_per_second", [1, 5, 10, 100])
def test_rate_limiter_with_different_rates(calls_per_second: int) -> None:
    limiter = RateLimiter(calls_per_second=calls_per_second)
        for _ in range(calls_per_second):
            limiter.wait_if_needed()
        patch("time.sleep") as mock_sleep,
        patch("time.time", return_value=time.time()),

================
File: tests/unit/__init__.py
================


================
File: tests/unit/mock_engine.py
================
class MockSearchEngine(SearchEngine):
    def __init__(self, config: EngineConfig, **kwargs: Any) -> None:
        super().__init__(config, **kwargs)
        self.should_fail = kwargs.get("should_fail", False)
    async def search(self, query: str) -> list[SearchResult]:
            raise Exception(msg)
        result_count = self.kwargs.get("result_count", 1)
            SearchResult(
                url=HttpUrl(f"https://example.com/{i + 1}"),
            for i in range(result_count)
register_engine(MockSearchEngine)

================
File: tests/web/test_bing_scraper.py
================
class MockSearchResult:
    def __init__(self, title: str, url: str, description: str = "") -> None:
def engine_config() -> EngineConfig:
    return EngineConfig(enabled=True)
def engine(engine_config: EngineConfig) -> BingScraperSearchEngine:
    return BingScraperSearchEngine(config=engine_config, num_results=5)
def mock_results() -> list[MockSearchResult]:
        MockSearchResult(
class TestBingScraperEngine:
    @patch("twat_search.web.engines.bing_scraper.BingScraper")
    def test_init(self, mock_BingScraper: MagicMock, engine: Any) -> None:
        mock_BingScraper.assert_not_called()
    async def test_search_basic(
        mock_instance = MagicMock()
        results = await engine.search("test query")
        assert len(results) == 2
        assert isinstance(results[0], SearchResult)
        assert str(results[0].url) == "https://example.com/1"
        mock_BingScraper.assert_called_once_with(max_retries=3, delay_between_requests=1.0)
        mock_instance.search.assert_called_once_with("test query", num_results=5)
    async def test_custom_parameters(self, mock_BingScraper: MagicMock) -> None:
        engine = BingScraperSearchEngine(
            config=EngineConfig(enabled=True),
        await engine.search("test query")
        mock_BingScraper.assert_called_once_with(max_retries=5, delay_between_requests=2.0)
        mock_instance.search.assert_called_once_with("test query", num_results=10)
    async def test_invalid_url_handling(self, mock_BingScraper: MagicMock, engine: BingScraperSearchEngine) -> None:
        assert len(results) == 1
    @patch("twat_search.web.api.search")
    async def test_bing_scraper_convenience_function(self, mock_search: AsyncMock) -> None:
            SearchResult(
                url=HttpUrl("https://example.com"),
        results = await bing_scraper(
        mock_search.assert_called_once()
    async def test_empty_query(self, mock_BingScraper: MagicMock, engine: BingScraperSearchEngine) -> None:
        with pytest.raises(EngineError) as excinfo:
            await engine.search("")
        assert "Search query cannot be empty" in str(excinfo.value)
    async def test_no_results(self, mock_BingScraper: MagicMock, engine: BingScraperSearchEngine) -> None:
        assert isinstance(results, list)
        assert len(results) == 0
    async def test_network_error(self, mock_BingScraper: MagicMock, engine: BingScraperSearchEngine) -> None:
        mock_instance.search.side_effect = ConnectionError("Network timeout")
        assert "Network error connecting to Bing" in str(excinfo.value)
    async def test_parsing_error(self, mock_BingScraper: MagicMock, engine: BingScraperSearchEngine) -> None:
        mock_instance.search.side_effect = RuntimeError("Failed to parse HTML")
        assert "Error parsing Bing search results" in str(excinfo.value)
    async def test_invalid_result_format(self, mock_BingScraper: MagicMock, engine: BingScraperSearchEngine) -> None:
        class InvalidResult:
            def __init__(self):
        mock_instance.search.return_value = [InvalidResult()]

================
File: tests/conftest.py
================
@pytest.fixture(autouse=True)
def isolate_env_vars(monkeypatch: MonkeyPatch) -> None:
    for env_var in list(os.environ.keys()):
        if any(env_var.endswith(suffix) for suffix in ["_API_KEY", "_ENABLED", "_DEFAULT_PARAMS"]):
            monkeypatch.delenv(env_var, raising=False)
    monkeypatch.setenv("_TEST_ENGINE", "true")
def env_vars_for_brave(monkeypatch: MonkeyPatch) -> None:
        sys.path.insert(0, str(Path(__file__).parent.parent))
        class MockBraveEngine(SearchEngine):
        register_engine(MockBraveEngine)
    monkeypatch.setenv("BRAVE_API_KEY", "test_brave_key")
    monkeypatch.setenv("BRAVE_ENABLED", "true")
    monkeypatch.setenv("BRAVE_DEFAULT_PARAMS", '{"count": 10}')
    monkeypatch.delenv("_TEST_ENGINE", raising=False)

================
File: tests/test_twat_search.py
================
def test_version():

================
File: .gitignore
================
*_autogen/
.DS_Store
__version__.py
__pycache__/
_Chutzpah*
_deps
_NCrunch_*
_pkginfo.txt
_Pvt_Extensions
_ReSharper*/
_TeamCity*
_UpgradeReport_Files/
!?*.[Cc]ache/
!.axoCover/settings.json
!.vscode/extensions.json
!.vscode/launch.json
!.vscode/settings.json
!.vscode/tasks.json
!**/[Pp]ackages/build/
!Directory.Build.rsp
.*crunch*.local.xml
.axoCover/*
.builds
.cr/personal
.fake/
.history/
.ionide/
.localhistory/
.mfractor/
.ntvs_analysis.dat
.paket/paket.exe
.sass-cache/
.vs/
.vscode
.vscode/*
.vshistory/
[Aa][Rr][Mm]/
[Aa][Rr][Mm]64/
[Bb]in/
[Bb]uild[Ll]og.*
[Dd]ebug/
[Dd]ebugPS/
[Dd]ebugPublic/
[Ee]xpress/
[Ll]og/
[Ll]ogs/
[Oo]bj/
[Rr]elease/
[Rr]eleasePS/
[Rr]eleases/
[Tt]est[Rr]esult*/
[Ww][Ii][Nn]32/
*_h.h
*_i.c
*_p.c
*_wpftmp.csproj
*- [Bb]ackup ([0-9]).rdl
*- [Bb]ackup ([0-9][0-9]).rdl
*- [Bb]ackup.rdl
*.[Cc]ache
*.[Pp]ublish.xml
*.[Rr]e[Ss]harper
*.a
*.app
*.appx
*.appxbundle
*.appxupload
*.aps
*.azurePubxml
*.bim_*.settings
*.bim.layout
*.binlog
*.btm.cs
*.btp.cs
*.build.csdef
*.cab
*.cachefile
*.code-workspace
*.coverage
*.coveragexml
*.d
*.dbmdl
*.dbproj.schemaview
*.dll
*.dotCover
*.DotSettings.user
*.dsp
*.dsw
*.dylib
*.e2e
*.exe
*.gch
*.GhostDoc.xml
*.gpState
*.ilk
*.iobj
*.ipdb
*.jfm
*.jmconfig
*.la
*.lai
*.ldf
*.lib
*.lo
*.log
*.mdf
*.meta
*.mm.*
*.mod
*.msi
*.msix
*.msm
*.msp
*.ncb
*.ndf
*.nuget.props
*.nuget.targets
*.nupkg
*.nvuser
*.o
*.obj
*.odx.cs
*.opendb
*.opensdf
*.opt
*.out
*.pch
*.pdb
*.pfx
*.pgc
*.pgd
*.pidb
*.plg
*.psess
*.publishproj
*.publishsettings
*.pubxml
*.pyc
*.rdl.data
*.rptproj.bak
*.rptproj.rsuser
*.rsp
*.rsuser
*.sap
*.sbr
*.scc
*.sdf
*.sln.docstates
*.sln.iml
*.slo
*.smod
*.snupkg
*.so
*.suo
*.svclog
*.tlb
*.tlh
*.tli
*.tlog
*.tmp
*.tmp_proj
*.tss
*.user
*.userosscache
*.userprefs
*.vbp
*.vbw
*.VC.db
*.VC.VC.opendb
*.VisualState.xml
*.vsp
*.vspscc
*.vspx
*.vssscc
*.xsd.cs
**/[Pp]ackages/*
**/*.DesktopClient/GeneratedArtifacts
**/*.DesktopClient/ModelManifest.xml
**/*.HTMLClient/GeneratedArtifacts
**/*.Server/GeneratedArtifacts
**/*.Server/ModelManifest.xml
*~
~$*
$tf/
AppPackages/
artifacts/
ASALocalRun/
AutoTest.Net/
Backup*/
BenchmarkDotNet.Artifacts/
bld/
BundleArtifacts/
ClientBin/
cmake_install.cmake
CMakeCache.txt
CMakeFiles
CMakeLists.txt.user
CMakeScripts
CMakeUserPresets.json
compile_commands.json
coverage*.info
coverage*.json
coverage*.xml
csx/
CTestTestfile.cmake
dlldata.c
DocProject/buildhelp/
DocProject/Help/*.hhc
DocProject/Help/*.hhk
DocProject/Help/*.hhp
DocProject/Help/*.HxC
DocProject/Help/*.HxT
DocProject/Help/html
DocProject/Help/Html2
ecf/
FakesAssemblies/
FodyWeavers.xsd
Generated_Code/
Generated\ Files/
healthchecksdb
install_manifest.txt
ipch/
Makefile
MigrationBackup/
mono_crash.*
nCrunchTemp_*
node_modules/
nunit-*.xml
OpenCover/
orleans.codegen.cs
Package.StoreAssociation.xml
paket-files/
project.fragment.lock.json
project.lock.json
publish/
PublishScripts/
rcf/
ScaffoldingReadMe.txt
ServiceFabricBackup/
StyleCopReport.xml
Testing
TestResult.xml
UpgradeLog*.htm
UpgradeLog*.XML
x64/
x86/
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# Distribution / packaging
!dist/.gitkeep

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/
.ruff_cache/

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# IDE
.idea/
.vscode/
*.swp
*.swo
*~

# OS
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Project specific
__version__.py
_private

================
File: .pre-commit-config.yaml
================
repos:
  - repo: https://github.com/asottile/pyupgrade
    rev: v3.19.1
    hooks:
      - id: pyupgrade
        args: [--py311-plus]
  - repo: https://github.com/pycqa/isort
    rev: 6.0.0
    hooks:
      - id: isort
        args: ['--profile=black', '--line-length=120']
  - repo: https://github.com/MarcoGorelli/absolufy-imports
    rev: v0.3.1
    hooks:
      - id: absolufy-imports
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v5.0.0
    hooks:
      - id: check-yaml
      - id: requirements-txt-fixer
  - repo: https://github.com/asottile/add-trailing-comma
    rev: v3.1.0
    hooks:
      - id: add-trailing-comma
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.9.7
    hooks:
      - id: ruff
        types: [python]
        additional_dependencies: ['typing-extensions']
        args: [
          '--fix',
          '--ignore=ARG001,ARG002,ARG004,ARG005,B904,C901,DTZ005,E501,F401,F811,FBT001,FBT002,FBT003,I001,ISC001,N803,PLR0911,PLR0912,PLR0913,PLR0915,PLR2004,S311,S603,S607,T201'
        ]
      - id: ruff-format
        args: [--respect-gitignore]
        types: [python]

================
File: CHANGELOG.md
================
# Changelog

All notable changes to this project will be documented in this file.

## Changes by date

### 2025-02-26

#### Changed

- Refactored the CLI to import engine functions more safely, handling missing dependencies.
- Updated the CLI to use a dictionary to map the registered engines to their convenience functions, making it easier to call functions.
- Standardized engine names to use underscores internally.
- Modified the `_parse_engines` method in the CLI to handle special strings ("free", "best", "all") for selecting groups of engines.
- Updated the `q` method in the CLI to use unified parameters and to control verbose output.
- Updated the `info` method in the CLI to use unified parameters, to list engines, and to provide detailed information about specific engines.
- Updated `_display_results` in the CLI to handle multiple results per engine and optional plain-text output (no table, just URLs).
- Updated the `_display_json_results` function in the CLI to create JSON output.
- Updated the `config` class to parse environment variables for engines using a regular expression, addressing issues with engine configuration.
- Updated various engine implementations (Brave, Tavily, You, Perplexity, SerpApi) to use unified parameters and improve URL handling.

#### Added

- Implemented `bing_scraper` convenience function.
- Added comprehensive tests to verify the correct behavior of the `bing_scraper` convenience function.
- Implemented the `WebScoutEngine` class, providing basic search functionality.
- Added new search engine integrations:
    - `google_scraper.py`: Implemented a `GoogleScraperEngine` and a `google_scraper` convenience function.
    - `searchit.py`: Implemented several search engines using the `searchit` library: `GoogleSearchitEngine`, `YandexSearchitEngine`, `QwantSearchitEngine`, `BingSearchitEngine`.
    - `anywebsearch.py`: Implemented multiple search engines using the `anywebsearch` library: `GoogleAnyWebSearchEngine`, `BingAnyWebSearchEngine`, `BraveAnyWebSearchEngine`, `QwantAnyWebSearchEngine`, `YandexAnyWebSearchEngine`.
    - `hasdata.py`: Implemented `HasDataGoogleEngine` and `HasDataGoogleLightEngine`.
- Added tests for `bing_scraper`.

### 2025-02-25

#### Added

- Created initial project structure and implemented several search engine integrations (Brave, Google, Tavily, Perplexity, You.com).
- Developed core functionality, including a command-line interface (CLI) and asynchronous search capabilities.
- Implemented configuration management and exception handling.
- Implemented rate limiting utility.
- Added Pydantic models for data validation.
- Added unit tests for the models, configuration, utility functions, exceptions, and the base search engine class.

#### Changed

- Updated the `config.py` file to correctly import BaseSettings from the pydantic-settings package.
- Updated the `pyproject.toml` file to add pydantic-settings as a dependency.
- Updated the example usage in `example.py`.
- Completed the implementation of the web search functionality as specified in TODO.md.
- Planned comprehensive tests for the package.

### 2025-02-25 - Initial Development

- Created initial project structure and files.
- Created a preliminary TODO.md, PROGRESS.md, and research.txt.

---

================
File: cleanup.py
================
LOG_FILE = Path("CLEANUP.txt")
os.chdir(Path(__file__).parent)
def new() -> None:
    if LOG_FILE.exists():
        LOG_FILE.unlink()
def prefix() -> None:
    readme = Path(".cursor/rules/0project.mdc")
    if readme.exists():
        log_message("\n=== PROJECT STATEMENT ===")
        content = readme.read_text()
        log_message(content)
def suffix() -> None:
    todo = Path("TODO.md")
    if todo.exists():
        log_message("\n=== TODO.md ===")
        content = todo.read_text()
def log_message(message: str) -> None:
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with LOG_FILE.open("a") as f:
        f.write(log_line)
def run_command(cmd: list[str], check: bool = True) -> subprocess.CompletedProcess:
        result = subprocess.run(
            log_message(result.stdout)
        log_message(f"Command failed: {' '.join(cmd)}")
        log_message(f"Error: {e.stderr}")
        return subprocess.CompletedProcess(cmd, 1, "", str(e))
def check_command_exists(cmd: str) -> bool:
        subprocess.run(["which", cmd], check=True, capture_output=True)
class Cleanup:
    def __init__(self) -> None:
        self.workspace = Path.cwd()
    def _check_required_files(self) -> bool:
            if not (self.workspace / file).exists():
                log_message(f"Error: {file} is missing")
    def _venv(self) -> None:
        log_message("Setting up virtual environment")
            run_command(["uv", "venv"])
            if venv_path.exists():
                os.environ["VIRTUAL_ENV"] = str(self.workspace / ".venv")
                log_message("Virtual environment created and activated")
                log_message(
            log_message(f"Failed to create virtual environment: {e}")
    def _install(self) -> None:
        log_message("Installing package with all extras")
            self._venv()
            run_command(["uv", "pip", "install", "-e", ".[test,dev]"])
            log_message("Package installed successfully")
            log_message(f"Failed to install package: {e}")
    def status(self) -> None:
        prefix()  # Add README.md content at start
        _print_header("Current Status")
        self._check_required_files()
        _generate_tree()
        result = run_command(["git", "status"], check=False)
        _print_header("Environment Status")
        self._install()
        _run_checks()
        suffix()  # Add TODO.md content at end
    def venv(self) -> None:
        _print_header("Virtual Environment Setup")
    def install(self) -> None:
        _print_header("Package Installation")
    def update(self) -> None:
        self.status()
        if _git_status():
            log_message("Changes detected in repository")
                run_command(
                run_command(["pre-commit", "run", "--all-files"])
                run_command(["git", "add", "-A", "."])
                run_command(["git", "commit", "-m", commit_msg])
                log_message("Changes committed successfully")
                log_message(f"Failed to commit changes: {e}")
            log_message("No changes to commit")
    def push(self) -> None:
        _print_header("Pushing Changes")
            run_command(["git", "push"])
            log_message("Changes pushed successfully")
            log_message(f"Failed to push changes: {e}")
def _generate_tree(self) -> None:
    if not check_command_exists("tree"):
        rules_dir = Path(".cursor/rules")
        rules_dir.mkdir(parents=True, exist_ok=True)
        tree_result = run_command(
        with open(rules_dir / "filetree.mdc", "w") as f:
            f.write("---\ndescription: File tree of the project\nglobs: \n---\n")
            f.write(tree_text)
        log_message("\nProject structure:")
        log_message(tree_text)
        log_message(f"Failed to generate tree: {e}")
def _git_status(self) -> bool:
    result = run_command(["git", "status", "--porcelain"], check=False)
    return bool(result.stdout.strip())
def _run_checks(self) -> None:
    log_message("Running code quality checks")
        log_message(">>> Running code fixes...")
        log_message(">>>Running type checks...")
        run_command(["python", "-m", "mypy", "src", "tests"], check=False)
        log_message(">>> Running tests...")
        run_command(["python", "-m", "pytest", "tests"], check=False)
        log_message("All checks completed")
        log_message(f"Failed during checks: {e}")
def _print_header(self, message: str) -> None:
    log_message(f"\n=== {message} ===")
def repomix(
            cmd.append("--compress")
            cmd.append("--remove-empty-lines")
            cmd.append("-i")
            cmd.append(ignore_patterns)
        cmd.extend(["-o", output_file])
        run_command(cmd)
        log_message(f"Repository content mixed into {output_file}")
        log_message(f"Failed to mix repository: {e}")
def print_usage() -> None:
    log_message("Usage:")
    log_message("  cleanup.py status   # Show current status and run all checks")
    log_message("  cleanup.py venv     # Create virtual environment")
    log_message("  cleanup.py install  # Install package with all extras")
    log_message("  cleanup.py update   # Update and commit changes")
    log_message("  cleanup.py push     # Push changes to remote")
def main() -> None:
    new()  # Clear log file
    if len(sys.argv) < 2:
        print_usage()
        sys.exit(1)
    cleanup = Cleanup()
            cleanup.status()
            cleanup.venv()
            cleanup.install()
            cleanup.update()
            cleanup.push()
        log_message(f"Error: {e}")
    repomix()
    print(LOG_FILE.read_text(encoding="utf-8"))
    main()

================
File: LICENSE
================
MIT License

Copyright (c) 2025 Adam Twardoch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

================
File: NEXTENGINES.md
================
# Next backends and engines to implement

## https://github.com/Sanix-Darker/falla

Implement all engines as `*-falla` from this. 

Token Usage:
GitHub Tokens: 7766
LLM Input Tokens: 0
LLM Output Tokens: 0
Total Tokens: 7766

FileTree:
.gitignore
README.md
app/__init__.py
app/core/Aol.py
app/core/Ask.py
app/core/Bing.py
app/core/DogPile.py
app/core/DuckDuckGo.py
app/core/Falla.py
app/core/Gibiru.py
app/core/Google.py
app/core/Mojeek.py
app/core/Qwant.py
app/core/SearchEncrypt.py
app/core/StartPage.py
app/core/Yahoo.py
app/core/Yandex.py
app/core/__init__.py
app/main.py
app/settings.py
app/utils.py
example.config.txt
requirements.txt
tests/test.py

Analysis:
.gitignore
```.gitignore
config.txt
*__pycache__
.idea
.vscode
.history
*.json
*.log
*.pyc
```
README.md
# FAllA

A search-engine-cli-scraper for more than 15 search engines, including Google. duckduckgo, Bing, Ask, etc...
<br>
**NOTE:** For educationnal purpose, am not responsible of the bad use of this tool !

## Requirements
- Python (3.x)
- Docker-CE (Not required for all search-engine, just few of them)

## How to install

- You need to install all requirements :
```shell-script
pip3 install -r requirements.txt
```
- Install geckodriver :
```shell-script
# For linux users
sudo apt install firefox-geckodriver

# For other OS's users, please check releases on https://github.com/mozilla/geckodriver/releases
```

- Pull and run the splash-scrap module from docker-hub (Some of search engine need this):
```shell-script
docker run -p 8050:8050 scrapinghub/splash
```

- Replace `example.config.txt` by `config.txt` and provide the running IP for the splash-scrap

## How to launch

How to use Falla:
```shell-script
usage: main.py [-h] [-e ENGINE] [-q QUERY]

optional arguments:
  -h, --help            show this help message and exit
  -e ENGINE, --engine ENGINE
                        The search engine
  -q QUERY, --query QUERY
                        The query text
```

- To list all search-engine:
```shell-script
$ python3 -m app.main
# output
[+] Falla [the search-engine-scraper]
[+] Listing search-Engines
[+] > google
[+] > bing
[+] > aol
[+] > dogpile
[+] > falla
[+] > ask
[+] > qwant
[+] > duckduckgo
[+] > mojeek
[+] > gibiru
[+] > yandex
[+] > yahoo
[+] > searchencrypt
[+] > iem
[+] > kallasearch
[+] > wosx
```

- To search something:
```shell-script
$ python3 -m app.main -e google -q "sanix darker"
# output
```
<img src="./images/falla.png">
## Author

- Sanix-darker
app/__init__.py
```.py

```
app/core/Aol.py
```.py
# Falla-Aol
# -*- encoding: utf-8 -*-
# Sanix-darker

from app.core.Falla import Falla


class Aol(Falla):
    def __init__(self):
        self.try_it = 0
        self.max_retry = 3
        self.source = "Aol"
        self.mode = "requests"
        self.results_box = "//div[@id='web']"
        self.each_element = {
            "tag": "li"
        }
        self.href = {
            "tag": "a",
            "type": "attribute",
            "key": "href",
            "child": {}
        }
        self.title = {
            "tag": "a",
            "type": "text"
        }
        self.cite = {
            "tag": "div:compText",
            "child": {
                "tag": "p",
                "type": "text"
            }
        }

    def search(self, search_text, pages=""):
        url = "https://search.aol.com/aol/search?q=" + search_text.replace(" ", "+") + pages
        print("[+] Searching results for '" + url.split("=")[1].replace("+", " ") +
              "' on '" + self.source + "' :\n")

        return self.fetch(url)

# a = Aol()
# print(a.search("un avion"))

```
app/core/Ask.py
```.py
# Falla-Ask
# -*- encoding: utf-8 -*-
# Sanix-darker

from app.core.Falla import Falla


class Ask(Falla):
    def __init__(self):
        self.try_it = 0
        self.max_retry = 3
        self.source = "Ask"
        self.mode = "requests"
        self.results_box = "//div[@class='l-mid-content']"
        self.each_element = {
            "tag": "div",
            "attr": {"class": "PartialSearchResults-item"}
        }
        self.href = {
            "tag": "a:PartialSearchResults-item-title-link",
            "type": "attribute",
            "key": "href",
            "child": {}
        }
        self.title = {
            "tag": "a:PartialSearchResults-item-title-link",
            "type": "text"
        }
        self.cite = {
            "tag": "p:PartialSearchResults-item-abstract",
            "type": "text"
        }

    def search(self, search_text, pages=""):
        url = "https://www.ask.com/web?q=" + search_text.replace(" ", "+") + pages
        print("[+] Searching results for '" + url.split("=")[1].replace("+", " ") +
              "' on '" + self.source + "' :\n")

        return self.fetch(url)

# ak = Ask()
# print(ak.search("un avion"))

```
app/core/Bing.py
```.py
# Falla-Bing
# -*- encoding: utf-8 -*-
# Sanix-darker

from selenium import webdriver
from selenium.webdriver.firefox.options import Options

from app.core.Falla import Falla


class Bing(Falla):
    def __init__(self):
        self.source = "Bing"
        self.mode = "splash_scrap"
        self.try_it = 0
        self.max_retry = 3
        self.results_box = "//ol[@id='b_results']"
        self.each_element = {
            "tag": "li",
            "attr": {"class": "b_algo"}
        }
        self.href = {
            "tag": "a",
            "type": "attribute",
            "key": "href",
            "child": {}
        }
        self.title = {
            "tag": "h2",
            "type": "text",
            "child": {}
        }
        self.cite = {
            "tag": "div:b_caption",
            "child": {
                "tag": "p",
                "type": "text"
            }
        }

    def search(self, search_text, pages=""):
        url = "https://www.bing.com/search?q=" + search_text.replace(" ", "+") + pages
        print("[+] Searching results for '" + url.split("=")[1].replace("+", " ") +
              "' on '" + self.source + "' :\n")

        return self.fetch(url)

# b = Bing()
# print(b.search("un avion"))

```
app/core/DogPile.py
```.py
# Falla-DogPile
# -*- encoding: utf-8 -*-
# Sanix-darker

from app.core.Falla import Falla


class DogPile(Falla):
    def __init__(self):
        self.try_it = 0
        self.max_retry = 3
        self.source = "DogPile"
        self.mode = "requests"
        self.results_box = "//div[@class='mainline-results']"
        self.each_element = {
            "tag": "div",
            "attr": {"class": "web-bing__result"}
        }
        self.href = {
            "tag": "a:web-bing__title",
            "type": "attribute",
            "key": "href",
            "child": {}
        }
        self.title = {
            "tag": "a:web-bing__title",
            "type": "text"
        }
        self.cite = {
            "tag": "span:web-bing__description",
            "type": "text"
        }

    def search(self, search_text, pages=""):
        url = "https://www.dogpile.com/serp?q=" + search_text.replace(" ", "+") + pages
        print("[+] Searching results for '" + url.split("=")[1].replace("+", " ") +
              "' on '" + self.source + "' :\n")

        return self.fetch(url)

# dp = DogPile()
# print(dp.search("un avion"))

```
app/core/DuckDuckGo.py
```.py
# Falla-DuckDuckGo
# -*- encoding: utf-8 -*-
# Sanix-darker

from selenium import webdriver
from selenium.webdriver.firefox.options import Options

from app.core.Falla import Falla


class DuckDuckGo(Falla):
    def __init__(self):
        self.option = Options()
        self.option.headless = True
        self.driver = webdriver.Firefox(options=self.option)

        self.source = "DuckDuckGo"
        self.mode = "selenium"
        self.try_it = 0
        self.max_retry = 3
        self.results_box = "//div[@id='links']"
        self.each_element = {
            "tag": "div",
            "attr": {"class": "result__body"}
        }
        self.href = {
            "tag": "a:result__a",
            "type": "attribute",
            "key": "href",
            "child": {}
        }
        self.title = {
            "tag": "a:result__a",
            "type": "text",
            "child": {}
        }
        self.cite = {
            "tag": "div:result__snippet",
            "type": "text",
            "child": {}
        }

    def search(self, search_text, pages=""):
        url = "https://duckduckgo.com/?q=" + search_text.replace(" ", "+") + pages
        print("[+] Searching results for '" + url.split("=")[1].replace("+", " ") +
              "' on '" + self.source + "' :\n")

        return self.fetch(url)

# d = DuckDuckGo()
# print(d.search("un avion"))

```
app/core/Falla.py
```.py
# Falla
# -*- encoding: utf-8 -*-
# Sanix-darker

import json
import sys
import time
import traceback
import requests
from bs4 import BeautifulSoup

from app.settings import *


class Falla:
    def __init__(self, results_box, each_element, href, title, cite):
        self.driver = None
        self.results_box = results_box
        self.each_element = each_element
        self.href = href
        self.title = title
        self.cite = cite
        self.try_it = 0
        self.max_retry = 4
        self.source = "Falla"
        self.mode = "requests"

        self.option = None
        self.option.headless = None
        self.driver = None

    def get_trace(self):
        """
        This method will just print the Traceback after an error occur
        """
        print("Exception in code:")
        print("-" * 60)
        traceback.print_exc(file=sys.stdout)
        print("-" * 60)

    def get_element_from_type(self, to_return, the_filter=None):
        """
        A parse-method to extract from parse_entry_point an element as attribute or text

        to_return: The BeautifulSoup object
        the_filter: The Json-Filter
        """
        if the_filter["type"] == "text":
            to_return = to_return.getText()
        elif the_filter["type"] == "attribute":
            try:
                to_return = to_return[the_filter["key"]]
            except Exception:
                self.get_trace()
                to_return = ""
        else:
            print("[x] Error, specify a valid type !")

        return to_return

    def get_tag(self, element, tag):
        """
        This method will extract the tag having a attriute or not

        element: The Beautifull-Soup object
        tag: The Json tag (can be a class or other)
        """
        if ":" in tag:
            return element.find(tag.split(":")[0], {"class": tag.split(":")[1]})
        else:
            return element.find(tag)

    def parse_entry_point(self, element, the_filter):
        """
        This method will extract the href, the title and the cite in result

        element: The Beautifull-Soup Object
        the_filter: The JSON object for the Filter
        """
        to_return = self.get_tag(element, the_filter["tag"])

        if "type" in the_filter:
            to_return = self.get_element_from_type(to_return, the_filter)
        else:
            if bool(the_filter["child"]):  # There are some child
                to_return = self.get_tag(to_return, the_filter["child"]["tag"])

                to_return = self.get_element_from_type(to_return, the_filter["child"])
            else:
                print("[x] Malformed filter !")

        return ' '.join(str(to_return).split())

    def get_each_elements(self, soup):
        """
        Get all elements list from Beautifull-Soup object

        soup: The Beautiful-Soup object
        """
        fetchs = []
        if "attr" in self.each_element:
            if self.each_element["attr"] is not None:
                fetchs = soup.findAll(self.each_element["tag"], self.each_element["attr"])
            else:
                fetchs = soup.findAll(self.each_element["tag"])
        else:
            fetchs = soup.findAll(self.each_element["tag"])

        return fetchs

    def scrapy_splash_request(self, to_fetch_url):
        """
        This method is responsible for scrapping a html_content from a url using splash-scrap

        to_fetch_url: The url of the website
        """
        json_data = {
            "response_body": False,
            "lua_source": "function main(splash, args)\r\n  assert(splash:go(args.url))\r\n  assert(splash:wait("
                          "0.5))\r\n  return {\r\n    html = splash:html(),\r\n    png = splash:png(),\r\n    har = "
                          "splash:har(),\r\n  }\r\nend",
            "url": to_fetch_url,
            "html5_media": False,
            "save_args": [],
            "viewport": "1024x768",
            "http_method": "GET",
            "resource_timeout": 0,
            "render_all": False,
            "png": 1,
            "har": 1,
            "timeout": 90,
            "request_body": False,
            "load_args": {},
            "html": 1,
            "images": 1,
            "wait": 0.7
        }

        r = requests.post(SPLASH_SCRAP_URL + "/execute", json=json_data)
        html_string = json.loads(r.content.decode())["html"]

        return html_string

    def get_html_content(self, url):
        """
        This method will check the mode of fetching and proceed

        url: The url of the target
        """
        html_content = ""
        if self.mode == "selenium":
            self.driver.get(url)
            self.driver.implicitly_wait(10)  # in seconds

            element = self.driver.find_element_by_xpath(self.results_box)
            html_content = element.get_attribute('outerHTML')
        elif self.mode == "splash_scrap":
            html_content = self.scrapy_splash_request(url)

        elif self.mode == "requests":
            r = requests.get(url, headers={"Upgrade-Insecure-Requests": "1",
                                           "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, "
                                                         "like Gecko) Chrome/80.0.3987.100 Safari/537.36",
                                           "Sec-Fetch-Dest": "document",
                                           "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,"
                                                     "image/webp,image/apng,*/*;q=0.8,"
                                                     "application/signed-exchange;v=b3;q=0.9 "
                                           })
            html_content = r.content.decode()

        return html_content

    def fetch(self, url):
        """
        The main method for fetching results from url

        url: The url of the target
        """
        html_content = self.get_html_content(url)
        soup = BeautifulSoup(html_content, 'html.parser')
        fetchs = self.get_each_elements(soup)

        results = []
        # print("fetchs: ", fetchs)
        # print("self.parse_entry_point(elt, self.href): ", self.parse_entry_point(fetchs[0], self.href))
        for elt in fetchs:
            try:
                element = {
                    "source": self.source,
                    "href": self.parse_entry_point(elt, self.href),  # elt.find("a")["href"]
                    "title": self.parse_entry_point(elt, self.title),  # str(elt.find("a").find("h3").getText())
                    "cite": self.parse_entry_point(elt, self.cite)  # str(elt.find("a").find("cite").getText())
                }
                results.append(element)
            except Exception:
                self.get_trace()

        if len(results) == 0 and self.try_it < self.max_retry:
            self.try_it += 1
            time.sleep(0.5)
            print("[+] try: ", self.try_it)
            self.fetch(url)

        if self.mode == "selenium":
            self.driver.quit()

        return json.dumps(results, ensure_ascii=False)

```
app/core/Gibiru.py
```.py
# Falla-Gibiru
# -*- encoding: utf-8 -*-
# Sanix-darker

from app.core.Falla import Falla


class Gibiru(Falla):
    def __init__(self):
        self.source = "Gibiru"
        self.mode = "splash_scrap"
        self.try_it = 0
        self.max_retry = 3
        self.results_box = "//div[@class='gsc-resultsRoot']"
        self.each_element = {
            "tag": "div",
            "attr": {"class": "gs-webResult"}
        }
        self.href = {
            "tag": "a:gs-title",
            "type": "attribute",
            "key": "data-ctorig",
            "child": {}
        }
        self.title = {
            "tag": "a:gs-title",
            "type": "text",
            "child": {}
        }
        self.cite = {
            "tag": "div:gs-snippet",
            "type": "text",
            "child": {}
        }

    def search(self, search_text, pages=""):
        url = "https://gibiru.com/results.html?q=" + search_text.replace(" ", "+") + pages
        print("[+] Searching results for '" + url.split("=")[1].replace("+", " ") +
              "' on '" + self.source + "' :\n")

        return self.fetch(url)

# gi = Gibiru()
# print(gi.search("un avion"))

```
app/core/Google.py
```.py
# Falla-Google
# -*- encoding: utf-8 -*-
# Sanix-darker

import json
import time
from selenium import webdriver
from selenium.webdriver.firefox.options import Options

from app.core.Falla import Falla


class Google(Falla):
    def __init__(self):
        self.option = Options()
        self.option.headless = True
        self.driver = webdriver.Firefox(options=self.option)

        self.try_it = 0
        self.max_retry = 3
        self.source = "Google"
        self.mode = "selenium"
        self.results_box = "//div[@id='search']"
        self.each_element = {
            "tag": "div",
            "attr": {"class": "g"}
        }
        self.href = {
            "tag": "a",
            "type": "attribute",
            "key": "href",
            "child": {}
        }
        self.title = {
            "tag": "a",
            "child": {
                "tag": "h3",
                "type": "text"
            }
        }
        self.cite = {
            "tag": "a",
            "child": {
                "tag": "cite",
                "type": "text"
            }
        }

    def search(self, search_text, pages=""):

        base_url = "https://www.google.com/search?q=" + search_text.replace(" ", "+") + pages
        print("[+] Searching results for '" + base_url.split("=")[1].replace("+", " ") +
              "' on '" + self.source + "' :\n")

        # results = json.loads(self.fetch(base_url))
        #
        # if pages > 1:
        #     for nb_page in range(pages):
        #         url = base_url + "&start=" + str(nb_page + 1) + "0"
        #         time.sleep(1)
        #         results = results + self.fetch(url)
        #
        # results = json.dumps(results)

        return self.fetch(base_url)


# g = Google()
# print(g.search("un avion"))

```
app/core/Mojeek.py
```.py
# Falla-Mojeek
# -*- encoding: utf-8 -*-
# Sanix-darker

from app.core.Falla import Falla


class Mojeek(Falla):
    def __init__(self):
        self.try_it = 0
        self.max_retry = 3
        self.source = "Mojeek"
        self.mode = "requests"
        self.results_box = "//ul[@class='results-standard']"
        self.each_element = {
            "tag": "li"
        }
        self.href = {
            "tag": "a:ob",
            "type": "attribute",
            "key": "href",
            "child": {}
        }
        self.title = {
            "tag": "a:ob",
            "type": "text"
        }
        self.cite = {
            "tag": "p:s",
            "type": "text"
        }

    def search(self, search_text, pages=""):
        url = "https://www.mojeek.com/search?q=" + search_text.replace(" ", "+") + pages
        print("[+] Searching results for '" + url.split("=")[1].replace("+", " ") +
              "' on '" + self.source + "' :\n")

        return self.fetch(url)

# mk = Mojeek()
# print(mk.search("un avion"))

```
app/core/Qwant.py
```.py
# Falla-Qwant
# -*- encoding: utf-8 -*-
# Sanix-darker

from selenium import webdriver
from selenium.webdriver.firefox.options import Options

from app.core.Falla import Falla


class Qwant(Falla):
    def __init__(self):
        self.option = Options()
        self.option.headless = True
        self.driver = webdriver.Firefox(options=self.option)

        self.try_it = 0
        self.max_retry = 3
        self.source = "Qwant"
        self.mode = "selenium"
        self.results_box = "//div[@class='result_fragment']"
        self.each_element = {
            "tag": "div",
            "attr": {"class": "result--web"}
        }
        self.href = {
            "tag": "a:result--web--link",
            "type": "attribute",
            "key": "href",
            "child": {}
        }
        self.title = {
            "tag": "span:result--web--title",
            "type": "text",
            "child": {}
        }
        self.cite = {
            "tag": "div:result--web",
            "child": {
                "tag": "p",
                "type": "text"
            }
        }

    def search(self, search_text, pages=""):
        url = "https://www.qwant.com/?q=" + search_text.replace(" ", "+") + pages
        print("[+] Searching results for '" + url.split("=")[1].replace("+", " ") +
              "' on '" + self.source + "' :\n")

        return self.fetch(url)

# qw = Qwant()
# print(qw.search("un avion"))

```
app/core/SearchEncrypt.py
```.py
# Falla-SearchEncrypt
# -*- encoding: utf-8 -*-
# Sanix-darker

from app.core.Falla import Falla


class SearchEncrypt(Falla):
    def __init__(self):
        self.try_it = 0
        self.max_retry = 3
        self.source = "SearchEncrypt"
        self.mode = "requests"
        self.results_box = "//section[@class='serp__results']"
        self.each_element = {
            "tag": "div",
            "attr": {"class": "web-result"}
        }
        self.href = {
            "tag": "a:web-result__link",
            "type": "attribute",
            "key": "href",
            "child": {}
        }
        self.title = {
            "tag": "a:web-result__link",
            "type": "text",
            "child": {}
        }
        self.cite = {
            "tag": "p:web-result__description",
            "child": {
                "tag": "span",
                "type": "text"
            }
        }

    def search(self, search_text, pages=""):
        url = "https://www.searchencrypt.com/search/?q=" + search_text.replace(" ", "+") + pages
        print("[+] Searching results for '" + url.split("=")[1].replace("+", " ") +
              "' on '" + self.source + "' :\n")

        return self.fetch(url)

# se = SearchEncrypt()
# print(se.search("un avion"))

```
app/core/StartPage.py
```.py
# Falla-StartPage
# -*- encoding: utf-8 -*-
# Sanix-darker

from app.core.Falla import Falla


class StartPage(Falla):
    def __init__(self):
        self.try_it = 0
        self.max_retry = 3
        self.source = "StartPage"
        self.mode = "requests"
        self.results_box = "//section[@id='w-gl--default']"
        self.each_element = {
            "tag": "div",
            "attr": {"class": "w-gl__result"}
        }
        self.href = {
            "tag": "a:w-gl__result-title",
            "type": "attribute",
            "key": "href",
            "child": {}
        }
        self.title = {
            "tag": "a:w-gl__result-title",
            "child": {
                "tag": "h3",
                "type": "text"
            }
        }
        self.cite = {
            "tag": "p:w-gl__description",
            "type": "text",
            "child": {}
        }

    def search(self, search_text, pages=""):
        url = "https://www.startpage.com/sp/search?q=" + search_text.replace(" ", "+") + pages
        print("[+] Searching results for '" + url.split("=")[1].replace("+", " ") +
              "' on '" + self.source + "' :\n")

        return self.fetch(url)

# sp = StartPage()
# print(sp.search("un avion"))

```
app/core/Yahoo.py
```.py
# Falla-Yahoo
# -*- encoding: utf-8 -*-
# Sanix-darker

from selenium import webdriver
from selenium.webdriver.firefox.options import Options

from app.core.Falla import Falla


class Yahoo(Falla):
    def __init__(self):
        self.option = Options()
        self.option.headless = True
        self.driver = webdriver.Firefox(options=self.option)

        self.try_it = 0
        self.max_retry = 3
        self.source = "Yahoo"
        self.mode = "selenium"
        self.results_box = "//div[@id='web']"
        self.each_element = {
            "tag": "li"
        }
        self.href = {
            "tag": "a",
            "type": "attribute",
            "key": "href",
            "child": {}
        }
        self.title = {
            "tag": "a",
            "type": "text"
        }
        self.cite = {
            "tag": "div:compText",
            "child": {
                "tag": "p",
                "type": "text"
            }
        }

    def search(self, search_text, pages=""):
        url = "https://search.yahoo.com/search?p=" + search_text.replace(" ", "+") + pages
        print("[+] Searching results for '" + url.split("=")[1].replace("+", " ") +
              "' on '" + self.source + "' :\n")

        return self.fetch(url)

# y = Yahoo()
# print(y.search("un avion"))

```
app/core/Yandex.py
```.py
# Falla-Yandex
# -*- encoding: utf-8 -*-
# Sanix-darker

from selenium import webdriver
from selenium.webdriver.firefox.options import Options

from app.core.Falla import Falla


class Yandex(Falla):
    def __init__(self):
        self.option = Options()
        self.option.headless = True
        self.driver = webdriver.Firefox(options=self.option)

        self.try_it = 0
        self.max_retry = 3
        self.source = "Yandex"
        self.mode = "selenium"
        self.results_box = "//div[@class='content__left']"
        self.each_element = {
            "tag": "li",
            "attr": {"class": "serp-item"}
        }
        self.href = {
            "tag": "a:link",
            "type": "attribute",
            "key": "href",
            "child": {}
        }
        self.title = {
            "tag": "div:organic__url-text",
            "type": "text"
        }
        self.cite = {
            "tag": "div:organic__content-wrapper",
            "child": {
                "tag": "div:text-container",
                "type": "text"
            }
        }

    def search(self, search_text, pages=""):
        url = "https://yandex.com/search/?text=" + search_text.replace(" ", "+") + pages
        print("[+] Searching results for '" + url.split("=")[1].replace("+", " ") +
              "' on '" + self.source + "' :\n")

        return self.fetch(url)

# y = Yandex()
# print(y.search("un avion"))

```
app/core/__init__.py
```.py
from app.core.Aol import Aol
from app.core.Ask import Ask
from app.core.Bing import Bing
from app.core.DogPile import DogPile
from app.core.DuckDuckGo import DuckDuckGo
from app.core.Gibiru import Gibiru
from app.core.Mojeek import Mojeek
from app.core.Google import Google
from app.core.Qwant import Qwant
from app.core.SearchEncrypt import SearchEncrypt
from app.core.StartPage import StartPage
from app.core.Yahoo import Yahoo
from app.core.Yandex import Yandex

```
app/main.py
```.py
# main-script
# -*- encoding: utf-8 -*-
#  _____ _    _     _        _    
# |  ___/ \  | |   | |      / \   
# | |_ / _ \ | |   | |     / _ \  
# |  _/ ___ \| |___| |___ / ___ \ 
# |_|/_/   \_\_____|_____/_/   \_\
#
# By Sanix-darker
__version__ = 0.1
__author__ = "Sanix-darker"

import argparse
from app.utils import *

if __name__ == "__main__":
    # Initialize the arguments
    # python3 -m app.main # Search-Engine list
    # python3 -m app.main -e aol -q "sanix darker"
    #
    # python3 -m app.main -e google -q "sanix darker" -p "&start=10"
    prs = argparse.ArgumentParser()
    prs.add_argument('-e', '--engine', help='The search engine', type=str, default="google")
    prs.add_argument('-q', '--query', help='The query text', type=str)
    prs.add_argument('-p', '--page', help='Number of pages to fetch', type=str, default="")
    prs = prs.parse_args()

    print("[+] Falla [the search-engine-scraper]")
    if prs.engine is not None and prs.query is not None:
        get_results(engine=prs.engine.lower(), query=prs.query.lower(), pages=prs.page)
    else:
        list_engines()

```
app/settings.py
```.py
import configparser as ConfigParser

# Configs parameters configParser.get('your-config', 'path1')
configParser = ConfigParser.RawConfigParser()
configFilePath = r'config.txt'
configParser.read(configFilePath)

# Filling parameters
SPLASH_SCRAP_URL = configParser.get('falla-config', 'SPLASH_SCRAP_URL')

```
app/utils.py
```.py
from os import listdir as ls
from app.core import *
import json


class Bcolors:
    """[summary]
    """
    AUTRE = '\033[96m'  # rose
    HEADER = '\033[95m'  # rose
    OKBLUE = '\033[94m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'  # jaune
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'


def list_engines():
    print(Bcolors().OKBLUE + "[+] Listing search-Engines" + Bcolors().ENDC)
    engines = [elt.replace(".py", "").lower() for elt in ls("./app/core/") if
               ".py" in elt and "__" not in elt and "falla" not in elt]
    for e in engines:
        print("[+] > " + Bcolors().AUTRE + e + Bcolors().ENDC)


def get_results(engine, query, pages):
    if engine == "aol" or engine == "al":
        f = Aol()
    elif engine == "ask" or engine == "ak":
        f = Ask()
    elif engine == "bing" or engine == "b":
        f = Bing()
    elif engine == "dogpile" or engine == "dp":
        f = DogPile()
    elif engine == "duckduckgo" or engine == "dd":
        f = DuckDuckGo()
    elif engine == "gibiru" or engine == "gu":
        f = Gibiru()
    elif engine == "mojeek" or engine == "m":
        f = Mojeek()
    elif engine == "qwant" or engine == "q":
        f = Qwant()
    elif engine == "searchencrypt" or engine == "se":
        f = SearchEncrypt()
    elif engine == "startpage" or engine == "sp":
        f = StartPage()
    elif engine == "yahoo" or engine == "y":
        f = Yahoo()
    elif engine == "google" or engine == "g":
        f = Google()
    else:
        f = Google()

    results = json.loads(f.search(query, pages))
    bcolors = Bcolors()

    for elt in results:
        print("|> " + bcolors.OKBLUE + elt["title"] + bcolors.ENDC)
        print("|- " + bcolors.WARNING + elt["href"] + bcolors.ENDC)
        print("|| " + elt["cite"])
        print("")

```
example.config.txt
```.txt
[falla-config]
SPLASH_SCRAP_URL = http://127.0.0.1:8050

```
requirements.txt
```.txt
requests==2.31.0
pandas==1.0.1
lxml==4.9.1
beautifulsoup4==4.8.2
selenium==3.141.0
configparser

```
tests/test.py
```.py

```

================
File: PROGRESS.md
================
---
this_file: PROGRESS.md
---

Consult @TODO.md for the detailed plan. Work through these items, checking them off as you complete them. Once a large part has been completed, update @TODO.md to reflect the progress.

# Task List

## 1. Fix Critical Engine Failures

- [x] Enhance `base.SearchEngine` class to enforce API key requirements
  - [x] Modify to require subclasses to define `env_api_key_names` as a class variable
  - [x] Add proper validation for engine code and API key requirements
  - [x] Centralize HTTP request handling with retries and error management

- [x] Enhance `config.py` for better API key handling
  - [x] Update `EngineConfig` to validate and require API keys as needed
  - [x] Improve error messages for missing API keys
  - [x] Add field validation for API keys

- [x] Update engine implementations correctly
  - [x] Fix You.com engines (`you` and `you_news`)
  - [x] Fix Perplexity (`pplx`) implementation
  - [ ] Fix SerpAPI integration
  - [ ] Ensure all engines implement proper API key handling

- [ ] Improve `get_engine` function and error handling
  - [ ] Add descriptive error messages when engines are not found or disabled
  - [ ] Handle engine initialization failures gracefully

- [ ] Add comprehensive tests for engine initialization
  - [ ] Test API key requirements
  - [ ] Test engine availability detection
  - [ ] Test disabled engine handling

## 2. Fix Data Integrity and Consistency Issues

- [!] Fix inconsistent `num_results` parameter handling
  - [x] Implement `_get_num_results` method in base class
  - [!] Implement proper result limiting in working engines (currently `-n 1` returns multiple results)
  - [!] Fix broken engines failing with "`object has no attribute 'get_num_results'`" error
    - [!] Fix `brave` engine implementation
    - [!] Fix `brave_news` engine implementation
    - [ ] Fix `critique` engine implementation
  - [ ] Ensure all engines properly handle and respect `num_results`
  - [ ] Add result limiting logic to all engine implementations

- [ ] Fix source attribution in search results
  - [ ] Ensure all engines use `self.engine_code` when creating `SearchResult` objects
  - [ ] Fix typos in result keys (e.g., `"snippetHighlitedWords"`)

- [!] Fix unwanted result combination in Google engines
  - [!] Fix `google_hasdata` engine returning results from other engines
  - [!] Fix `google_hasdata_full` engine returning results from other engines

## 3. Address Engine Reliability Issues (Empty Results)

- [!] Fix `searchit`-based engines
  - [!] Fix `bing_searchit` returning empty results
  - [!] Fix `google_searchit` returning empty results
  - [!] Fix `qwant_searchit` returning empty results
  - [!] Fix `yandex_searchit` returning empty results
  - [ ] Verify proper installation and dependencies
  - [ ] Create test script to isolate issues
  - [ ] Add detailed logging for debugging
  - [ ] Fix implementation issues or remove if necessary

- [!] Fix API key-based engines with initialization failures
  - [!] Fix `pplx` engine initialization
  - [!] Fix `you` engine initialization 
  - [!] Fix `you_news` engine initialization
  - [ ] Debug initialization failures and API key handling

- [ ] Test and fix other engines with empty results
  - [ ] Identify and address common failure patterns

## 4. Codebase Cleanup

- [ ] Remove all `anywebsearch` engines
  - [ ] Delete `src/twat_search/web/engines/anywebsearch.py`
  - [ ] Remove related imports from all files
  - [ ] Update engine constants and registration

- [x] Fix code duplication in `base.SearchEngine`
  - [x] Centralize common functionality
  - [x] Improve error handling consistency

## 5. Improve CLI Interface

- [ ] Update the `q` command in the CLI
  - [ ] Remove engine-specific parameters
  - [ ] Only use common parameters
  - [ ] Test CLI with various parameter combinations

## 6. Enforce Consistent JSON Output Format

- [ ] Standardize JSON output across all engines
  - [ ] Utilize the existing `SearchResult` model consistently
  - [ ] Remove utility functions like `_process_results` and `_display_json_results`
  - [ ] Remove `CustomJSONEncoder` class
  - [ ] Update engine `search` methods to return list of `SearchResult` objects

- [ ] Update API function return types
  - [ ] Change return type to `list[SearchResult]`
  - [ ] Ensure proper handling of results from engines

- [ ] Update CLI display functions
  - [ ] Use `model_dump` for JSON serialization
  - [ ] Implement simplified result display

## 7. Testing and Documentation

- [!] Fix failing tests
  - [ ] Address test failures in `test_api.py`
  - [ ] Address test failures in `test_config.py`
  - [ ] Address test failures in `test_bing_scraper.py`

- [ ] Create comprehensive test suite
  - [ ] Test each engine individually
  - [ ] Test parameter handling
  - [ ] Test error conditions and recovery

- [ ] Enhance debug logging throughout the codebase
  - [ ] Add consistent logging in all engines
  - [ ] Improve error messages for common failure cases

- [ ] Update documentation
  - [ ] Document engine-specific requirements
  - [ ] Add troubleshooting guidelines
  - [ ] Document parameter handling

## 8. Final Verification

- [ ] Run test command on all engines
  - [ ] Verify correct output format
  - [ ] Verify proper parameter handling
  - [ ] Check for consistent error handling

- [x] Run linters and code quality tools
  - [x] Address all Ruff and Mypy issues
  - [ ] Run `cleanup.py status` regularly during development

- [ ] Verify full system functionality
  - [ ] Test with the problem case command
  - [ ] Ensure all engines return proper results

================
File: pyproject.toml
================
[build-system]
requires = [
    'hatchling>=1.27.0',
    'hatch-vcs>=0.4.0',
]
build-backend = 'hatchling.build'
[tool.hatch.build.targets.wheel]
packages = ['src/twat_search']
[tool.hatch.build.hooks.vcs]
version-file = 'src/twat_search/__version__.py'

[tool.hatch.version]
source = 'vcs'

[tool.hatch.version.raw-options]
version_scheme = 'post-release'
[tool.hatch.envs.default]
dependencies = [
    'pytest',
    'pytest-cov',
    'mypy>=1.15.0',
    'ruff>=0.9.6',
    'absolufy-imports>=0.3.1',
    'pre-commit>=4.1.0',
    'pyupgrade>=3',
    'isort>=6.0.0',
]

[tool.hatch.envs.default.scripts]
test = 'pytest {args:tests}'
test-cov = 'pytest --cov-report=term-missing --cov-config=pyproject.toml --cov=src/twat_search --cov=tests {args:tests}'
type-check = 'mypy src/twat_search tests'
lint = [
    'ruff check src/twat_search tests',
    'ruff format src/twat_search tests',
]
[[tool.hatch.envs.all.matrix]]
python = [
    '3.10',
    '3.11',
    '3.12',
]

[tool.hatch.envs.lint]
detached = true
dependencies = [
    'mypy>=1.15.0',
    'ruff>=0.9.6',
    'absolufy-imports>=0.3.1',
    'pre-commit>=4.1.0',
    'pyupgrade>=3',
    'isort>=6.0.0',
]

[tool.hatch.envs.lint.scripts]
typing = 'mypy --install-types --non-interactive {args:src/twat_search tests}'
style = [
    'ruff check {args:.}',
    'ruff format {args:.}',
]
fmt = [
    'ruff format {args:.}',
    'ruff check --fix {args:.}',
]
all = [
    'style',
    'typing',
]

[tool.hatch.envs.test]
dependencies = ['.[test]']

[tool.hatch.envs.test.scripts]
test = 'python -m pytest -n auto {args:tests}'
test-cov = 'python -m pytest -n auto --cov-report=term-missing --cov-config=pyproject.toml --cov=src/twat_search --cov=tests {args:tests}'
bench = 'python -m pytest -v -p no:briefcase tests/test_benchmark.py --benchmark-only'
bench-save = 'python -m pytest -v -p no:briefcase tests/test_benchmark.py --benchmark-only --benchmark-json=benchmark/results.json'

[tool.ruff]
target-version = 'py310'
line-length = 120

[tool.ruff.lint]
select = [
    'A',
    'ARG',
    'B',
    'C',
    'DTZ',
    'E',
    'EM',
    'F',
    'FBT',
    'I',
    'ICN',
    'ISC',
    'N',
    'PLC',
    'PLE',
    'PLR',
    'PLW',
    'Q',
    'RUF',
    'S',
    'T',
    'TID',
    'UP',
    'W',
    'YTT',
]
ignore = [
    'ARG001',
    'ARG002',
    'ARG004',
    'ARG005',
    'B904',
    'C901',
    'DTZ005',
    'E501',
    'F401',
    'F811',
    'FBT001',
    'FBT002',
    'FBT003',
    'I001',
    'ISC001',
    'N803',
    'PLR0911',
    'PLR0912',
    'PLR0913',
    'PLR0915',
    'PLR2004',
    'S311',
    'S603',
    'S607',
    'T201',
]

[tool.ruff.lint.per-file-ignores]
"tests/*" = ['S101']

[tool.mypy]
python_version = '3.10'
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
[tool.coverage.run]
source_pkgs = [
    'twat_search',
    'tests',
]
branch = true
parallel = true
omit = ['src/twat_search/__about__.py']

[tool.coverage.paths]
twat_search = [
    'src/twat_search',
    '*/twat-search/src/twat_search',
]
tests = [
    'tests',
    '*/twat-search/tests',
]

[tool.coverage.report]
exclude_lines = [
    'no cov',
    'if __name__ == .__main__.:',
    'if TYPE_CHECKING:',
]
[tool.pytest.ini_options]
markers = ['''benchmark: marks tests as benchmarks (select with '-m benchmark')''']
addopts = '-v -p no:briefcase'
testpaths = ['tests']
python_files = ['test_*.py']
filterwarnings = [
    'ignore::DeprecationWarning',
    'ignore::UserWarning',
]
asyncio_mode = 'auto'

[tool.pytest-benchmark]
min_rounds = 100
min_time = 0.1
histogram = true
storage = 'file'
save-data = true
compare = [
    'min',
    'max',
    'mean',
    'stddev',
    'median',
    'iqr',
    'ops',
    'rounds',
]

[project]
name = 'twat-search'
dynamic = ['version']
description = 'Advanced search utilities and tools for the twat ecosystem'
readme = 'README.md'
requires-python = '>=3.10'
license = 'MIT'
keywords = [
    'twat',
    'search',
    'utilities',
    'text-search',
    'indexing',
]
classifiers = [
    'Development Status :: 4 - Beta',
    'Programming Language :: Python',
    'Programming Language :: Python :: 3.10',
    'Programming Language :: Python :: 3.11',
    'Programming Language :: Python :: 3.12',
    'Programming Language :: Python :: Implementation :: CPython',
    'Programming Language :: Python :: Implementation :: PyPy',
]
dependencies = [
    'twat>=1.8.1',
    'pydantic>=2.10.6',
    'pydantic-settings>=2.8.0',
    'httpx>=0.28.1',
    'python-dotenv>=1.0.1',
    'fire>=0.5.0',
    'rich>=13.6.0',
    'requests>=2.31.0',
]

[[project.authors]]
name = 'Adam Twardoch'
email = 'adam+github@twardoch.com'

[project.urls]
Documentation = 'https://github.com/twardoch/twat-search#readme'
Issues = 'https://github.com/twardoch/twat-search/issues'
Source = 'https://github.com/twardoch/twat-search'
[project.entry-points."twat.plugins"]
search = 'twat_search'

[project.optional-dependencies]
test = [
    'pytest>=8.3.4',
    'pytest-cov>=6.0.0',
    'pytest-xdist>=3.6.1',
    'pytest-benchmark[histogram]>=5.1.0',
    'pytest-asyncio>=0.25.3',
]
dev = [
    'pre-commit>=4.1.0',
    'ruff>=0.9.6',
    'mypy>=1.15.0',
    'absolufy-imports>=0.3.1',
    'pyupgrade>=3',
    'isort>=6.0.0',
]
brave = []
duckduckgo = ['duckduckgo-search>=7.5.0']
bing_scraper = ['scrape-bing>=0.1.2.1']
tavily = ['tavily-python>=0.5.0']
pplx = []
serpapi = ['serpapi>=0.1.5']
hasdata = []
google_scraper = ['googlesearch-python>=1.3.0']
searchit = ['searchit']
all = [
    'twat',
    'duckduckgo-search>=7.5.0',
    'scrape-bing>=0.1.2.1',
    'tavily-python>=0.5.0',
    'serpapi>=0.1.5',
    'googlesearch-python>=1.3.0',
    'searchit',
    'requests>=2.31.0',
]

[project.scripts]
twat-search = 'twat_search.__main__:main'
twat-search-web = 'twat_search.web.cli:main'

================
File: README.md
================
# Twat Search: multi-engine web search aggregator

## Executive summary

Twat Search is a powerful, asynchronous Python package that provides a unified interface to query multiple search engines simultaneously. It facilitates efficient information retrieval by aggregating, normalizing, and processing results from various search providers through a consistent API. This comprehensive documentation serves as a definitive guide for both CLI and Python usage of the package.

## Key features

- **Multi-Engine Search**: A single query can simultaneously search across multiple providers including Brave, Google (via SerpAPI/HasData), Tavily, Perplexity, You.com, Bing (via web scraping), and more
- **Asynchronous Operation**: Leverages `asyncio` for concurrent searches, maximizing speed and efficiency
- **Rate Limiting**: Built-in mechanisms to prevent exceeding API limits of individual search providers
- **Strong Typing**: Full type annotations and Pydantic validation for improved code reliability and maintainability
- **Robust Error Handling**: Custom exception classes for graceful error management
- **Flexible Configuration**: Configure search engines via environment variables, `.env` files, or directly in code
- **Extensible Architecture**: Designed for easy addition of new search engines
- **Command-Line Interface**: Rich, interactive CLI for searching and exploring engine configurations
- **JSON Output**: Supports JSON output for easy integration with other tools

## Installation options

### Full installation

```bash
uv pip install --system twat-search[all]
```

or 

```bash
uv pip install --system twat-search[all]
```


### Selective installation

Install only specific engine dependencies:

```bash
# Example: install only brave and duckduckgo dependencies
pip install "twat-search[brave,duckduckgo]"

# Example: install duckduckgo and bing scraper
pip install "twat-search[duckduckgo,bing_scraper]"
```

After installation, both `Twat Search` and `Twat Search-web` commands should be available in your PATH. Alternatively, you can run:

```bash
python -m twat_search.__main__
python -m twat_search.web.cli
```

## Quick start guide

### Python API

```python
import asyncio
from twat_search.web import search

async def main():
    # Search across all configured engines
    results = await search("quantum computing applications")

    # Print results
    for result in results:
        print(f"[{result.source}] {result.title}")
        print(f"URL: {result.url}")
        print(f"Snippet: {result.snippet}\n")

# Run the async function
asyncio.run(main())
```

### Command line interface

```bash
# Search using all available engines
Twat Search q "climate change solutions"

# Search with specific engines
Twat Search q "machine learning frameworks" --engines brave,tavily

# Get json output
Twat Search q "renewable energy" --json

# Use engine-specific command
Twat Search brave "web development trends" --count 10
```

## Core architecture

### Module structure

```
twat_search/
â””â”€â”€ web/
    â”œâ”€â”€ engines/            # Individual search engine implementations
    â”‚   â”œâ”€â”€ __init__.py     # Engine registration and availability checks
    â”‚   â”œâ”€â”€ base.py         # Base SearchEngine class definition
    â”‚   â”œâ”€â”€ brave.py        # Brave search implementation
    â”‚   â”œâ”€â”€ bing_scraper.py # Bing scraper implementation
    â”‚   â””â”€â”€ ...             # Other engine implementations
    â”œâ”€â”€ __init__.py         # Module exports
    â”œâ”€â”€ api.py              # Main search API
    â”œâ”€â”€ cli.py              # Command-line interface
    â”œâ”€â”€ config.py           # Configuration handling
    â”œâ”€â”€ exceptions.py       # Custom exceptions
    â”œâ”€â”€ models.py           # Data models
    â””â”€â”€ utils.py            # Utility functions
```

## Supported search engines

Twat Search provides a consistent interface to the following search engines:

| Engine | Module | API Key Required | Description | Package Extra |
| --- | --- | --- | --- | --- |
| Brave | `brave` | Yes | Web search via Brave Search API | `brave` |
| Brave News | `brave_news` | Yes | News search via Brave API | `brave` |
| You.com | `you` | Yes | Web search via You.com API | - |
| You.com News | `you_news` | Yes | News search via You.com API | - |
| Tavily | `tavily` | Yes | Research-focused search API | `tavily` |
| Perplexity | `pplx` | Yes | AI-powered search with detailed answers | `pplx` |
| SerpAPI | `serpapi` | Yes | Google search results via SerpAPI | `serpapi` |
| HasData Google | `hasdata-google` | Yes | Google search results via HasData API | `hasdata` |
| HasData Google Light | `hasdata-google-light` | Yes | Light version of HasData API | `hasdata` |
| Critique | `critique` | Yes | Visual and textual search capabilities | - |
| DuckDuckGo | `duckduckgo` | No | Privacy-focused search results | `duckduckgo` |
| Bing Scraper | `bing_scraper` | No | Web scraping of Bing search results | `bing_scraper` |

## Detailed usage guide

### Python API

#### The `search()` function

The core function for performing searches is `twat_search.web.search()` :

```python
from twat_search.web import search, Config

# Basic usage
results = await search("python async programming")

# Advanced usage with specific engines and parameters
results = await search(
    query="python async programming",
    engines=["brave", "tavily", "bing_scraper"],
    num_results=5,
    language="en",
    country="US",
    safe_search=True
)
```

Parameters:

- **`query`**: The search query string (required)
- **`engines`**: A list of engine names to use (e.g., `["brave", "tavily"]`). If `None` or empty, all configured engines will be used
- **`config`**: A `Config` object. If `None`, configuration is loaded from environment variables
- **`**kwargs`\*\*: Additional parameters passed to engines. These can be:
  - General parameters applied to all engines (e.g., `num_results=10`)
  - Engine-specific parameters with prefixes (e.g., `brave_count=20`, `tavily_search_depth="advanced"`)

#### Engine-specific functions

Each engine provides a direct function for individual access:

```python
from twat_search.web.engines.brave import brave
from twat_search.web.engines.bing_scraper import bing_scraper

# Using brave search
brave_results = await brave(
    query="machine learning tutorials",
    count=10,
    country="US",
    safe_search=True
)

# Using bing scraper (no api key required)
bing_results = await bing_scraper(
    query="data science projects",
    num_results=10,
    max_retries=3,
    delay_between_requests=1.0
)
```

#### Working with search results

The `SearchResult` model provides a consistent structure across all engines:

```python
from twat_search.web.models import SearchResult
from pydantic import HttpUrl

# Creating a search result
result = SearchResult(
    title="Example Search Result",
    url=HttpUrl("https://example.com"),
    snippet="This is an example search result snippet...",
    source="brave",
    raw={"original_data": "from_engine"}  # Optional raw data
)

# Accessing properties
print(result.title)    # "Example Search Result"
print(result.url)      # "https://example.com/"
print(result.source)   # "brave"
print(result.snippet)  # "This is an example search result snippet..."
```

### Command line interface

The CLI provides convenient access to all search engines through the `Twat Search` command.

#### General search command

```bash
Twat Search q <query> [options]
```

Common options:

- `--engines <engine1,engine2,...>`: Specify engines to use
- `--num_results <n>`: Number of results to return
- `--country <country_code>`: Country to search in (e.g., "US", "GB")
- `--language <lang_code>`: Language to search in (e.g., "en", "es")
- `--safe_search <true|false>`: Enable or disable safe search
- `--json`: Output results in JSON format
- `--verbose`: Enable verbose logging

Engine-specific parameters can be passed with `--<engine>_<param> <value>` , for example:

```bash
Twat Search q "machine learning" --brave_count 15 --tavily_search_depth advanced
```

#### Engine information command

```bash
Twat Search info [engine_name] [--json]
```

- Shows information about available search engines
- If `engine_name` is provided, shows detailed information about that engine
- The `--json` flag outputs in JSON format

#### Engine-specific commands

Each engine has a dedicated command for direct access:

```bash
# Brave search
Twat Search brave "web development trends" --count 10

# Duckduckgo search
Twat Search duckduckgo "privacy tools" --max_results 5

# Bing scraper
Twat Search bing_scraper "python tutorials" --num_results 10

# Critique with image
Twat Search critique --image-url "https://example.com/image.jpg" "Is this image real?"
```

## Configuration management

### Environment variables

Configure engines using environment variables:

```bash
# Api keys
BRAVE_API_KEY=your_brave_api_key
TAVILY_API_KEY=your_tavily_api_key
PERPLEXITY_API_KEY=your_perplexity_api_key
YOU_API_KEY=your_you_api_key
SERPAPI_API_KEY=your_serpapi_api_key
CRITIQUE_API_KEY=your_critique_api_key
HASDATA_API_KEY=your_hasdata_api_key

# Engine enablement
BRAVE_ENABLED=true
TAVILY_ENABLED=true
PERPLEXITY_ENABLED=true
YOU_ENABLED=true
SERPAPI_ENABLED=true
CRITIQUE_ENABLED=true
DUCKDUCKGO_ENABLED=true
BING_SCRAPER_ENABLED=true
HASDATA_GOOGLE_ENABLED=true

# Default parameters (json format)
BRAVE_DEFAULT_PARAMS={"count": 10, "safesearch": "off"}
TAVILY_DEFAULT_PARAMS={"max_results": 5, "search_depth": "basic"}
PERPLEXITY_DEFAULT_PARAMS={"model": "pplx-7b-online"}
YOU_DEFAULT_PARAMS={"safe_search": true, "count": 8}
SERPAPI_DEFAULT_PARAMS={"num": 10, "gl": "us"}
HASDATA_GOOGLE_DEFAULT_PARAMS={"location": "Austin,Texas,United States", "device_type": "desktop"}
DUCKDUCKGO_DEFAULT_PARAMS={"max_results": 10, "safesearch": "moderate", "time": "d"}
BING_SCRAPER_DEFAULT_PARAMS={"max_retries": 3, "delay_between_requests": 1.0}

# Global default for all engines
NUM_RESULTS=5
```

You can store these in a `.env` file in your project directory, which will be automatically loaded by the library using `python-dotenv` .

### Programmatic configuration

Configure engines programmatically when using the Python API:

```python
from twat_search.web import Config, EngineConfig, search

# Create custom configuration
config = Config(
    engines={
        "brave": EngineConfig(
            api_key="your_brave_api_key",
            enabled=True,
            default_params={"count": 10, "country": "US"}
        ),
        "bing_scraper": EngineConfig(
            enabled=True,
            default_params={"max_retries": 3, "delay_between_requests": 1.0}
        ),
        "tavily": EngineConfig(
            api_key="your_tavily_api_key",
            enabled=True,
            default_params={"search_depth": "advanced"}
        )
    }
)

# Use the configuration
results = await search("quantum computing", config=config)
```

## Engine-specific parameters

Each search engine accepts different parameters. Here's a reference for commonly used ones:

### Brave search

```python
await brave(
    query="search term",
    count=10,              # Number of results (default: 10)
    country="US",          # Country code (ISO 3166-1 alpha-2)
    search_lang="en",      # Search language
    ui_lang="en",          # UI language
    safe_search=True,      # Safe search (True/False)
    freshness="day"        # Time frame (day, week, month)
)
```

### Bing scraper

```python
await bing_scraper(
    query="search term",
    num_results=10,                # Number of results
    max_retries=3,                 # Maximum retry attempts
    delay_between_requests=1.0     # Delay between requests (seconds)
)
```

### Tavily

```python
await tavily(
    query="search term",
    max_results=5,               # Number of results (default: 5)
    search_depth="basic",        # Search depth (basic, advanced)
    include_domains=["example.com"],  # Domains to include
    exclude_domains=["spam.com"],     # Domains to exclude
    include_answer=True,         # Include AI-generated answer
    search_type="search"         # Search type (search, news, etc.)
)
```

### Perplexity (pplx)

```python
await pplx(
    query="search term",
    model="pplx-70b-online"      # Model to use for search
)
```

### You.com

```python
await you(
    query="search term",
    num_results=10,              # Number of results
    country_code="US",           # Country code
    safe_search=True             # Safe search (True/False)
)
```

### Duckduckgo

```python
await duckduckgo(
    query="search term",
    max_results=10,              # Number of results
    region="us-en",              # Region code
    safesearch=True,             # Safe search (True/False)
    timelimit="m",               # Time limit (d=day, w=week, m=month)
    timeout=10                   # Request timeout (seconds)
)
```

### Critique (with image)

```python
await critique(
    query="Is this image real?",
    image_url="https://example.com/image.jpg",  # URL to image
    # OR
    image_base64="base64_encoded_image_data",   # Base64 encoded image
    source_whitelist=["trusted-site.com"],      # Optional domain whitelist
    source_blacklist=["untrusted-site.com"],    # Optional domain blacklist
    output_format="text"                        # Output format
)
```

## Error handling framework

Twat Search provides custom exception classes for proper error handling:

```python
from twat_search.web.exceptions import SearchError, EngineError

try:
    results = await search("quantum computing")
except EngineError as e:
    print(f"Engine-specific error: {e}")
    # e.g., "Engine 'brave': API key is required"
except SearchError as e:
    print(f"General search error: {e}")
    # e.g., "No search engines configured"
```

The exception hierarchy:

- `SearchError`: Base class for all search-related errors
- `EngineError`: Subclass for engine-specific errors, includes the engine name in the message

Typical error scenarios:

- Missing API keys
- Network errors
- Rate limiting
- Invalid responses
- Configuration errors

## Advanced usage techniques

### Concurrent searches

Search across multiple engines concurrently:

```python
import asyncio
from twat_search.web.engines.brave import brave
from twat_search.web.engines.tavily import tavily

async def search_multiple(query):
    brave_task = brave(query)
    tavily_task = tavily(query)

    results = await asyncio.gather(brave_task, tavily_task, return_exceptions=True)

    brave_results, tavily_results = [], []
    if isinstance(results[0], list):
        brave_results = results[0]
    if isinstance(results[1], list):
        tavily_results = results[1]

    return brave_results + tavily_results

# Usage
results = await search_multiple("artificial intelligence")
```

### Custom engine parameters

Specify engine-specific parameters in the unified search function:

```python
from twat_search.web import search

results = await search(
    "machine learning",
    engines=["brave", "tavily", "bing_scraper"],
    # Common parameters
    num_results=10,
    country="US",

    # Engine-specific parameters
    brave_count=15,
    brave_freshness="week",
    tavily_search_depth="advanced",
    bing_scraper_max_retries=5
)
```

### Rate limiting

Use the built-in rate limiter to avoid hitting API limits:

```python
from twat_search.web.utils import RateLimiter

# Create a rate limiter with 5 calls per second
limiter = RateLimiter(calls_per_second=5)

# Use in an async context
async def rate_limited_search():
    for query in ["python", "javascript", "rust", "golang"]:
        limiter.wait_if_needed()  # Wait if necessary
        results = await search(query)
        # Process results...
```

## Development guide

### Running tests

```bash
# Install test dependencies
pip install "twat-search[test]"

# Run tests
pytest

# Run with coverage
pytest --cov=src/twat_search

# Run tests in parallel
pytest -n auto
```

### Adding a new search engine

To add a new search engine:

1. Create a new file in `src/twat_search/web/engines/`
2. Implement a class that inherits from `SearchEngine`
3. Implement the required methods and register the engine

Example:

```python
from pydantic import HttpUrl
from twat_search.web.engines.base import SearchEngine, register_engine
from twat_search.web.models import SearchResult
from twat_search.web.config import EngineConfig


@register_engine
class MyNewSearchEngine(SearchEngine):
    engine_code = "my_new_engine"
    env_api_key_names = ["MY_NEW_ENGINE_API_KEY"]

    def __init__(self, config: EngineConfig, **kwargs) -> None:
        super().__init__(config, **kwargs)
        # Initialize engine-specific parameters

    async def search(self, query: str) -> list[SearchResult]:
        # Implement search logic
        return [
            SearchResult(
                title="My Result",
                url=HttpUrl("https://example.com"),
                snippet="Result snippet",
                source=self.name
            )
        ]


# Convenience function
async def my_new_engine(query: str, **kwargs):
# Implement convenience function
# ...
```

### Development setup

To contribute to `Twat Search` , follow these steps:

1. Clone the repository:

```bash
   git clone https://github.com/twardoch/Twat Search.git
   cd Twat Search
```

2. Set up the virtual environment with `uv`:

```bash
   uv venv
   source .venv/bin/activate
```

3. Install development dependencies:

```bash
   uv pip install -e ".[test,dev]"
```

4. Run tests:

```bash
   uv run pytest
```

5. Run type checking:

```bash
   uv run mypy src tests
```

6. Run linting:

```bash
   uv run ruff check src tests
```

7. Use `cleanup.py` for project maintenance:

```bash
   python cleanup.py status
```

## Troubleshooting guide

### Api key issues

If you're encountering API key errors:

1. Verify the API key is set correctly in environment variables
2. Check the API key format is valid for the specific provider
3. Ensure the API key has the necessary permissions
4. For engines that require API keys, verify the key is set via one of these methods:
   - Environment variable (e.g., `BRAVE_API_KEY` )
   - `.env` file
   - Programmatic configuration

### Rate limiting problems

If you're being rate limited by search providers:

1. Reduce the number of concurrent requests
2. Use the `RateLimiter` utility to space out requests
3. Consider upgrading your API plan with the provider
4. Add delay between requests for engines that support it (e.g., `delay_between_requests` for Bing Scraper)

### No results returned

If you're not getting results:

1. Check that the engine is enabled (`ENGINE_ENABLED=true`)
2. Verify your query is not empty or too restrictive
3. Try with safe search disabled to see if content filtering is the issue
4. Check for engine-specific errors in the logs (use `--verbose` flag with CLI)
5. Ensure you have the required dependencies installed for the engine

### Common error messages

- `"Engine 'X': API key is required"`: The engine requires an API key that hasn't been configured
- `"No search engines configured"`: No engines are enabled or available
- `"Unknown search engine: X"`: The specified engine name is invalid
- `"Engine 'X': is disabled"`: The engine is registered but disabled in configuration

## Development status

Version: 1.8.1

Twat Search is actively developed. See [PROGRESS.md](PROGRESS.md) for completed tasks and [TODO.md](TODO.md) for planned features and improvements.

## Contributing

Contributions are welcome! Please check [TODO.md](TODO.md) for areas that need work. Submit pull requests or open issues on GitHub. Key areas for contribution:

- Adding new search engines
- Improving test coverage
- Enhancing documentation
- Optimizing performance
- Implementing advanced features (e.g., caching, result normalization)

## License

Twat Search is released under the MIT License. See the [LICENSE](LICENSE) file for details.

---

## Appendix: available engines and requirements

| Engine | Package Extra | API Key Required | Environment Variable | Notes |
| --- | --- | --- | --- | --- |
| Brave | `brave` | Yes | `BRAVE_API_KEY` | General web search engine |
| Brave News | `brave` | Yes | `BRAVE_API_KEY` | News-specific search |
| You.com | - | Yes | `YOU_API_KEY` | AI-powered web search |
| You.com News | - | Yes | `YOU_API_KEY` | News-specific search |
| Tavily | `tavily` | Yes | `TAVILY_API_KEY` | Research-focused search |
| Perplexity | `pplx` | Yes | `PPLX_API_KEY` | AI-powered search with detailed answers |
| SerpAPI | `serpapi` | Yes | `SERPAPI_API_KEY` | Google search results API |
| HasData Google | `hasdata` | Yes | `HASDATA_API_KEY` | Google search results API |
| HasData Google Light | `hasdata` | Yes | `HASDATA_API_KEY` | Lightweight Google search API |
| Critique | - | Yes | `CRITIQUE_API_KEY` | Supports image analysis |
| DuckDuckGo | `duckduckgo` | No | - | Privacy-focused search |
| Bing Scraper | `bing_scraper` | No | - | Uses web scraping techniques |

================
File: TODO.md
================
--- 
this_file: TODO.md
--- 

# twat-search 

Tip: Periodically run `./cleanup.py status` to see results of lints and tests.

Edit the detailed plan in this file @TODO.md. Based on the plan, write an itemized list of tasks in @PROGRESS.md and then, as you work, check off your @PROGRESS.md.  

## 1. Progress Update (2025-02-27)

### 1.1. 1.1 Completed Tasks

We have made significant progress in fixing several critical issues:

1. Enhanced the `base.SearchEngine` class:
   - Fixed API key handling with proper validation
   - Improved error handling in HTTP requests with retries and better error messages
   - Implemented type annotations to fix incompatible type errors
   - Added proper documentation

2. Updated engine implementations:
   - Fixed You.com engines (`you` and `you_news`) with proper API key handling
   - Fixed Perplexity (`pplx`) implementation with explicit string conversion in error messages
   - Fixed Ruff code style issues across multiple files

3. Fixed linter issues:
   - Replaced assert statements with conditional checks
   - Fixed string literals in exceptions
   - Added proper type annotations

### 1.2. Urgent TODOs

Based on testing all engines with the command:
```
for engine in $(twat-search web info --plain); do echo; echo; echo; echo ">>> $engine"; twat-search web q -e $engine "Adam Twardoch" -n 1; done;
```

We've identified the following critical issues that need urgent attention:

#### 1.2.1. Result Limiting Not Working
Even with `-n 1` parameter, multiple results are returned for working engines. This needs to be fixed to ensure the `num_results` parameter is respected.

#### 1.2.2. Engine Initialization Failures
Several engines fail to initialize with error about missing `get_num_results` method:
- `brave` - Object has no attribute 'get_num_results'
- `brave_news` - Object has no attribute 'get_num_results'
- `critique` - Initialization failure
- `pplx` - Fails to initialize (likely API key related)
- `you` - Fails to initialize (likely API key related)
- `you_news` - Fails to initialize (likely API key related)

#### 1.2.3. Empty Results from SearchIt-based Engines
These engines return empty results consistently:
- `bing_searchit`
- `google_searchit`
- `qwant_searchit`
- `yandex_searchit`

#### 1.2.4. Unwanted Result Combination
Some engines are incorrectly combining results from multiple sources:
- `google_hasdata` - Returns its own results plus results from other engines
- `google_hasdata_full` - Returns its own results plus results from other engines

#### 1.2.5. Working Engines
The following engines are working, but have the issue with not respecting `num_results`:
- `bing_scraper`
- `duckduckgo`
- `google_scraper`
- `google_serpapi`
- `tavily`

### 1.3. `brave`

twat-search web q "Adam Twardoch" -n 1 --verbose --json -e brave

[02/26/25 20:51:35] DEBUG    Using num_results=5                                                                                                            cli.py:295
                    DEBUG    Using selector: KqueueSelector                                                                                      selector_events.py:64
                    DEBUG    Attempting to search with engines: ['brave']                                                                                   cli.py:171
                    DEBUG    Search requested with num_results=5                                                                                            api.py:148
                    DEBUG    Initializing engine 'brave' with num_results=5                                                                                  api.py:71
                    WARNING  Failed to initialize engine 'brave': EngineError                                                                                api.py:92
                    ERROR    Error initializing engine 'brave': Engine 'brave': Failed to initialize engine 'brave': 'BraveSearchEngine' object has no       api.py:95
                             attribute 'get_num_results'                                                                                                              
                    WARNING  Failed to initialize engine 'brave': Engine 'brave': Failed to initialize engine 'brave': 'BraveSearchEngine' object has no    api.py:184
                             attribute 'get_num_results'                                                                                                              
                    WARNING  Failed to initialize engines: brave                                                                                            api.py:191
                    ERROR    No search engines could be initialized from requested engines: brave                                                           api.py:199
                    ERROR    Search failed: No search engines could be initialized from requested engines: brave                                            api.py:246
                    ERROR    Search failed: No search engines could be initialized from requested engines: brave                                            cli.py:177
                            âŒ Search Errors                            
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Error                                                                â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ No search engines could be initialized from requested engines: brave â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
{}


### 1.4. `brave_news`

```
twat-search web q "Adam Twardoch" -n 1 --verbose --json -e brave_news
```

[02/26/25 20:52:52] DEBUG    Using num_results=5                                                                                                            cli.py:295
                    DEBUG    Using selector: KqueueSelector                                                                                      selector_events.py:64
                    DEBUG    Attempting to search with engines: ['brave_news']                                                                              cli.py:171
                    DEBUG    Search requested with num_results=5                                                                                            api.py:148
                    DEBUG    Initializing engine 'brave_news' with num_results=5                                                                             api.py:71
                    WARNING  Failed to initialize engine 'brave_news': EngineError                                                                           api.py:92
                    ERROR    Error initializing engine 'brave_news': Engine 'brave_news': Failed to initialize engine 'brave_news': 'BraveNewsSearchEngine'  api.py:95
                             object has no attribute 'get_num_results'                                                                                                
                    WARNING  Failed to initialize engine 'brave_news': Engine 'brave_news': Failed to initialize engine 'brave_news':                       api.py:184
                             'BraveNewsSearchEngine' object has no attribute 'get_num_results'                                                                        
                    WARNING  Failed to initialize engines: brave_news                                                                                       api.py:191
                    ERROR    No search engines could be initialized from requested engines: brave_news                                                      api.py:199
                    ERROR    Search failed: No search engines could be initialized from requested engines: brave_news                                       api.py:246
                    ERROR    Search failed: No search engines could be initialized from requested engines: brave_news                                       cli.py:177
                              âŒ Search Errors                               
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Error                                                                     â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ No search engines could be initialized from requested engines: brave_news â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
{}

### 1.5. 1.8 Further TODOs

Several issues still need immediate attention:

1. Fix failing tests - The test suite has 5 failing tests out of 49:
   - Tests in `test_api.py` related to search functionality
   - Tests in `test_config.py` about configuration expectations
   - A test in `test_bing_scraper.py` for the convenience function

2. Address inconsistent `num_results` parameter handling in all engines

3. Fix `searchit`-based engines that are returning empty results

## 2. Project Status

The basic implementation of the `twat-search` web package is complete. This multi-provider search tool now supports multiple search engines and has a functional CLI interface.

## 3. Problem case

This is the test command that I'm targeting: 

```bash
for engine in $(twat-search web info --plain); do echo; echo; echo; echo ">>> $engine"; twat-search web q -e $engine "Adam Twardoch" -n 1 --json --verbose; done;
```

## 4. Proposed solution

Okay, let's synthesize all the provided information into a single, comprehensive development guide for the `twat-search` project.  This guide will combine the problem statement, the detailed action plan (including code samples), the testing strategy, and the documentation considerations into a cohesive document. It's structured to be a practical guide for someone working on improving this project.

## 5. Twat Search Development Guide

This guide details the current state, problems, and a plan for refactoring and improving the `twat-search` web package.

### 5.1. Project Overview

`twat-search` is an asynchronous Python package designed to query multiple search engines simultaneously, aggregate their results, and present them through a unified API and command-line interface (CLI). It aims for:

*   **Multi-Engine Support:**  Integrate with various search providers (Brave, Google, Tavily, etc.).
*   **Asynchronous Operation:** Utilize `asyncio` for concurrent requests and improved performance.
*   **Configurability:** Allow users to configure engines, API keys, and search parameters via environment variables or configuration files.
*   **Extensibility:**  Make it easy to add new search engine integrations.
*   **Robustness:**  Handle errors gracefully, including API limits and network issues.
*   **Consistency:**  Provide a consistent JSON output format for all engines.

### 5.2. Current Status (as of 2025-02-27)

The basic implementation is complete, with several engines integrated and a functional CLI. Major improvements have been made to the base `SearchEngine` class and error handling. However, there are still issues with test failures and some engines returning empty results that need to be addressed.

### 5.3. Identified Problems

The following problems have been identified through testing and code review:

1.  **Test Failures:** Several tests are failing, particularly in `test_api.py`, `test_config.py`, and `test_bing_scraper.py`. These need to be fixed to ensure the code is working as expected.

2.  **Inconsistent `num_results` Handling:**  The `num_results` parameter, intended to control the number of results returned, is not consistently respected by all engines.

3.  **Empty Results from Some Engines:** The `searchit`-based engines (and potentially others) consistently return empty result sets, indicating a problem with their integration or underlying libraries.

4.  **Inconsistent JSON Output:** Different engines return results with varying key names and structures, making it difficult to process results uniformly. Example: Typos in keys like  `"snippetHighlitedWords"`.

5.  **Code Duplication and Inconsistency in `base.SearchEngine`:**  The base class contains duplicated logic for handling HTTP requests, retries, and parameter validation.  This should be centralized.

6. **Anywebsearch:** Remove all `anywebsearch` engines.

7. **Searchit:** Review the implementation of the `searchit` engines, or remove them.

### 5.4. Action Plan and Implementation Guide

This section provides a detailed, step-by-step plan to address the identified problems.

#### 5.4.1. Fixing Critical Engine Failures

**Goal:** Ensure all engines initialize correctly, handling API key requirements gracefully.

**4.1.1.  Enforce API Key Requirements in `base.SearchEngine`**

*   **File:** `src/twat_search/web/engines/base.py`
*   **Change:**  Modify the `SearchEngine` base class to *require* subclasses to define `env_api_key_names` as an abstract property. Remove the class-level default.

    ```python
    import abc
    import asyncio
    import random  # Import the random module
    import httpx
    from pydantic import BaseModel, HttpUrl, field_validator, ValidationError
    from typing import Any, Optional
    import logging
    from twat_search.web.exceptions import SearchError, EngineError
    from twat_search.web.config import EngineConfig
    from twat_search.web.models import SearchResult
    from twat_search.web.engine_constants import (
        USER_AGENTS,
        ENGINE_FRIENDLY_NAMES,
        standardize_engine_name,
    )

    logger = logging.getLogger(__name__)


    class SearchEngine(abc.ABC):
        engine_code: str  # REQUIRED: Engine code for config and CLI

        # Should not be a class variable, should be an instance
        # and a required property.
        @property
        @abc.abstractmethod
        def env_api_key_names(self) -> list[str]:
            """List of environment variable names that can hold the API key."""
            ...  # raise NotImplementedError("Subclasses must define env_api_key_names")

        @property
        @abc.abstractmethod
        def engine_code(self) -> str:
            """
            The engine code should be a short, unique identifier for the engine,
            using only lowercase letters and underscores.
            """
            ...


        def __init__(self, config: EngineConfig, **kwargs: Any) -> None:
            self.config = config
            # Prioritize kwargs, then config.default_params, then fallbacks.
            self.num_results = self._get_num_results()  # Use the new method.
            self.country = kwargs.get("country") or self.config.default_params.get("country")
            self.language = kwargs.get("language") or self.config.default_params.get("language")
            self.safe_search = kwargs.get("safe_search", self.config.default_params.get("safe_search", True))  # type: ignore
            self.time_frame = kwargs.get("time_frame") or self.config.default_params.get("time_frame")
            self.timeout = kwargs.get("timeout", self.config.default_params.get("timeout", 10))
            self.retries = kwargs.get("retries", self.config.default_params.get("retries", 2))
            self.retry_delay = kwargs.get("retry_delay", self.config.default_params.get("retry_delay", 1.0))
            self.use_random_user_agent = kwargs.get("use_random_user_agent", self.config.default_params.get("use_random_user_agent", True))
            # Store original kwargs for use in subclasses
            self.kwargs = kwargs


            # Check for API key if required
            if self.env_api_key_names:  # Use the property, not the class variable
                api_key = config.api_keys.get(self.engine_code)
                if not api_key:
                  msg = (
                        f"API key is required for {self.engine_code}. "
                        f"Please set it via one of these env vars: {', '.join(self.env_api_key_names)}" # Use property
                    )
                  logger.error(msg)
                  raise EngineError(self.engine_code, msg)  # Use consistent EngineError
                self.api_key = api_key # Assign it to the object if it passes.
            else:
              self.api_key = None



        def _get_num_results(self, param_name: str = "num_results", min_value: int = 1) -> int:
            # Get value from kwargs
            value = self.kwargs.get(param_name)
            if value is not None:
                try:
                    return max(min_value, int(value))
                except (TypeError, ValueError):
                    logger.warning(
                        f"Invalid value for '{param_name}' ({value!r}) in {self.engine_code}, using default."
                    )
            #If not in kwargs get from config
            default = self.config.default_params.get(
                param_name
            ) or self.config.default_params.get("num_results") # num_results as fallback
            if default is not None:
                try:
                    return max(min_value, int(default))
                except (TypeError, ValueError):
                    logger.warning(
                        f"Invalid value for '{param_name}' ({value!r}) in {self.engine_code}, using default."
                    )
            return self.num_results



        async def make_http_request(
            self, method, url, headers=None, **kwargs
        ) -> httpx.Response: # return the full httpx.Response object
            headers = headers or {}
            if self.use_random_user_agent and "user-agent" not in {k.lower() for k in headers}:
                headers["User-Agent"] = random.choice(USER_AGENTS)

            delay = self.retry_delay #initiate delay
            for attempt in range(1, self.retries + 2):  # One extra attempt
                try:
                    async with httpx.AsyncClient(timeout=self.timeout) as client:

                        response = await client.request(
                            method, url, headers=headers, **kwargs
                        )
                        response.raise_for_status()  # Raise for status here
                        return response # return response

                except httpx.HTTPError as e:
                    if attempt > self.retries:  # Use  self.retries
                        # Final retry failed.
                        logger.error(f"Request failed after {self.retries} attempts: {e}")
                        raise EngineError(self.engine_code, f"HTTP request failed after multiple retries: {e}") from e
                    # exponential backoff with jitter
                    jitter = random.uniform(0.5, 1.5)  # Add jitter
                    actual_retry_delay = delay * jitter
                    logger.warning(f"Request failed (attempt {attempt}), retrying in {actual_retry_delay:.2f} seconds: {e}")
                    await asyncio.sleep(actual_retry_delay)
                    delay *= 2  # Exponential backoff

        @abc.abstractmethod
        async def search(self, query: str) -> list[SearchResult]:
            """Perform the search and return a list of SearchResult objects."""
            ...

    _engine_registry = {} # Move to the bottom, after the class definition


    def register_engine(engine_class: type[SearchEngine]) -> type[SearchEngine]:
        try:
          # Check that the engine class has required attributes.
          if not hasattr(engine_class, "engine_code"):
              raise AttributeError("Engine must define an 'engine_code' attribute.")

          # Now using property, and check for its presence.
          if not hasattr(engine_class, "env_api_key_names") or not callable(getattr(engine_class, 'env_api_key_names', None)):
            raise AttributeError("Engine must define a 'env_api_key_names' property.")

          standardized_name = standardize_engine_name(engine_class.engine_code)
          # Check for collisions in engine codes
          if standardized_name in _engine_registry:
                raise ValueError(f"An engine with code '{standardized_name}' is already registered.")
          _engine_registry[standardized_name] = engine_class
          return engine_class
        except Exception as e:
            logger.warning(f"Failed to register engine {engine_class}: {e}")
            return None # Ensure it is not treated as registed.




    def get_engine(engine_name: str, config: EngineConfig, **kwargs: Any) -> SearchEngine:
        engine_class = _engine_registry.get(engine_name)
        if engine_class is None:
            raise EngineError(engine_name, f"Unknown search engine: '{engine_name}'.  Available engines: {', '.join(sorted(_engine_registry.keys()))}") # Improved error message.
        if not config.enabled:
            raise EngineError(engine_name, f"Search engine '{engine_name}' is disabled.")

        return engine_class(config, **kwargs)

    def get_registered_engines() -> dict[str, type[SearchEngine]]:
        return _engine_registry.copy()
    ```
    **Key changes:**

    *   `@abc.abstractmethod` is used for `env_api_key_names` and the new `_get_num_results`. This *forces* subclasses to implement them.
    *   `engine_code` validation added.
    *   A `ValueError` is raised if `engine_code` or the correct `env_api_key_names` are not provided. This prevents silent failures.
    *   `get_engine` now raises a more descriptive `EngineError` if the engine isn't found or is disabled.
    *   `make_http_request` centralizes request logic, including retries, user-agent randomization, and exception handling.  It now returns the full `httpx.Response` object, giving the calling engine more control.
    *  The num_results is validated so that if an invalid value is passed it will take the default value.
    * `_get_num_results` includes a fallback using the general `num_results`.

**4.1.2.  Enhance `config.py`**

*   **File:** `src/twat_search/web/config.py`
*   **Change:** Modify `EngineConfig` to perform validation and require an API key if the engine specifies `env_api_key_names`.

```python
# In src/twat_search/web/config.py
from pydantic import BaseModel, Field, field_validator, ValidationError
import os
import json
import logging
from pathlib import Path

load_dotenv()
logger = logging.getLogger(__name__)

# ... (rest of your config.py) ...

class EngineConfig(BaseModel):
    api_key: str | None = Field(default=None) # Optional in general
    enabled: bool = Field(default=False)
    default_params: dict[str, Any] = Field(default_factory=dict)

    @field_validator("api_key", mode="before")
    def check_api_key(cls, v: str | None, values: ValidationInfo) -> str:
        # print("check_api_key", v, values)
        # if engine has api key names but no key, raise an error
        engine_code = values.data.get('engine_code') # get it from data to avoid errors
        if engine_code is None:
            return v

        try:
            engine_class = get_registered_engines()[standardize_engine_name(engine_code)]
        except KeyError:
            # if engine is not registed yet, we can't check for api_key
            return v

        if hasattr(engine_class, "env_api_key_names"):
            for key_name in engine_class.env_api_key_names:
                if key := os.environ.get(key_name):
                    return key

            # if none is set
            if v is None: # and also no api_key direct value set
                    raise ValueError(
                    f"Engine '{engine_code}' requires an API key. "
                    f"Please set one of these environment variables: {', '.join(engine_class.env_api_key_names)}"
                )

        return v

#keep Config class as is.
class Config(BaseModel):
    engines: dict[str, EngineConfig] = Field(default_factory=dict)
    # ... (rest of your Config class) ...

#Rest of file is ok.
```

**Key changes:**

*   **`@field_validator("api_key", mode="before")`:**  This validator runs *before* the `api_key` field is assigned.  This is crucial because we need to check if the key is required *before* Pydantic's default value logic kicks in.  We use `values.data.get('engine_code')` to safely access the engine code *during* validation, even if it hasn't been fully assigned yet.
*   **`get_registered_engines()`:** We call the `get_registered_engines()` function to access the registry.
*   **`KeyError` Handling:** The code now includes a `try...except KeyError` block. This is important.  During the *initial* loading of the configuration, the `EngineConfig` for an engine might be created *before* the engine class itself is registered.  In this case, looking up the engine class in `_engine_registry` would raise a `KeyError`.  The `try...except` handles this gracefully, skipping the API key check if the engine isn't registered yet.  This prevents a chicken-and-egg problem during startup.
* **Clear Error Message:**  The `ValueError` now provides a very specific message, telling the user *exactly* which environment variable(s) they need to set.
* No longer need for the map with environment variables.

**4.1.3. Update Engine Implementations**

*   **Files:**  `src/twat_search/web/engines/you.py`, `src/twat_search/web/engines/pplx.py`, `src/twat_search/web/engines/serpapi.py` (and any other engine requiring an API key)
*   **Change:**  Define the `env_api_key_names` correctly. Remove any existing API key handling from the `__init__` method (it's now handled by the base class and config).

    ```python
    # Example: src/twat_search/web/engines/you.py
    @register_engine
    class YouSearchEngine(YouBaseEngine):
        engine_code = "you"
        @property
        def env_api_key_names(self) -> list[str]:
          return ["YOU_API_KEY"]
        # ... rest of the engine implementation ...

    # Example: src/twat_search/web/engines/pplx.py
    @register_engine
    class PerplexitySearchEngine(SearchEngine):
      engine_code = "pplx"
      @property
      def env_api_key_names(self) -> list[str]:
        return ["PERPLEXITY_API_KEY"] # Or PERPLEXITYAI_API_KEY if that's correct
    # ...
    #Example: src/twat_search/web/engines/serpapi.py
    @register_engine
    class SerpApiSearchEngine(SearchEngine):
        engine_code = "serpapi"

        @property
        def env_api_key_names(self) -> list[str]:
            return ["SERPAPI_API_KEY"]
    ```
    Also in each class, make sure the `engine_code` is correct, and `_get_num_results` is defined.

**4.1.4. Improve `get_engine`**

Already addressed in 4.1.1.

**4.1.5 Test Engine Initialization**

*   **File:**  `tests/unit/web/engines/test_base.py` (and potentially engine-specific test files)
*   **Change:** Add tests to cover the API key requirement and engine availability:

    ```python
    # Inside test_base.py or a new test file.
    import pytest
    from twat_search.web.engines.base import get_engine, SearchEngine, register_engine, EngineError
    from twat_search.web.config import EngineConfig

    @register_engine
    class MockEngine(SearchEngine):
        engine_code = "mock_engine"
        @property
        def env_api_key_names(self):
          return ["MOCK_API_KEY"]

        async def search(self, query: str) -> list:
            return []

    def test_get_engine_missing_api_key(monkeypatch):
        monkeypatch.delenv("MOCK_API_KEY", raising=False)  # Ensure it's not set
        config = EngineConfig(enabled=True)  # No API key provided
        with pytest.raises(EngineError) as excinfo:
            get_engine("mock_engine", config)
        assert "MOCK_API_KEY" in str(excinfo.value) # Check for expected message

    def test_get_engine_api_key_present(monkeypatch):
        monkeypatch.setenv("MOCK_API_KEY", "test_key")
        config = EngineConfig(enabled=True)
        engine = get_engine("mock_engine", config) # Should not raise error
        assert engine.api_key == "test_key"  # Access api_key (if needed)

    def test_get_engine_disabled(monkeypatch):
      monkeypatch.delenv("MOCK_API_KEY", raising=False)  # Ensure it's not set
      config = EngineConfig(enabled=False)
      with pytest.raises(EngineError):
          get_engine("mock_engine", config)

    ```

**4.2. Data Integrity and Consistency**

**4.2.1. `num_results` Implementation**

*   **`base.SearchEngine`:** Already addressed above (abstract `_get_num_results` method).
*   **Engine-Specific Implementation (Example: `bing_scraper.py`):**

    ```python

    class BingScraperSearchEngine(SearchEngine):
        engine_code = "bing_scraper" # No API key
        @property
        def env_api_key_names(self):
          return []

        def __init__(self, config: EngineConfig, **kwargs: Any) -> None:
          super().__init__(config, **kwargs)
          self.max_results = self._get_num_results()

        #... rest of your BingScraperSearchEngine code ...

    ```
    Make sure `self.max_results` is used to limit the results in the `search` method:
    ```python
    async def search(self, query: str) -> list[SearchResult]:
      # ... your code to fetch results ...

      results = []
      for result in raw_results: # raw_results would be results from scraper.
          search_result = self._convert_result(result)
          if search_result:  # Handle potential None values.
              results.append(search_result)
          if len(results) >= self.max_results:  # Limit results here.
              break

      return results
    ```

*   **Repeat:**  Apply the same `_get_num_results` implementation and result limiting logic to *all* other engine classes.  This is repetitive but crucial.

**4.2.2.  Source Attribution**
*   **`base.SearchEngine`:**  Already addressed with the abstract `engine_code` property.
*   **Engine-Specific Implementation:** In each engine, make *absolutely sure* you're using `self.engine_code` when creating the `SearchResult`:

    ```python
    # Example within an engine's search method:
    def _convert_result(self, raw: dict[str, Any]) -> SearchResult | None:
      try:
        return SearchResult(
            title=raw.get("title", ""),
            url=HttpUrl(raw.get("link", "")), # Using link
            snippet=raw.get("snippet", ""),
            source=self.engine_code,  # ALWAYS use self.engine_code
            raw=raw  # Include the raw data
        )
      except ValidationError as e:
        # warning message
        return None

    ```

**4.3. Engine Reliability (Empty Results)**

*   **`searchit` Engines:**
    1.  **Verify Installation:**  Run `uv pip install searchit` in your activated virtual environment.
    2.  **Minimal Example:** Create a *separate* Python script (`test_searchit.py`):

        ```python
        import asyncio
        from searchit.search import GoogleScraper, ScrapeRequest, SearchitResult

        async def test_searchit():
            scraper = GoogleScraper()
            request = ScrapeRequest(query="test query")
            results = await scraper.scrape(request)
            print(results)

        if __name__ == "__main__":
            asyncio.run(test_searchit())

        ```

        Run this script independently: `python test_searchit.py`.  If this fails, the problem is with the `searchit` library itself, or your installation of it (or potentially network/proxy issues).  If it *works*, the problem is in your integration within `twat-search`.

    3.  **Debugging (if the minimal example works):** Add detailed logging to `src/twat_search/web/engines/searchit.py`:

        ```python
        # Inside the GoogleSearchitEngine, YandexSearchitEngine, etc.
        async def search(self, query: str) -> list[SearchResult]:
            if not query:
                raise EngineError(self.engine_code, "Search query cannot be empty")
            logger.info(f"Searching Google (searchit) with query: '{query}'")

            request = ScrapeRequest(
                query=query,
                domain=self.domain,
                language=self.language,
                geo=self.geo,
                max_results=self.max_results, # Use self.max_results
                sleep_interval=self.sleep_interval,

            )
            logger.debug(f"searchit request: {request!r}")  # Log the request object

            scraper = GoogleScraper( # initiate object here, to pass max_results_per_page
                max_results_per_page=min(100, self.max_results),
            )

            try:
              raw_results = await self._run_scraper(scraper, request) # run scraper
              logger.debug(f"Raw results from searchit: {raw_results!r}")  # Log raw results
              if not raw_results:
                  logger.info("No results returned from Google (searchit)")
                  return []
              logger.debug(
                  f"Received {len(raw_results)} raw results from Google (searchit)",
              )
            except Exception as exc:
                error_msg = f"Error running searchit scraper: {exc}"
                logger.error(error_msg)
                raise EngineError(self.engine_code, error_msg) from exc
        # ...rest of code ...
        ```
*   **Brave News, Perplexity:**  Start by getting the *base* Brave and Perplexity engines working.  Then debug the news-specific versions, looking for differences in API endpoints, parameters, or response formats.  Use extensive logging.

**4.4. Codebase Cleanup (`anywebsearch` Removal)**

This is straightforward:

1.  **Delete File:** Delete `src/twat_search/web/engines/anywebsearch.py`.
2.  **Remove Imports:**  Remove *all* imports related to `anywebsearch` from:
    *   `src/twat_search/web/engines/__init__.py`
    *   `src/twat_search/web/engine_constants.py`
    *   Any test files that might have been using it.
3.  **Run Linters:** Use `ruff check --fix` and `ruff format` to automatically clean up unused imports and formatting.
4.  **Run Tests:**  Run `pytest` to ensure that no other parts of the codebase were relying on `anywebsearch`.

**4.5. Improve the CLI (q command)**
* **Remove Engine Specific Params:** The `q` command in the CLI needs to be updated to only take common params. This should be done by making the parameters not prefixed.
    ```python
     @app.command(
        help="Search across multiple engines simultaneously.",
        context_settings={"allow_extra_args": True, "ignore_unknown_options": True},
    )
    def q(
        self,
        ctx: fire.core.Context,
        query: str = typer.Argument(..., help="The search query."),
        engines: str | None = typer.Option(
            None,
            "--engines",
            "-e",
            help="Comma-separated list of search engines to use (e.g., brave,tavily).",
        ),
        num_results: int = typer.Option(
            None,
            "--num-results",
            "-n",
            help="Number of results to return (overrides engine defaults).",
        ),
        country: str = typer.Option(
            None,
            "--country",
            "-c",
            help="Country code for the search (e.g., US, GB).",
        ),
        language: str = typer.Option(
            None,
            "--language",
            "-l",
            help="Language code for the search (e.g., en, es).",
        ),
        safe_search: bool = typer.Option(
            None,
            "--safe-search",
            "-s",
            help="Enable or disable safe search (True/False).",
        ),
        json_output: bool = typer.Option(
            False, "--json", help="Output results in JSON format."
        ),
        verbose: bool = typer.Option(
            False, "--verbose", "-v", help="Enable verbose logging."
        ),
        plain: bool = typer.Option(
            False, "--plain", "-p", help="Output in plain text, url only."
        ),
    ) -> None:
    ```
* **Remove engine specific parameters:**  from the various engine specific commands, like brave, tavily, pplx, etc.
* **Test:** Run `cleanup.py status`

**4.6 Enforce JSON Output Format**

To standardize the JSON output format across all search engines, you should create a helper function that takes the necessary data and returns a dictionary with a consistent structure. This function will be used by each engine to format its results before returning them.  Since you are already using `pydantic`, use your existing `SearchResult` model.

Here's how you would do this:

1.  **Use `SearchResult` model:**

    In `src/twat_search/web/models.py`, you have the following `SearchResult` model:

    ```python
    class SearchResult(BaseModel):
        url: HttpUrl
        title: str
        snippet: str
        source: str
        raw: dict[str, Any] | None = None  # To store any engine-specific data

        @field_validator("title", "snippet", "source")
        def validate_non_empty(cls, v: str) -> str:
            if not v or not v.strip():
                msg = f"value must not be None or empty string, got='{v}'"
                raise ValueError(msg)
            return v.strip()
    ```

2. **Remove utility function**
Remove the `_process_results` and `_display_json_results`. Remove `CustomJSONEncoder`.

3.  **Modify Engine `search` Methods:**

    Each engine's `search` method should now create and return a list of `SearchResult` objects directly, instead of plain dictionaries.

    Example (using a simplified `DuckDuckGoSearchEngine` for illustration):

    ```python
    # src/twat_search/web/engines/duckduckgo.py
    class DuckDuckGoSearchEngine(SearchEngine):
      # ...

      async def search(self, query: str) -> list[SearchResult]:
          # ... (fetch raw results from DuckDuckGo API) ...
          results: list[SearchResult] = []
          for raw_result in raw_results:
            try:
              search_result = SearchResult(
                  url=raw_result["url"],
                  title=raw_result["title"],
                  snippet=raw_result.get("snippet", ""),  # Provide a default
                  source=self.engine_code, # Set source
                  raw=raw_result,  # Store the raw result
              )
              results.append(search_result)
              if len(results) >= self.max_results:
                break # respect the num_results/max_results.
            except ValidationError as e:
                logger.warning(f"Validation error for result: {e}")
                # Consider logging the raw_result here, for debugging.
                continue  # Skip invalid results
            except Exception as e: # catch unexpected errors
                logger.warning(f"Unexpected error converting result: {e}")
                continue

          return results
    ```
    Key points here:
    * Return type is now `list[SearchResult]`.
    * A SearchResult is instantiated for *every* valid result.
    * `source=self.engine_code` is used to correctly set the source.
    * A `try...except ValidationError` block handles potential Pydantic validation errors, logging them and skipping the invalid result.
    * We have added another exception block to catch other exceptions.
    * Added a limit on the number of results using `self.max_results`.
4. **Update API function**

```python
#src/twat_search/web/api.py

async def search(
    query: str,
    engines: list[str] | None = None,
    config: Config | None = None,
    strict_mode: bool = True, # added to ensure that error is raised if engine does not exist.
    **kwargs: Any,
) -> list[SearchResult]: # Change return type
```

Change the return type to `list[SearchResult]`.

```python
    results = await asyncio.gather(*tasks, return_exceptions=True)

    flattened_results: list[SearchResult] = []
    for engine_name, result in zip(engine_names, results, strict=False):
        if isinstance(result, Exception):
            logger.error(
                f"Engine '{engine_name}' failed: {type(result).__name__} - {result}",
            )
        elif isinstance(result, list):
            # This part now expects a list of SearchResult objects. No conversion needed!
            logger.info(
                f"âœ… Engine '{engine_name}' returned {len(result)} results",
            )

            for search_result in result:
                search_result.source = engine_name # override and use the engine name
                flattened_results.append(search_result)
        else:
            logger.warning(
                f"âš ï¸ Engine '{engine_name}' returned no results or unexpected type: {type(result)}",
            )

    if not flattened_results:
        logger.error(f"Search failed: {e}")
        raise SearchError("No results found from any search engine.")

    return flattened_results

```
The important part here is that `flattened_result` and the return is a list of `SearchResult`.

5. **Update CLI function**
Update the `q` command in `cli.py`

```python
@app.command(
    help="Search across multiple engines simultaneously.",
    context_settings={"allow_extra_args": True, "ignore_unknown_options": True},
)
def q(
    self,
    ctx: fire.core.Context,
    query: str = typer.Argument(..., help="The search query."),
    engines: str | None = typer.Option(
        None,
        "--engines",
        "-e",
        help="Comma-separated list of search engines to use (e.g., brave,tavily).",
    ),
    num_results: int = typer.Option(
        None,
        "--num-results",
        "-n",
        help="Number of results to return (overrides engine defaults).",
    ),
    country: str = typer.Option(
        None,
        "--country",
        "-c",
        help="Country code for the search (e.g., US, GB).",
    ),
    language: str = typer.Option(
        None,
        "--language",
        "-l",
        help="Language code for the search (e.g., en, es).",
    ),
    safe_search: bool = typer.Option(
        None,
        "--safe-search",
        "-s",
        help="Enable or disable safe search (True/False).",
    ),
    json_output: bool = typer.Option(
        False, "--json", help="Output results in JSON format."
    ),
    verbose: bool = typer.Option(
        False, "--verbose", "-v", help="Enable verbose logging."
    ),
    plain: bool = typer.Option(
        False, "--plain", "-p", help="Output in plain text, url only."
    ),
) -> None:

    self._configure_logging(verbose)
    #... parse engines, as you did before ...
    if num_results is not None:
        self.logger.debug(f"Using num_results={num_results}")
        common_params["num_results"] = num_results

    # Run search and handle potential errors.
    try:
        results = asyncio.run(
            self._run_search(
                query,
                engine_list,
                **common_params
            )
        )
        if json_output:
            # Use model_dump to serialize Pydantic models to dict
            print(json.dumps([r.model_dump() for r in results], indent=2)) # Now serializing correctly.
        else:
            _display_results(results, verbose, plain) #Simplified _display_results

    except SearchError as e:
        self.logger.error(f"Search failed: {e}")
        _display_errors([str(e)])
```

*   **Key changes:**
    *   The `q` command no longer takes engine-specific parameters.  It only takes common parameters, which are passed to the `_run_search` function.
    * The JSON output uses pydantic `model_dump` to convert `SearchResult` to dict.
    *   The non-JSON output uses a simplified `_display_results` function (see below).

```python
#Simplified display function
def _display_results(results: list[SearchResult], verbose: bool, plain: bool) -> None:
    if not results:
        console.print("[bold red]No results found![/bold red]")
        return

    if plain:
        urls = set()
        for result in results:
            # Avoid duplicate URLs
            if str(result.url) not in urls:
                urls.add(str(result.url))
        for url in sorted(urls):
            console.print(url)
        return

    table = Table()  # Remove show_lines=True to eliminate row separator lines
    table.add_column("Engine", style="magenta")
    if verbose:
      table.add_column("Title", style="green")
      table.add_column("URL", style="blue", overflow="fold", max_width=70)
    else:
      table.add_column("URL", style="blue", overflow="fold")

    for result in results:
        if verbose:
          table.add_row(result.source, result.title, str(result.url))
        else:
          table.add_row(result.source, str(result.url))
    console.print(table)
```

* **Remove helper functions:**
`_process_results`, `_display_json_results` and `CustomJSONEncoder` class are not needed anymore since `twat_search.web.models.SearchResult` model and pydantic `model_dump` are used.

**6. Testing:**

*   Create test cases for each of the above changes, especially within `tests/unit/web/engines/test_base.py` and in individual engine test files.
*   Test the CLI using `fire.Fire`'s testing utilities, as demonstrated in your existing `tests/test_twat_search.py`.

By systematically applying these steps, the code will become more robust, maintainable, and consistent. The detailed plan and code samples will guide you through the refactoring process. Remember to execute `cleanup.py status` regularly and run `pytest` to check for any errors or regressions introduced during the modifications.  Address *all* Ruff and Mypy issues.

================
File: VERSION.txt
================
v1.8.1



================================================================
End of Codebase
================================================================
