This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix. The content has been processed where empty lines have been removed.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: .specstory/**/*.md, .venv/**, _private/**, CLEANUP.txt, **/*.json, *.lock
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Empty lines have been removed from all files

Additional Info:
----------------

================================================================
Directory Structure
================================================================
.cursor/
  rules/
    0project.mdc
    cleanup.mdc
    filetree.mdc
.github/
  workflows/
    push.yml
    release.yml
resources/
  brave/
    brave_image.md
    brave_news.md
    brave_video.md
    brave.md
  pplx/
    pplx_urls.txt
    pplx.md
  you/
    you_news.md
    you_news.txt
    you.md
    you.txt
  pricing.md
src/
  twat_search/
    web/
      engines/
        __init__.py
        base.py
        bing_scraper.py
        brave.py
        critique.py
        duckduckgo.py
        google_scraper.py
        hasdata.py
        pplx.py
        searchit.py
        serpapi.py
        tavily.py
        you.py
      __init__.py
      api.py
      cli.py
      config.py
      engine_constants.py
      exceptions.py
      models.py
      utils.py
    __init__.py
    __main__.py
tests/
  unit/
    web/
      engines/
        __init__.py
        test_base.py
      __init__.py
      test_api.py
      test_config.py
      test_exceptions.py
      test_models.py
      test_utils.py
    __init__.py
    mock_engine.py
  web/
    test_bing_scraper.py
  conftest.py
  test_twat_search.py
.gitignore
.pre-commit-config.yaml
CHANGELOG.md
cleanup.py
LICENSE
NEXTENGINES.md
PROGRESS.md
pyproject.toml
README.md
TODO.md
VERSION.txt

================================================================
Files
================================================================

================
File: .cursor/rules/0project.mdc
================
---
description: About this project
globs: 
alwaysApply: false
---
# About this project

`twat-search` is a multi-provider search 

## Development Notes
- Uses `uv` for Python package management
- Quality tools: ruff, mypy, pytest
- Clear provider protocol for adding new search backends
- Strong typing and runtime checks throughout

================
File: .cursor/rules/cleanup.mdc
================
---
description: Run `cleanup.py` script before and after changes
globs: 
alwaysApply: false
---
Before you do any changes or if I say "cleanup", run the `cleanup.py update` script in the main folder. Analyze the results, describe recent changes in [PROGRESS.md](mdc:PROGRESS.md) and edit @TODO.md to update priorities and plan next changes. PERFORM THE CHANGES, then run the `cleanup.py status` script and react to the results.

When you edit @TODO.md, lead in lines with empty GFM checkboxes if things aren't done (`- [ ] `) vs. filled (`- [x] `) if done.

================
File: .cursor/rules/filetree.mdc
================
---
description: File tree of the project
globs: 
---
[1.0K]  .
├── [  64]  .benchmarks
├── [  96]  .cursor
│   └── [ 192]  rules
│       ├── [ 334]  0project.mdc
│       ├── [ 558]  cleanup.mdc
│       └── [5.7K]  filetree.mdc
├── [  96]  .github
│   └── [ 128]  workflows
│       ├── [2.7K]  push.yml
│       └── [1.4K]  release.yml
├── [3.5K]  .gitignore
├── [1.5K]  .pre-commit-config.yaml
├── [ 128]  .specstory
│   ├── [ 992]  history
│   │   ├── [2.0K]  .what-is-this.md
│   │   ├── [ 52K]  2025-02-25_01-58-creating-and-tracking-project-tasks.md
│   │   ├── [7.4K]  2025-02-25_02-17-project-task-continuation-and-progress-update.md
│   │   ├── [ 11K]  2025-02-25_02-24-planning-tests-for-twat-search-web-package.md
│   │   ├── [196K]  2025-02-25_02-27-implementing-tests-for-twat-search-package.md
│   │   ├── [ 46K]  2025-02-25_02-58-transforming-python-script-into-cli-tool.md
│   │   ├── [ 93K]  2025-02-25_03-09-generating-a-name-for-the-chat.md
│   │   ├── [5.5K]  2025-02-25_03-33-untitled.md
│   │   ├── [ 57K]  2025-02-25_03-54-integrating-search-engines-into-twat-search.md
│   │   ├── [ 72K]  2025-02-25_04-05-consolidating-you-py-and-youcom-py.md
│   │   ├── [6.1K]  2025-02-25_04-13-missing-env-api-key-names-in-pplx-py.md
│   │   ├── [118K]  2025-02-25_04-16-implementing-functions-for-brave-search-engines.md
│   │   ├── [286K]  2025-02-25_04-48-unifying-search-engine-parameters-in-twat-search.md
│   │   ├── [ 83K]  2025-02-25_05-36-implementing-duckduckgo-search-engine.md
│   │   ├── [194K]  2025-02-25_05-43-implementing-the-webscout-search-engine.md
│   │   ├── [ 23K]  2025-02-25_06-07-implementing-bing-scraper-engine.md
│   │   ├── [ 15K]  2025-02-25_06-12-continuing-bing-scraper-engine-implementation.md
│   │   ├── [121K]  2025-02-25_06-34-implementing-safe-import-patterns-in-modules.md
│   │   ├── [9.9K]  2025-02-25_07-09-refactoring-plan-and-progress-update.md
│   │   ├── [ 40K]  2025-02-25_07-17-implementing-phase-1-from-todo-md.md
│   │   ├── [292K]  2025-02-25_07-34-integrating-hasdata-google-serp-apis.md
│   │   ├── [142K]  2025-02-25_08-19-implementing-search-engines-from-nextengines-md.md
│   │   ├── [175K]  2025-02-26_09-54-implementing-plain-option-for-search-commands.md
│   │   ├── [264K]  2025-02-26_10-55-standardizing-engine-naming-conventions.md
│   │   ├── [4.3K]  2025-02-26_11-30-untitled.md
│   │   ├── [102K]  2025-02-26_12-11-update-config-py-to-use-engine-constants.md
│   │   ├── [278K]  2025-02-26_12-18-update-engine-imports-and-exports-in-init-py.md
│   │   ├── [268K]  2025-02-26_13-40-search-engine-initialization-errors.md
│   │   └── [ 54K]  2025-02-26_14-15-untitled.md
│   └── [2.2M]  history.txt
├── [3.2K]  CHANGELOG.md
├── [ 499]  CLEANUP.txt
├── [1.0K]  LICENSE
├── [ 30K]  NEXTENGINES.md
├── [1.2K]  PROGRESS.md
├── [ 21K]  README.md
├── [5.0K]  TODO.md
├── [   7]  VERSION.txt
├── [ 12K]  cleanup.py
├── [ 192]  dist
├── [1.5K]  fix_name_attribute.py
├── [ 10K]  pyproject.toml
├── [ 128]  src
│   └── [ 256]  twat_search
│       ├── [ 577]  __init__.py
│       ├── [2.4K]  __main__.py
│       └── [ 416]  web
│           ├── [1.7K]  __init__.py
│           ├── [5.8K]  api.py
│           ├── [ 45K]  cli.py
│           ├── [ 13K]  config.py
│           ├── [2.6K]  engine_constants.py
│           ├── [ 576]  engines
│           │   ├── [8.5K]  __init__.py
│           │   ├── [ 25K]  anywebsearch.py
│           │   ├── [4.4K]  base.py
│           │   ├── [ 11K]  bing_scraper.py
│           │   ├── [7.9K]  brave.py
│           │   ├── [8.5K]  critique.py
│           │   ├── [6.9K]  duckduckgo.py
│           │   ├── [ 12K]  google_scraper.py
│           │   ├── [7.5K]  hasdata.py
│           │   ├── [5.1K]  pplx.py
│           │   ├── [ 26K]  searchit.py
│           │   ├── [7.2K]  serpapi.py
│           │   ├── [7.6K]  tavily.py
│           │   └── [7.5K]  you.py
│           ├── [1.0K]  exceptions.py
│           ├── [1.3K]  models.py
│           └── [1.5K]  utils.py
├── [ 256]  tests
│   ├── [  64]  .benchmarks
│   ├── [2.0K]  conftest.py
│   ├── [ 157]  test_twat_search.py
│   ├── [ 192]  unit
│   │   ├── [  42]  __init__.py
│   │   ├── [1.5K]  mock_engine.py
│   │   └── [ 320]  web
│   │       ├── [  46]  __init__.py
│   │       ├── [ 160]  engines
│   │       │   ├── [  37]  __init__.py
│   │       │   └── [4.3K]  test_base.py
│   │       ├── [5.1K]  test_api.py
│   │       ├── [2.7K]  test_config.py
│   │       ├── [2.0K]  test_exceptions.py
│   │       ├── [4.5K]  test_models.py
│   │       └── [3.5K]  test_utils.py
│   └── [ 160]  web
│       └── [ 10K]  test_bing_scraper.py
├── [158K]  twat_search.txt
└── [267K]  uv.lock

19 directories, 87 files

================
File: .github/workflows/push.yml
================
name: Build & Test
on:
  push:
    branches: [main]
    tags-ignore: ["v*"]
  pull_request:
    branches: [main]
  workflow_dispatch:
permissions:
  contents: write
  id-token: write
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true
jobs:
  quality:
    name: Code Quality
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Run Ruff lint
        uses: astral-sh/ruff-action@v3
        with:
          version: "latest"
          args: "check --output-format=github"
      - name: Run Ruff Format
        uses: astral-sh/ruff-action@v3
        with:
          version: "latest"
          args: "format --check --respect-gitignore"
  test:
    name: Run Tests
    needs: quality
    strategy:
      matrix:
        python-version: ["3.10", "3.11", "3.12"]
        os: [ubuntu-latest]
      fail-fast: true
    runs-on: ${{ matrix.os }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: ${{ matrix.python-version }}
          enable-cache: true
          cache-suffix: ${{ matrix.os }}-${{ matrix.python-version }}
      - name: Install test dependencies
        run: |
          uv pip install --system --upgrade pip
          uv pip install --system ".[test]"
      - name: Run tests with Pytest
        run: uv run pytest -n auto --maxfail=1 --disable-warnings --cov-report=xml --cov-config=pyproject.toml --cov=src/twat_search --cov=tests tests/
      - name: Upload coverage report
        uses: actions/upload-artifact@v4
        with:
          name: coverage-${{ matrix.python-version }}-${{ matrix.os }}
          path: coverage.xml
  build:
    name: Build Distribution
    needs: test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: "3.12"
          enable-cache: true
      - name: Install build tools
        run: uv pip install build hatchling hatch-vcs
      - name: Build distributions
        run: uv run python -m build --outdir dist
      - name: Upload distribution artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dist-files
          path: dist/
          retention-days: 5

================
File: .github/workflows/release.yml
================
name: Release
on:
  push:
    tags: ["v*"]
permissions:
  contents: write
  id-token: write
jobs:
  release:
    name: Release to PyPI
    runs-on: ubuntu-latest
    environment:
      name: pypi
      url: https://pypi.org/p/twat-search
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: "3.12"
          enable-cache: true
      - name: Install build tools
        run: uv pip install build hatchling hatch-vcs
      - name: Build distributions
        run: uv run python -m build --outdir dist
      - name: Verify distribution files
        run: |
          ls -la dist/
          test -n "$(find dist -name '*.whl')" || (echo "Wheel file missing" && exit 1)
          test -n "$(find dist -name '*.tar.gz')" || (echo "Source distribution missing" && exit 1)
      - name: Publish to PyPI
        uses: pypa/gh-action-pypi-publish@release/v1
        with:
          password: ${{ secrets.PYPI_TOKEN }}
      - name: Create GitHub Release
        uses: softprops/action-gh-release@v1
        with:
          files: dist/*
          generate_release_notes: true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

================
File: resources/brave/brave_image.md
================
[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard) [ Documentation](/app/documentation)

[ Web Search](/app/documentation/web-search) [ Summarizer
Search](/app/documentation/summarizer-search) [ Image
Search](/app/documentation/image-search)

[ Get Started](/app/documentation/image-search/get-started)[ Query
Parameters](/app/documentation/image-search/query)[ Request
Headers](/app/documentation/image-search/request-headers)[ Response
Headers](/app/documentation/image-search/response-headers)[ Response
Objects](/app/documentation/image-search/responses)[
Codes](/app/documentation/image-search/codes)[ API
Changelog](/app/documentation/image-search/api-changelog)[ How to
Guides](/app/documentation/image-search/guides)

[ Video Search](/app/documentation/video-search) [ News
Search](/app/documentation/news-search) [ Suggest](/app/documentation/suggest)
[ Spellcheck](/app/documentation/spellcheck) [
General](/app/documentation/general)

[Login](/login) [Register](/register)

# Brave Search Image Search API

Brave Search API is a REST API to query Brave Search and get back search
results from the web. The following sections describe how to curate requests,
including parameters and headers, to Brave Search API and get a JSON response
back.

> To try the API on a Free plan, you’ll still need to subscribe — you simply
> won’t be charged. Once subscribed, you can get an API key in the [API Keys
> section](/app/keys).

## Endpoints

Brave Search API exposes multiple endpoints for specific types of data, based
on the level of your subscription. If you don’t see the endpoint you’re
interested in, you may need to change your subscription.

Brave Image Search API is currently available at the following endpoint and
exposes an API to get images from the web relevant to the query.

    
    
    https://api.search.brave.com/res/v1/images/search
    
    

## Example

Get started immediately with CURL. An example request will look something like
this:

    
    
    
    curl -s --compressed "https://api.search.brave.com/res/v1/images/search?q=munich&safesearch=strict&count=20&search_lang=en&country=us&spellcheck=1" \
      -H "Accept: application/json" \
      -H "Accept-Encoding: gzip" \
      -H "X-Subscription-Token: <YOUR_API_KEY>"
    

## Next Steps

To learn what parameters are available and what responses can be expected
while querying Brave Search, please review the following pages:

  * [Query Parameters](/app/documentation/image-search/query)
  * [Request Headers](/app/documentation/image-search/request-headers)
  * [Response Headers](/app/documentation/image-search/response-headers)
  * [Response Objects](/app/documentation/image-search/responses)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard) [ Documentation](/app/documentation)

[ Web Search](/app/documentation/web-search) [ Summarizer
Search](/app/documentation/summarizer-search) [ Image
Search](/app/documentation/image-search)

[ Get Started](/app/documentation/image-search/get-started)[ Query
Parameters](/app/documentation/image-search/query)[ Request
Headers](/app/documentation/image-search/request-headers)[ Response
Headers](/app/documentation/image-search/response-headers)[ Response
Objects](/app/documentation/image-search/responses)[
Codes](/app/documentation/image-search/codes)[ API
Changelog](/app/documentation/image-search/api-changelog)[ How to
Guides](/app/documentation/image-search/guides)

[ Video Search](/app/documentation/video-search) [ News
Search](/app/documentation/news-search) [ Suggest](/app/documentation/suggest)
[ Spellcheck](/app/documentation/spellcheck) [
General](/app/documentation/general)

[Login](/login) [Register](/register)

###### Brave Image Search API

## Query Parameters

#### # Image Search API

This table lists the query parameters supported by the Image Search API. Some
are required, but most are optional.

Parameter| Required| Type| Default| Description  
---|---|---|---|---  
q| true| string| |  The user’s search query term. Query can not be empty. Maximum of 400 characters and 50 words in the query.  
country| false| string| US|  The search query country, where the results come
from. The country string is limited to 2 character country codes of supported
countries. For a list of supported values, see [Country
Codes](/app/documentation/image-search/codes#country-codes).  
search_lang| false| string| en|  The search language preference. The 2 or more
character language code for which the search results are provided. For a list
of possible values, see [Language Codes](/app/documentation/image-
search/codes#language-codes).  
count| false| number| 50|  The number of search results returned in response.
The maximum is `100`. The actual number delivered may be less than requested.  
safesearch| false| string| strict|  Filters search results for adult content.
The following values are supported:

  * `off`: No filtering is done.
  * `strict`: Drops all adult content from search results.

  
spellcheck| false| bool| 1|  Whether to spellcheck provided query. If the
spellchecker is enabled, the modified query is always used for search. The
modified query can be found in `altered` key from the
[query](/app/documentation/image-search/responses#Query) response model.

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard) [ Documentation](/app/documentation)

[ Web Search](/app/documentation/web-search) [ Summarizer
Search](/app/documentation/summarizer-search) [ Image
Search](/app/documentation/image-search)

[ Get Started](/app/documentation/image-search/get-started)[ Query
Parameters](/app/documentation/image-search/query)[ Request
Headers](/app/documentation/image-search/request-headers)[ Response
Headers](/app/documentation/image-search/response-headers)[ Response
Objects](/app/documentation/image-search/responses)[
Codes](/app/documentation/image-search/codes)[ API
Changelog](/app/documentation/image-search/api-changelog)[ How to
Guides](/app/documentation/image-search/guides)

[ Video Search](/app/documentation/video-search) [ News
Search](/app/documentation/news-search) [ Suggest](/app/documentation/suggest)
[ Spellcheck](/app/documentation/spellcheck) [
General](/app/documentation/general)

[Login](/login) [Register](/register)

###### Brave Image Search API

## Request Headers

#### Image Search API Request Headers

This table lists the request headers supported by the Image Search API, most
of which are optional.

Header| Required| Name| Description  
---|---|---|---  
Accept| false| Accept|  The default supported media type is `application/json`  
Accept-Encoding| false| Accept Encoding|  The supported compression type is
`gzip`.  
Api-Version| false| Web Search API Version|  The Brave Web Search API version
to use. This is denoted by the format `YYYY-MM-DD`. The latest version is used
by default, and the previous ones can be found in the [API Changelog](./api-
changelog).  
Cache-Control| false| Cache Control|  Search will return cached web search
results by default. To prevent caching set the Cache-Control header to `no-
cache`. This is currently done as best effort.  
User-Agent| false| User Agent|  The user agent of the client sending the
request. Search can utilize the user agent to provide a different experience
depending on the client sending the request. The user agent should follow the
commonly used browser agent strings on each platform. For more information on
curating user agents, see [RFC 9110](https://www.rfc-
editor.org/rfc/rfc9110.html#name-user-agent). User agent string examples by
platform:

  * **Android** : Mozilla/5.0 (Linux; Android 13; Pixel 7 Pro) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Mobile Safari/537.36
  * **iOS** : Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Mobile/15E148 Safari/604.1
  * **macOS** : Mozilla/5.0 (Macintosh; Intel Mac OS X 12_0_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/
  * **Windows** : Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/

  
X-Subscription-Token| true| Authentication token|  The secret token for the
subscribed plan to authenticate the request. Can be obtained from [API
Keys](/app/keys).

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard) [ Documentation](/app/documentation)

[ Web Search](/app/documentation/web-search) [ Summarizer
Search](/app/documentation/summarizer-search) [ Image
Search](/app/documentation/image-search)

[ Get Started](/app/documentation/image-search/get-started)[ Query
Parameters](/app/documentation/image-search/query)[ Request
Headers](/app/documentation/image-search/request-headers)[ Response
Headers](/app/documentation/image-search/response-headers)[ Response
Objects](/app/documentation/image-search/responses)[
Codes](/app/documentation/image-search/codes)[ API
Changelog](/app/documentation/image-search/api-changelog)[ How to
Guides](/app/documentation/image-search/guides)

[ Video Search](/app/documentation/video-search) [ News
Search](/app/documentation/news-search) [ Suggest](/app/documentation/suggest)
[ Spellcheck](/app/documentation/spellcheck) [
General](/app/documentation/general)

[Login](/login) [Register](/register)

###### Brave Image Search API

## Response Headers

#### Global

This table lists the response headers supported by the Image Search API.

Header| Name| Description  
---|---|---  
X-RateLimit-Limit| Rate Limit|  Rate limits associated with the requested
plan. An example rate limit `X-RateLimit-Limit: 1, 15000` means 1 request per
second and 15000 requests per month.  
X-RateLimit-Policy| Rate Limit Policy|  Rate limit policies currently
associated with the requested plan. An example policy `X-RateLimit-Policy:
1;w=1, 15000;w=2592000` means a limit of 1 request over a 1 second window and
15000 requests over a month window. The windows are always given in seconds.  
X-RateLimit-Remaining| Rate Limit Remaining|  Remaining quota units associated
with the expiring limits. An example remaining limit `X-RateLimit-Remaining:
1, 1000` indicates the API is able to be accessed once during the current
second, and 1000 times over the current month. **Note** : Only successful
requests are counted and billed.  
X-RateLimit-Reset| Rate Limit Reset|  The number of seconds until the quota
associated with the expiring limits resets. An example reset limit
`X-RateLimit-Reset: 1, 1419704` means a single request can be done again in a
second and in 1419704 seconds the full monthly quota associated with the plan
will be available again.

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard) [ Documentation](/app/documentation)

[ Web Search](/app/documentation/web-search) [ Summarizer
Search](/app/documentation/summarizer-search) [ Image
Search](/app/documentation/image-search)

[ Get Started](/app/documentation/image-search/get-started)[ Query
Parameters](/app/documentation/image-search/query)[ Request
Headers](/app/documentation/image-search/request-headers)[ Response
Headers](/app/documentation/image-search/response-headers)[ Response
Objects](/app/documentation/image-search/responses)[
Codes](/app/documentation/image-search/codes)[ API
Changelog](/app/documentation/image-search/api-changelog)[ How to
Guides](/app/documentation/image-search/guides)

[ Video Search](/app/documentation/video-search) [ News
Search](/app/documentation/news-search) [ Suggest](/app/documentation/suggest)
[ Spellcheck](/app/documentation/spellcheck) [
General](/app/documentation/general)

[Login](/login) [Register](/register)

###### Brave Image Search API

## Response Objects

#### # ImageSearchApiResponse

Top level response model for successful Image Search API requests. The API can
also respond back with an error response based on invalid subscription keys
and rate limit events.

Field| Type| Description  
---|---|---  
type| "images"| The type of search API result. The value is always images.  
query| Query| Image search query string.  
results| list [ ImageResult ]| The list of image results for the given query.  
  
#### # Query

A model representing information gathered around the requested query.

Field| Type| Description  
---|---|---  
original| string| The original query that was requested.  
altered| string| The altered query by the spellchecker. This is the query that
is used to search.  
spellcheck_off| bool| Whether the spell checker is enabled or disabled.  
show_strict_warning| string| The value is True if the lack of results is due
to a 'strict' safesearch setting. Adult content relevant to the query was
found, but was blocked by safesearch.  
  
#### # ImageResult

A model representing an image result for the requested query.

Field| Type| Description  
---|---|---  
type| image_result| The type of image search API result. The value is always
image_result.  
title| string| The title of the image.  
url| string| The original page url where the image was found.  
source| string| The source domain where the image was found.  
page_fetched| string| The iso date time when the page was last fetched. The
format is YYYY-MM-DDTHH:MM:SSZ  
thumbnail| Thumbnail| The thumbnail for the image.  
properties| Properties| Metadata for the image.  
meta_url| MetaUrl| Aggregated information on the url associated with the image
search result.  
  
#### # Thumbnail

Aggregated details representing the image thumbnail

Field| Type| Description  
---|---|---  
src| string| The served url of the image.  
  
#### # Properties

Metadata on an image.

Field| Type| Description  
---|---|---  
url| string| The image URL.  
placeholder| string| The lower resolution placeholder image url.  
  
#### # MetaUrl

Aggregated information about a url.

Field| Type| Description  
---|---|---  
scheme| string| The protocol scheme extracted from the url.  
netloc| string| The network location part extracted from the url.  
hostname| string| The lowercased domain name extracted from the url.  
favicon| string| The favicon used for the url.  
path| string| The hierarchical path of the url useful as a display string.

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard) [ Documentation](/app/documentation)

[ Web Search](/app/documentation/web-search) [ Summarizer
Search](/app/documentation/summarizer-search) [ Image
Search](/app/documentation/image-search)

[ Get Started](/app/documentation/image-search/get-started)[ Query
Parameters](/app/documentation/image-search/query)[ Request
Headers](/app/documentation/image-search/request-headers)[ Response
Headers](/app/documentation/image-search/response-headers)[ Response
Objects](/app/documentation/image-search/responses)[
Codes](/app/documentation/image-search/codes)[ API
Changelog](/app/documentation/image-search/api-changelog)[ How to
Guides](/app/documentation/image-search/guides)

[ Video Search](/app/documentation/video-search) [ News
Search](/app/documentation/news-search) [ Suggest](/app/documentation/suggest)
[ Spellcheck](/app/documentation/spellcheck) [
General](/app/documentation/general)

[Login](/login) [Register](/register)

## Codes

#### [#](/app/documentation/image-search/query#country) Country Codes

This table lists the country codes supported by the `country` parameter.

Country| Code  
---|---  
All Regions| ALL  
Argentina| AR  
Australia| AU  
Austria| AT  
Belgium| BE  
Brazil| BR  
Canada| CA  
Chile| CL  
Denmark| DK  
Finland| FI  
France| FR  
Germany| DE  
Hong Kong| HK  
India| IN  
Indonesia| ID  
Italy| IT  
Japan| JP  
Korea| KR  
Malaysia| MY  
Mexico| MX  
Netherlands| NL  
New Zealand| NZ  
Norway| NO  
Peoples Republic of China| CN  
Poland| PL  
Portugal| PT  
Republic of the Philippines| PH  
Russia| RU  
Saudi Arabia| SA  
South Africa| ZA  
Spain| ES  
Sweden| SE  
Switzerland| CH  
Taiwan| TW  
Turkey| TR  
United Kingdom| GB  
United States| US  
  
#### [#](/app/documentation/image-search/query#language) Language Codes

This table lists the language codes supported by the `search_lang` parameter.

Language| Code  
---|---  
Arabic| ar  
Basque| eu  
Bengali| bn  
Bulgarian| bg  
Catalan| ca  
Chinese Simplified| zh-hans  
Chinese Traditional| zh-hant  
Croatian| hr  
Czech| cs  
Danish| da  
Dutch| nl  
English| en  
English United Kingdom| en-gb  
Estonian| et  
Finnish| fi  
French| fr  
Galician| gl  
German| de  
Gujarati| gu  
Hebrew| he  
Hindi| hi  
Hungarian| hu  
Icelandic| is  
Italian| it  
Japanese| jp  
Kannada| kn  
Korean| ko  
Latvian| lv  
Lithuanian| lt  
Malay| ms  
Malayalam| ml  
Marathi| mr  
Norwegian Bokmål| nb  
Polish| pl  
Portuguese Brazil| pt-br  
Portuguese Portugal| pt-pt  
Punjabi| pa  
Romanian| ro  
Russian| ru  
Serbian Cyrylic| sr  
Slovak| sk  
Slovenian| sl  
Spanish| es  
Swedish| sv  
Tamil| ta  
Telugu| te  
Thai| th  
Turkish| tr  
Ukrainian| uk  
Vietnamese| vi

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard) [ Documentation](/app/documentation)

[ Web Search](/app/documentation/web-search) [ Summarizer
Search](/app/documentation/summarizer-search) [ Image
Search](/app/documentation/image-search)

[ Get Started](/app/documentation/image-search/get-started)[ Query
Parameters](/app/documentation/image-search/query)[ Request
Headers](/app/documentation/image-search/request-headers)[ Response
Headers](/app/documentation/image-search/response-headers)[ Response
Objects](/app/documentation/image-search/responses)[
Codes](/app/documentation/image-search/codes)[ API
Changelog](/app/documentation/image-search/api-changelog)[ How to
Guides](/app/documentation/image-search/guides)

[ Video Search](/app/documentation/video-search) [ News
Search](/app/documentation/news-search) [ Suggest](/app/documentation/suggest)
[ Spellcheck](/app/documentation/spellcheck) [
General](/app/documentation/general)

[Login](/login) [Register](/register)

# Brave Search API Changelog - Images

This changelog lists all updates to the Brave Image Search API in
chronological order.

### 2023-08-09

  * Initial release of the Image Search API.

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard) [ Documentation](/app/documentation)

[ Web Search](/app/documentation/web-search) [ Summarizer
Search](/app/documentation/summarizer-search) [ Image
Search](/app/documentation/image-search)

[ Get Started](/app/documentation/image-search/get-started)[ Query
Parameters](/app/documentation/image-search/query)[ Request
Headers](/app/documentation/image-search/request-headers)[ Response
Headers](/app/documentation/image-search/response-headers)[ Response
Objects](/app/documentation/image-search/responses)[
Codes](/app/documentation/image-search/codes)[ API
Changelog](/app/documentation/image-search/api-changelog)[ How to
Guides](/app/documentation/image-search/guides)

[ Video Search](/app/documentation/video-search) [ News
Search](/app/documentation/news-search) [ Suggest](/app/documentation/suggest)
[ Spellcheck](/app/documentation/spellcheck) [
General](/app/documentation/general)

[Login](/login) [Register](/register)

# A simple guide to handle missing and small image results

##### Brave Search Image API

There are two limitations of the Brave Search Image API:

  * the image results could be pointing to urls that cannot be resolved (404)
  * the image dimensions `width` and `height` are not provided ahead of time.

We’ve worked around the very same limitations to build our image search
vertical at [search.brave.com/images](https://search.brave.com/images?q=cats).
This guide is a simpler version of the same approach.

Our project structure will be this:

    
    
    index.js
    public/
        images.html
        styles.css
    

To get started, let’s create a folder for our image search page as well as
create the files we will need later on:

    
    
    mkdir img-search && cd img-search
    mkdir public
    touch public/images.html
    touch public/styles.css
    

We’ll use a simple [node.js express server](https://expressjs.com/) to call
the API. So let’s go ahead and install it as a dependency (our only one).

    
    
    npm install express --save
    

The following `index.js` is our server, and will be handling the API calls. It
has to be done through a server environment so that there are no Cross-Origin
Resource Sharing ([CORS](https://developer.mozilla.org/en-
US/docs/Web/HTTP/CORS)) issues.

#### index.js

    
    
    const express = require('express');
    const app = express();
    const port = 4000;
    
    app.use(express.static('public'));
    
    const API_KEY = '<YOUR_API_KEY>';
    const API_PATH = 'https://api.search.brave.com/res/v1/images/search';
    
    app.get('/api/images', async (req, res) => {
      try {
        const params = new URLSearchParams({
          q: req.query.q,
          count: 20,
          search_lang: 'en',
          country: 'us',
          spellcheck: 1,
        });
        const response = await fetch(`${API_PATH}?${params}`, {
          headers: {
            'x-subscription-token': API_KEY,
            accept: 'application/json',
          },
        });
        const data = await response.json();
        res.json(data);
        return;
      } catch (err) {
        console.log(err);
      }
      res.status(500).send('Internal Server Error');
    });
    
    app.listen(port, () => {
      console.log(`Example app listening on port ${port}`);
    });
    

Now that we have our server, let’s focus on the client side scripts needed to
load image outside of the DOM, to learn if they exist, and what the `width`
and `height` is.

#### public/images.html

    
    
    <html>
      <head>
        <title>Image Search</title>
        <link rel="stylesheet" href="/styles.css" />
      </head>
      <body>
        <script lang="javascript">
          async function fetchImages(query) {
            const params = new URLSearchParams({ q: query });
            const response = await fetch(`/api/images?${params}`);
            return await response.json();
            return data;
          }
    
          function renderImages(images) {
            const imagesContainer = document.getElementById('images');
            imagesContainer.innerHTML = '';
            images.forEach(({ image }) => {
              const figElement = document.createElement('figure');
    
              const imgElement = document.createElement('img');
              imgElement.src = image.thumbnail.src;
              imgElement.alt = image.title;
              imgElement.width = image.thumbnail.width;
              imgElement.height = image.thumbnail.height;
    
              const figCaptionElement = document.createElement('figcaption');
              figCaptionElement.innerHTML =
                `<div class="dimensions">${image.thumbnail.width} x ${image.thumbnail.height}</div>` +
                image.title;
    
              figElement.appendChild(imgElement);
              figElement.appendChild(figCaptionElement);
    
              imagesContainer.appendChild(figElement);
            });
          }
    
          function loadImage(result) {
            return new Promise((resolve, reject) => {
              const img = new Image();
              img.crossOrigin = 'anonymous';
              img.onload = (e) => {
                if (e.target) {
                  const image = e.target;
                  const width = image.naturalWidth;
                  const height = image.naturalHeight;
    
                  // Filter images that are too small
                  if (width < 275 || height < 275) {
                    console.error('[Img] Image too small', result);
                    reject(result);
                    return;
                  }
    
                  // Fill missing info; use the size the image will be scaled into instead of actual size
                  result.thumbnail.width = width;
                  result.thumbnail.height = height;
    
                  resolve(result);
                } else {
                  console.error('[Img] onLoad returned no image', result);
                  reject(result);
                }
              };
              img.onerror = (e) => {
                console.error('[Img] onError loading img', e);
                reject(result);
              };
              img.onabort = (e) => {
                console.error('[Img] onAbort loading img', e);
                reject(result);
              };
              img.src = result.thumbnail.src;
            });
          }
    
          async function load(query) {
            // Load images from API
            const response = await fetchImages(query);
            const loadedImages = [];
            let i = 0;
            const count = 5;
            const initialPromises = [];
    
            // Load the first 5 images in parallel
            for (let i = 0; i < count; i++) {
              initialPromises.push(async () => {
                const result = response.results[i];
                try {
                  const image = await loadImage(result);
                  loadedImages.push({
                    image,
                  });
                } catch (err) {
                  // pass
                }
              });
            }
    
            // Wait for the first 5 images to resolve
            await Promise.all(initialPromises);
    
            // Load images sequentially until there is 5 results, then show them.
            while (loadedImages.length < 5 || i >= response.results.length) {
              const result = response.results[i];
              try {
                const image = await loadImage(result);
                loadedImages.push({
                  image,
                });
              } catch (err) {
                // pass
              }
              i++;
            }
    
            renderImages(loadedImages);
          }
        </script>
        <div id="search">
          <input type="text" id="query" value="" onchan />
          <button onclick="load(document.getElementById('query').value)">Search</button>
        </div>
        <div id="images"></div>
      </body>
    </html>
    

The approach here, is to load images outside of the DOM, 5 at a time (which
you can configure), and images that we manage successfully, and are larger
than a given size (`width > 275 || height > 275` in the example), we show in
the DOM.

Lastly, we need some styles to make sure what we see does not look too bad:

### styles.css

    
    
    #images {
      display: flex;
      flex-wrap: wrap;
      gap: 20px;
      max-width: 1000px;
      margin: 2rem auto;
      justify-content: center;
    }
    #search {
      display: flex;
      gap: 1rem;
      align-items: center;
      width: 600px;
      margin: auto;
    }
    #search input {
      width: 100%;
      padding: 10px 20px;
      border-radius: 12px;
      outline: none;
      border: solid 1px #d3d3d3;
      background: #f3f3f3;
    }
    button {
      cursor: pointer;
      padding: 10px 20px;
      border-radius: 12px;
      outline: none;
      border: solid 1px #d3d3d3;
      background: #f3f3f3;
    }
    button:hover {
      background: #111;
      color: #fff;
    }
    figure {
      border: solid 1px #d3d3d3;
      display: flex;
      flex-flow: column;
      padding: 5px 5px 10px 5px;
      max-width: 220px;
      gap: 1rem;
      border-radius: 12px;
      margin: 0;
    }
    
    img {
      max-width: 220px;
      max-height: 150px;
      object-fit: contain;
      border-radius: 12px;
    }
    
    figcaption {
      display: flex;
      text-align: center;
      font-family: sans-serif;
      font-size: 14px;
      flex-direction: column;
      gap: 5px;
      align-items: center;
    }
    figcaption .dimensions {
      font-size: 12px;
      color: blue;
      background: #e7e7fc;
      padding: 5px 10px;
      border-radius: 12px;
      width: fit-content;
    }
    

And we’re ready to go. You can now run the project:

    
    
    node index.js
    

Open `localhost:4000/images.html` in the browser, and start searching.

![Image Search Client](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/img-search-client.Yjc8cU7X.png)

================
File: resources/brave/brave_news.md
================
[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard) [ Documentation](/app/documentation)

[ Web Search](/app/documentation/web-search) [ Summarizer
Search](/app/documentation/summarizer-search) [ Image
Search](/app/documentation/image-search) [ Video
Search](/app/documentation/video-search) [ News
Search](/app/documentation/news-search)

[ Get Started](/app/documentation/news-search/get-started)[ Query
Parameters](/app/documentation/news-search/query)[ Request
Headers](/app/documentation/news-search/request-headers)[ Response
Headers](/app/documentation/news-search/response-headers)[ Response
Objects](/app/documentation/news-search/responses)[
Codes](/app/documentation/news-search/codes)[ API
Changelog](/app/documentation/news-search/api-changelog)

[ Suggest](/app/documentation/suggest) [
Spellcheck](/app/documentation/spellcheck) [
General](/app/documentation/general)

[Login](/login) [Register](/register)

# Brave News Search API

Brave Search API is a REST API to query Brave Search and get back search
results from the web. The following sections describe how to curate requests,
including parameters and headers, to Brave Search API and get a JSON response
back.

> To try the API on a Free plan, you’ll still need to subscribe — you simply
> won’t be charged. Once subscribed, you can get an API key in the [API Keys
> section](/app/keys).

## Endpoints

Brave Search API exposes multiple endpoints for specific types of data, based
on the level of your subscription. If you don’t see the endpoint you’re
interested in, you may need to change your subscription.

Brave News Search API is currently available at the following endpoint and
exposes an API to get news from the web relevant to the query.

    
    
    https://api.search.brave.com/res/v1/news/search
    

## Example

Get started immediately with CURL. An example request will look something like
this:

    
    
    
    curl -s --compressed "https://api.search.brave.com/res/v1/news/search?q=munich&count=10&country=us&search_lang=en&spellcheck=1" \
      -H "Accept: application/json" \
      -H "Accept-Encoding: gzip" \
      -H "X-Subscription-Token: <YOUR_API_KEY>"
    

## Next Steps

To learn what parameters are available and what responses can be expected
while querying Brave Search, please review the following pages:

  * [Query Parameters](/app/documentation/news-search/query)
  * [Request Headers](/app/documentation/news-search/request-headers)
  * [Response Headers](/app/documentation/news-search/response-headers)
  * [Response Objects](/app/documentation/news-search/responses)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard) [ Documentation](/app/documentation)

[ Web Search](/app/documentation/web-search) [ Summarizer
Search](/app/documentation/summarizer-search) [ Image
Search](/app/documentation/image-search) [ Video
Search](/app/documentation/video-search) [ News
Search](/app/documentation/news-search)

[ Get Started](/app/documentation/news-search/get-started)[ Query
Parameters](/app/documentation/news-search/query)[ Request
Headers](/app/documentation/news-search/request-headers)[ Response
Headers](/app/documentation/news-search/response-headers)[ Response
Objects](/app/documentation/news-search/responses)[
Codes](/app/documentation/news-search/codes)[ API
Changelog](/app/documentation/news-search/api-changelog)

[ Suggest](/app/documentation/suggest) [
Spellcheck](/app/documentation/spellcheck) [
General](/app/documentation/general)

[Login](/login) [Register](/register)

###### Brave News Search API

## Query Parameters

#### # News Search API

This table lists the query parameters supported by the News Search API. Some
are required, but most are optional.

Parameter| Required| Type| Default| Description  
---|---|---|---|---  
q| true| string| |  The user’s search query term. Query can not be empty. Maximum of 400 characters and 50 words in the query.  
country| false| string| US|  The search query country, where the results come
from. The country string is limited to 2 character country codes of supported
countries. For a list of supported values, see [Country
Codes](/app/documentation/news-search/codes#country-codes).  
search_lang| false| string| en|  The search language preference. The 2 or more
character language code for which the search results are provided. For a list
of possible values, see [Language Codes](/app/documentation/news-
search/codes#language-codes).  
ui_lang| false| string| en-US|  User interface language preferred in response.
Usually of the format ‘<language_code>-<country_code>’. For more, see [RFC
9110](https://www.rfc-editor.org/rfc/rfc9110.html#name-accept-language). For a
list of supported values, see [UI Language Codes](/app/documentation/news-
search/codes#market-codes).  
count| false| number| 20|  The number of search results returned in response.
The maximum is `50`. The actual number delivered may be less than requested.
Combine this parameter with `offset` to paginate search results.  
offset| false| number| 0|  The zero based offset that indicates number of
search results per page (count) to skip before returning the result. The
maximum is `9`. The actual number delivered may be less than requested based
on the query. In order to paginate results use this parameter together with
`count`. For example, if your user interface displays 20 search results per
page, set `count` to `20` and offset to `0` to show the first page of results.
To get subsequent pages, increment `offset` by 1 (e.g. 0, 1, 2). The results
may overlap across multiple pages.  
spellcheck| false| bool| 1|  Whether to spellcheck provided query. If the
spellchecker is enabled, the modified query is always used for search. The
modified query can be found in `altered` key from the
[query](/app/documentation/news-search/responses#Query) response model.  
safesearch| false| string| moderate|  Filters search results for adult
content. The following values are supported:

  * `off` \- No filtering.
  * `moderate` \- Filter out explicit content.
  * `strict` \- Filter out explicit and suggestive content.

  
freshness| false| string| |  Filters search results by when they were discovered. The following values are supported: \- `pd`: Discovered within the last 24 hours. \- `pw`: Discovered within the last 7 Days. \- `pm`: Discovered within the last 31 Days. \- `py`: Discovered within the last 365 Days… \- `YYYY-MM-DDtoYYYY-MM-DD`: timeframe is also supported by specifying the date range e.g. `2022-04-01to2022-07-30`.  
extra_snippets| false| bool| |  A snippet is an excerpt from a page you get as a result of the query, and extra_snippets allow you to get up to 5 additional, alternative excerpts. Only available under `Free AI`, `Base AI`, `Pro AI`, `Base Data`, `Pro Data` and `Custom plans`.

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard) [ Documentation](/app/documentation)

[ Web Search](/app/documentation/web-search) [ Summarizer
Search](/app/documentation/summarizer-search) [ Image
Search](/app/documentation/image-search) [ Video
Search](/app/documentation/video-search) [ News
Search](/app/documentation/news-search)

[ Get Started](/app/documentation/news-search/get-started)[ Query
Parameters](/app/documentation/news-search/query)[ Request
Headers](/app/documentation/news-search/request-headers)[ Response
Headers](/app/documentation/news-search/response-headers)[ Response
Objects](/app/documentation/news-search/responses)[
Codes](/app/documentation/news-search/codes)[ API
Changelog](/app/documentation/news-search/api-changelog)

[ Suggest](/app/documentation/suggest) [
Spellcheck](/app/documentation/spellcheck) [
General](/app/documentation/general)

[Login](/login) [Register](/register)

###### Brave News Search API

## Request Headers

#### News Search API Request Headers

This table lists the request headers supported by the News Search API, most of
which are optional.

Header| Required| Name| Description  
---|---|---|---  
Accept| false| Accept|  The default supported media type is `application/json`  
Accept-Encoding| false| Accept Encoding|  The supported compression type is
`gzip`.  
Api-Version| false| Web Search API Version|  The Brave Web Search API version
to use. This is denoted by the format `YYYY-MM-DD`. The latest version is used
by default, and the previous ones can be found in the [API Changelog](./api-
changelog).  
Cache-Control| false| Cache Control|  Search will return cached web search
results by default. To prevent caching set the Cache-Control header to `no-
cache`. This is currently done as best effort.  
User-Agent| false| User Agent|  The user agent of the client sending the
request. Search can utilize the user agent to provide a different experience
depending on the client sending the request. The user agent should follow the
commonly used browser agent strings on each platform. For more information on
curating user agents, see [RFC 9110](https://www.rfc-
editor.org/rfc/rfc9110.html#name-user-agent). User agent string examples by
platform:

  * **Android** : Mozilla/5.0 (Linux; Android 13; Pixel 7 Pro) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Mobile Safari/537.36
  * **iOS** : Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Mobile/15E148 Safari/604.1
  * **macOS** : Mozilla/5.0 (Macintosh; Intel Mac OS X 12_0_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/
  * **Windows** : Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/

  
X-Subscription-Token| true| Authentication token|  The secret token for the
subscribed plan to authenticate the request. Can be obtained from [API
Keys](/app/keys).

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard) [ Documentation](/app/documentation)

[ Web Search](/app/documentation/web-search) [ Summarizer
Search](/app/documentation/summarizer-search) [ Image
Search](/app/documentation/image-search) [ Video
Search](/app/documentation/video-search) [ News
Search](/app/documentation/news-search)

[ Get Started](/app/documentation/news-search/get-started)[ Query
Parameters](/app/documentation/news-search/query)[ Request
Headers](/app/documentation/news-search/request-headers)[ Response
Headers](/app/documentation/news-search/response-headers)[ Response
Objects](/app/documentation/news-search/responses)[
Codes](/app/documentation/news-search/codes)[ API
Changelog](/app/documentation/news-search/api-changelog)

[ Suggest](/app/documentation/suggest) [
Spellcheck](/app/documentation/spellcheck) [
General](/app/documentation/general)

[Login](/login) [Register](/register)

###### Brave News Search API

## Response Headers

#### Global

This table lists the response headers supported by the News Search API.

Header| Name| Description  
---|---|---  
X-RateLimit-Limit| Rate Limit|  Rate limits associated with the requested
plan. An example rate limit `X-RateLimit-Limit: 1, 15000` means 1 request per
second and 15000 requests per month.  
X-RateLimit-Policy| Rate Limit Policy|  Rate limit policies currently
associated with the requested plan. An example policy `X-RateLimit-Policy:
1;w=1, 15000;w=2592000` means a limit of 1 request over a 1 second window and
15000 requests over a month window. The windows are always given in seconds.  
X-RateLimit-Remaining| Rate Limit Remaining|  Remaining quota units associated
with the expiring limits. An example remaining limit `X-RateLimit-Remaining:
1, 1000` indicates the API is able to be accessed once during the current
second, and 1000 times over the current month. **Note** : Only successful
requests are counted and billed.  
X-RateLimit-Reset| Rate Limit Reset|  The number of seconds until the quota
associated with the expiring limits resets. An example reset limit
`X-RateLimit-Reset: 1, 1419704` means a single request can be done again in a
second and in 1419704 seconds the full monthly quota associated with the plan
will be available again.

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard) [ Documentation](/app/documentation)

[ Web Search](/app/documentation/web-search) [ Summarizer
Search](/app/documentation/summarizer-search) [ Image
Search](/app/documentation/image-search) [ Video
Search](/app/documentation/video-search) [ News
Search](/app/documentation/news-search)

[ Get Started](/app/documentation/news-search/get-started)[ Query
Parameters](/app/documentation/news-search/query)[ Request
Headers](/app/documentation/news-search/request-headers)[ Response
Headers](/app/documentation/news-search/response-headers)[ Response
Objects](/app/documentation/news-search/responses)[
Codes](/app/documentation/news-search/codes)[ API
Changelog](/app/documentation/news-search/api-changelog)

[ Suggest](/app/documentation/suggest) [
Spellcheck](/app/documentation/spellcheck) [
General](/app/documentation/general)

[Login](/login) [Register](/register)

###### Brave News Search API

## Response Objects

#### # NewsSearchApiResponse

Top level response model for successful News Search API requests. The API can
also respond back with an error response based on invalid subscription keys
and rate limit events.

Field| Type| Description  
---|---|---  
type| "news"| The type of search API result. The value is always news.  
query| Query| News search query string.  
results| list [ NewsResult ]| The list of news results for the given query.  
  
#### # Query

A model representing information gathered around the requested query.

Field| Type| Description  
---|---|---  
original| string| The original query that was requested.  
altered| string| The altered query by the spellchecker. This is the query that
is used to search if any.  
cleaned| string| The cleaned noramlized query by the spellchecker. This is the
query that is used to search if any.  
spellcheck_off| bool| Whether the spell checker is enabled or disabled.  
show_strict_warning| bool| The value is True if the lack of results is due to
a 'strict' safesearch setting. Adult content relevant to the query was found,
but was blocked by safesearch.  
  
#### # NewsResult

A model representing a news result for the requested query.

Field| Type| Description  
---|---|---  
type| news_result| The type of news search API result. The value is always
news_result.  
url| string| The source url of the news article.  
title| string| The title of the news article.  
description| string| The description for the news article.  
age| string| A human readable representation of the page age.  
page_age| string| The page age found from the source web page.  
page_fetched| string| The iso date time when the page was last fetched. The
format is YYYY-MM-DDTHH:MM:SSZ  
breaking| bool| Whether the result includes breaking news.  
thumbnail| Thumbnail| The thumbnail for the news article.  
meta_url| MetaUrl| Aggregated information on the url associated with the news
search result.  
extra_snippets| list [ string ]| A list of extra alternate snippets for the
news search result.  
  
#### # Thumbnail

Aggregated details representing the news thumbnail

Field| Type| Description  
---|---|---  
src| string| The served url of the thumbnail associated with the news article.  
original| string| The original url of the thumbnail associated with the news
article.  
  
#### # MetaUrl

Aggregated information about a url.

Field| Type| Description  
---|---|---  
scheme| string| The protocol scheme extracted from the url.  
netloc| string| The network location part extracted from the url.  
hostname| string| The lowercased domain name extracted from the url.  
favicon| string| The favicon used for the url.  
path| string| The hierarchical path of the url useful as a display string.

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard) [ Documentation](/app/documentation)

[ Web Search](/app/documentation/web-search) [ Summarizer
Search](/app/documentation/summarizer-search) [ Image
Search](/app/documentation/image-search) [ Video
Search](/app/documentation/video-search) [ News
Search](/app/documentation/news-search)

[ Get Started](/app/documentation/news-search/get-started)[ Query
Parameters](/app/documentation/news-search/query)[ Request
Headers](/app/documentation/news-search/request-headers)[ Response
Headers](/app/documentation/news-search/response-headers)[ Response
Objects](/app/documentation/news-search/responses)[
Codes](/app/documentation/news-search/codes)[ API
Changelog](/app/documentation/news-search/api-changelog)

[ Suggest](/app/documentation/suggest) [
Spellcheck](/app/documentation/spellcheck) [
General](/app/documentation/general)

[Login](/login) [Register](/register)

###### Brave News Search API

## Codes

#### [#](/app/documentation/news-search/query#country) Country Codes

This table lists the country codes supported by the `country` parameter.

Country| Code  
---|---  
All Regions| ALL  
Argentina| AR  
Australia| AU  
Austria| AT  
Belgium| BE  
Brazil| BR  
Canada| CA  
Chile| CL  
Denmark| DK  
Finland| FI  
France| FR  
Germany| DE  
Hong Kong| HK  
India| IN  
Indonesia| ID  
Italy| IT  
Japan| JP  
Korea| KR  
Malaysia| MY  
Mexico| MX  
Netherlands| NL  
New Zealand| NZ  
Norway| NO  
Peoples Republic of China| CN  
Poland| PL  
Portugal| PT  
Republic of the Philippines| PH  
Russia| RU  
Saudi Arabia| SA  
South Africa| ZA  
Spain| ES  
Sweden| SE  
Switzerland| CH  
Taiwan| TW  
Turkey| TR  
United Kingdom| GB  
United States| US  
  
#### [#](/app/documentation/news-search/query#language) Language Codes

This table lists the language codes supported by the `search_lang` parameter.

Language| Code  
---|---  
Arabic| ar  
Basque| eu  
Bengali| bn  
Bulgarian| bg  
Catalan| ca  
Chinese Simplified| zh-hans  
Chinese Traditional| zh-hant  
Croatian| hr  
Czech| cs  
Danish| da  
Dutch| nl  
English| en  
English United Kingdom| en-gb  
Estonian| et  
Finnish| fi  
French| fr  
Galician| gl  
German| de  
Gujarati| gu  
Hebrew| he  
Hindi| hi  
Hungarian| hu  
Icelandic| is  
Italian| it  
Japanese| jp  
Kannada| kn  
Korean| ko  
Latvian| lv  
Lithuanian| lt  
Malay| ms  
Malayalam| ml  
Marathi| mr  
Norwegian Bokmål| nb  
Polish| pl  
Portuguese Brazil| pt-br  
Portuguese Portugal| pt-pt  
Punjabi| pa  
Romanian| ro  
Russian| ru  
Serbian Cyrylic| sr  
Slovak| sk  
Slovenian| sl  
Spanish| es  
Swedish| sv  
Tamil| ta  
Telugu| te  
Thai| th  
Turkish| tr  
Ukrainian| uk  
Vietnamese| vi  
  
#### [#](/app/documentation/news-search/query#market-code) Market Codes

This table lists the country language codes supported by the `ui_lang`
parameter.

Country| Language| Code  
---|---|---  
Argentina| Spanish| es-AR  
Australia| English| en-AU  
Austria| German| de-AT  
Belgium| Dutch| nl-BE  
Belgium| French| fr-BE  
Brazil| Portuguese| pt-BR  
Canada| English| en-CA  
Canada| French| fr-CA  
Chile| Spanish| es-CL  
Denmark| Danish| da-DK  
Finland| Finnish| fi-FI  
France| French| fr-FR  
Germany| German| de-DE  
Hong Kong SAR| Traditional Chinese| zh-HK  
India| English| en-IN  
Indonesia| English| en-ID  
Italy| Italian| it-IT  
Japan| Japanese| ja-JP  
Korea| Korean| ko-KR  
Malaysia| English| en-MY  
Mexico| Spanish| es-MX  
Netherlands| Dutch| nl-NL  
New Zealand| English| en-NZ  
Norway| Norwegian| no-NO  
People's republic of China| Chinese| zh-CN  
Poland| Polish| pl-PL  
Republic of the Philippines| English| en-PH  
Russia| Russian| ru-RU  
South Africa| English| en-ZA  
Spain| Spanish| es-ES  
Sweden| Swedish| sv-SE  
Switzerland| French| fr-CH  
Switzerland| German| de-CH  
Taiwan| Traditional Chinese| zh-TW  
Turkey| Turkish| tr-TR  
United Kingdom| English| en-GB  
United States| English| en-US  
United States| Spanish| es-US

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard) [ Documentation](/app/documentation)

[ Web Search](/app/documentation/web-search) [ Summarizer
Search](/app/documentation/summarizer-search) [ Image
Search](/app/documentation/image-search) [ Video
Search](/app/documentation/video-search) [ News
Search](/app/documentation/news-search)

[ Get Started](/app/documentation/news-search/get-started)[ Query
Parameters](/app/documentation/news-search/query)[ Request
Headers](/app/documentation/news-search/request-headers)[ Response
Headers](/app/documentation/news-search/response-headers)[ Response
Objects](/app/documentation/news-search/responses)[
Codes](/app/documentation/news-search/codes)[ API
Changelog](/app/documentation/news-search/api-changelog)

[ Suggest](/app/documentation/suggest) [
Spellcheck](/app/documentation/spellcheck) [
General](/app/documentation/general)

[Login](/login) [Register](/register)

# Brave Search API Changelog - News

This changelog lists all updates to the Brave News Search API in chronological
order.

### 2023-08-15

  * Add news search.

================
File: resources/brave/brave_video.md
================
[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard) [ Documentation](/app/documentation)

[ Web Search](/app/documentation/web-search) [ Summarizer
Search](/app/documentation/summarizer-search) [ Image
Search](/app/documentation/image-search) [ Video
Search](/app/documentation/video-search)

[ Get Started](/app/documentation/video-search/get-started)[ Query
Parameters](/app/documentation/video-search/query)[ Request
Headers](/app/documentation/video-search/request-headers)[ Response
Headers](/app/documentation/video-search/response-headers)[ Response
Objects](/app/documentation/video-search/responses)[
Codes](/app/documentation/video-search/codes)[ API
Changelog](/app/documentation/video-search/api-changelog)

[ News Search](/app/documentation/news-search) [
Suggest](/app/documentation/suggest) [
Spellcheck](/app/documentation/spellcheck) [
General](/app/documentation/general)

[Login](/login) [Register](/register)

# Brave Video Search API

Brave Search API is a REST API to query Brave Search and get back search
results from the web. The following sections describe how to curate requests,
including parameters and headers, to Brave Search API and get a JSON response
back.

> To try the API on a Free plan, you’ll still need to subscribe — you simply
> won’t be charged. Once subscribed, you can get an API key in the [API Keys
> section](/app/keys).

## Endpoints

Brave Search API exposes multiple endpoints for specific types of data, based
on the level of your subscription. If you don’t see the endpoint you’re
interested in, you may need to change your subscription.

Brave Video Search API is currently available at the following endpoint and
exposes an API to get videos from the web relevant to the query.

    
    
    https://api.search.brave.com/res/v1/videos/search
    

## Example

Get started immediately with CURL. An example request will look something like
this:

    
    
    curl -s --compressed "https://api.search.brave.com/res/v1/videos/search?q=munich&count=10&country=us&search_lang=en&spellcheck=1" \
      -H "Accept: application/json" \
      -H "Accept-Encoding: gzip" \
      -H "X-Subscription-Token: <YOUR_API_KEY>"
    

## Next Steps

To learn what parameters are available and what responses can be expected
while querying Brave Search, please review the following pages:

  * [Query Parameters](/app/documentation/video-search/query)
  * [Request Headers](/app/documentation/video-search/request-headers)
  * [Response Headers](/app/documentation/video-search/response-headers)
  * [Response Objects](/app/documentation/video-search/responses)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard) [ Documentation](/app/documentation)

[ Web Search](/app/documentation/web-search) [ Summarizer
Search](/app/documentation/summarizer-search) [ Image
Search](/app/documentation/image-search) [ Video
Search](/app/documentation/video-search)

[ Get Started](/app/documentation/video-search/get-started)[ Query
Parameters](/app/documentation/video-search/query)[ Request
Headers](/app/documentation/video-search/request-headers)[ Response
Headers](/app/documentation/video-search/response-headers)[ Response
Objects](/app/documentation/video-search/responses)[
Codes](/app/documentation/video-search/codes)[ API
Changelog](/app/documentation/video-search/api-changelog)

[ News Search](/app/documentation/news-search) [
Suggest](/app/documentation/suggest) [
Spellcheck](/app/documentation/spellcheck) [
General](/app/documentation/general)

[Login](/login) [Register](/register)

###### Brave Video Search API

## Query Parameters

#### # Video Search API

This table lists the query parameters supported by the Video Search API. Some
are required, but most are optional.

Parameter| Required| Type| Default| Description  
---|---|---|---|---  
q| true| string| |  The user’s search query term. Query can not be empty. Maximum of 400 characters and 50 words in the query.  
country| false| string| US|  The search query country, where the results come
from. The country string is limited to 2 character country codes of supported
countries. For a list of supported values, see [Country
Codes](/app/documentation/video-search/codes#country-codes).  
search_lang| false| string| en|  The search language preference. The 2 or more
character language code for which the search results are provided. For a list
of possible values, see [Language Codes](/app/documentation/video-
search/codes#language-codes).  
ui_lang| false| string| en-US|  User interface language preferred in response.
Usually of the format ‘<language_code>-<country_code>’. For more, see [RFC
9110](https://www.rfc-editor.org/rfc/rfc9110.html#name-accept-language). For a
list of supported values, see [UI Language Codes](/app/documentation/video-
search/codes#market-codes).  
count| false| number| 20|  The number of search results returned in response.
The maximum is `50`. The actual number delivered may be less than requested.
Combine this parameter with `offset` to paginate search results.  
offset| false| number| 0|  The zero based offset that indicates number of
search results per page (count) to skip before returning the result. The
maximum is `9`. The actual number delivered may be less than requested based
on the query. In order to paginate results use this parameter together with
`count`. For example, if your user interface displays 20 search results per
page, set `count` to `20` and offset to `0` to show the first page of results.
To get subsequent pages, increment `offset` by 1 (e.g. 0, 1, 2). The results
may overlap across multiple pages.  
spellcheck| false| bool| 1|  Whether to spellcheck provided query. If the
spellchecker is enabled, the modified query is always used for search. The
modified query can be found in `altered` key from the
[query](/app/documentation/video-search/responses#Query) response model.  
safesearch| false| string| moderate|  Filters search results for adult
content. The following values are supported:

  * `off` \- No filtering.
  * `moderate` \- Filter out explicit content.
  * `strict` \- Filter out explicit and suggestive content.

  
freshness| false| string| |  Filters search results by when they were discovered. The following values are supported: \- `pd`: Discovered within the last 24 hours. \- `pw`: Discovered within the last 7 Days. \- `pm`: Discovered within the last 31 Days. \- `py`: Discovered within the last 365 Days… \- `YYYY-MM-DDtoYYYY-MM-DD`: timeframe is also supported by specifying the date range e.g. `2022-04-01to2022-07-30`.

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard) [ Documentation](/app/documentation)

[ Web Search](/app/documentation/web-search) [ Summarizer
Search](/app/documentation/summarizer-search) [ Image
Search](/app/documentation/image-search) [ Video
Search](/app/documentation/video-search)

[ Get Started](/app/documentation/video-search/get-started)[ Query
Parameters](/app/documentation/video-search/query)[ Request
Headers](/app/documentation/video-search/request-headers)[ Response
Headers](/app/documentation/video-search/response-headers)[ Response
Objects](/app/documentation/video-search/responses)[
Codes](/app/documentation/video-search/codes)[ API
Changelog](/app/documentation/video-search/api-changelog)

[ News Search](/app/documentation/news-search) [
Suggest](/app/documentation/suggest) [
Spellcheck](/app/documentation/spellcheck) [
General](/app/documentation/general)

[Login](/login) [Register](/register)

###### Brave Video Search API

## Request Headers

#### Video Search API Request Headers

This table lists the request headers supported by the Video Search API, most
of which are optional.

Header| Required| Name| Description  
---|---|---|---  
Accept| false| Accept|  The default supported media type is `application/json`  
Accept-Encoding| false| Accept Encoding|  The supported compression type is
`gzip`.  
Api-Version| false| Web Search API Version|  The Brave Web Search API version
to use. This is denoted by the format `YYYY-MM-DD`. The latest version is used
by default, and the previous ones can be found in the [API Changelog](./api-
changelog).  
Cache-Control| false| Cache Control|  Search will return cached web search
results by default. To prevent caching set the Cache-Control header to `no-
cache`. This is currently done as best effort.  
User-Agent| false| User Agent|  The user agent of the client sending the
request. Search can utilize the user agent to provide a different experience
depending on the client sending the request. The user agent should follow the
commonly used browser agent strings on each platform. For more information on
curating user agents, see [RFC 9110](https://www.rfc-
editor.org/rfc/rfc9110.html#name-user-agent). User agent string examples by
platform:

  * **Android** : Mozilla/5.0 (Linux; Android 13; Pixel 7 Pro) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Mobile Safari/537.36
  * **iOS** : Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Mobile/15E148 Safari/604.1
  * **macOS** : Mozilla/5.0 (Macintosh; Intel Mac OS X 12_0_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/
  * **Windows** : Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/

  
X-Subscription-Token| true| Authentication token|  The secret token for the
subscribed plan to authenticate the request. Can be obtained from [API
Keys](/app/keys).

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard) [ Documentation](/app/documentation)

[ Web Search](/app/documentation/web-search) [ Summarizer
Search](/app/documentation/summarizer-search) [ Image
Search](/app/documentation/image-search) [ Video
Search](/app/documentation/video-search)

[ Get Started](/app/documentation/video-search/get-started)[ Query
Parameters](/app/documentation/video-search/query)[ Request
Headers](/app/documentation/video-search/request-headers)[ Response
Headers](/app/documentation/video-search/response-headers)[ Response
Objects](/app/documentation/video-search/responses)[
Codes](/app/documentation/video-search/codes)[ API
Changelog](/app/documentation/video-search/api-changelog)

[ News Search](/app/documentation/news-search) [
Suggest](/app/documentation/suggest) [
Spellcheck](/app/documentation/spellcheck) [
General](/app/documentation/general)

[Login](/login) [Register](/register)

###### Brave Video Search API

## Response Headers

#### Global

This table lists the response headers supported by the Video Search API.

Header| Name| Description  
---|---|---  
X-RateLimit-Limit| Rate Limit|  Rate limits associated with the requested
plan. An example rate limit `X-RateLimit-Limit: 1, 15000` means 1 request per
second and 15000 requests per month.  
X-RateLimit-Policy| Rate Limit Policy|  Rate limit policies currently
associated with the requested plan. An example policy `X-RateLimit-Policy:
1;w=1, 15000;w=2592000` means a limit of 1 request over a 1 second window and
15000 requests over a month window. The windows are always given in seconds.  
X-RateLimit-Remaining| Rate Limit Remaining|  Remaining quota units associated
with the expiring limits. An example remaining limit `X-RateLimit-Remaining:
1, 1000` indicates the API is able to be accessed once during the current
second, and 1000 times over the current month. **Note** : Only successful
requests are counted and billed.  
X-RateLimit-Reset| Rate Limit Reset|  The number of seconds until the quota
associated with the expiring limits resets. An example reset limit
`X-RateLimit-Reset: 1, 1419704` means a single request can be done again in a
second and in 1419704 seconds the full monthly quota associated with the plan
will be available again.

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard) [ Documentation](/app/documentation)

[ Web Search](/app/documentation/web-search) [ Summarizer
Search](/app/documentation/summarizer-search) [ Image
Search](/app/documentation/image-search) [ Video
Search](/app/documentation/video-search)

[ Get Started](/app/documentation/video-search/get-started)[ Query
Parameters](/app/documentation/video-search/query)[ Request
Headers](/app/documentation/video-search/request-headers)[ Response
Headers](/app/documentation/video-search/response-headers)[ Response
Objects](/app/documentation/video-search/responses)[
Codes](/app/documentation/video-search/codes)[ API
Changelog](/app/documentation/video-search/api-changelog)

[ News Search](/app/documentation/news-search) [
Suggest](/app/documentation/suggest) [
Spellcheck](/app/documentation/spellcheck) [
General](/app/documentation/general)

[Login](/login) [Register](/register)

###### Brave Video Search API

## Response Objects

#### # VideoSearchApiResponse

Top level response model for successful Video Search API requests. The API can
also respond back with an error response based on invalid subscription keys
and rate limit events.

Field| Type| Description  
---|---|---  
type| "videos"| The type of search API result. The value is always video.  
query| Query| Video search query string.  
results| list [ VideoResult ]| The list of video results for the given query.  
  
#### # Query

A model representing information gathered around the requested query.

Field| Type| Description  
---|---|---  
original| string| The original query that was requested.  
altered| string| The altered query by the spellchecker. This is the query that
is used to search if any.  
cleaned| string| The cleaned noramlized query by the spellchecker. This is the
query that is used to search if any.  
spellcheck_off| bool| Whether the spell checker is enabled or disabled.  
show_strict_warning| string| The value is True if the lack of results is due
to a 'strict' safesearch setting. Adult content relevant to the query was
found, but was blocked by safesearch.  
  
#### # VideoResult

A model representing a video result for the requested query.

Field| Type| Description  
---|---|---  
type| video_result| The type of video search API result. The value is always
video_result.  
url| string| The source url of the video.  
title| string| The title of the video.  
description| string| The description for the video.  
age| string| A human readable representation of the page age.  
page_age| string| The page age found from the source web page.  
page_fetched| string| The iso date time when the page was last fetched. The
format is YYYY-MM-DDTHH:MM:SSZ  
thumbnail| Thumbnail| The thumbnail for the video.  
video| VideoData| Metadata for the video.  
meta_url| MetaUrl| Aggregated information on the url associated with the video
search result.  
  
#### # Thumbnail

Aggregated details representing the video thumbnail

Field| Type| Description  
---|---|---  
src| string| The served url of the thumbnail associated with the video.  
original| string| The original url of the thumbnail associated with the video.  
  
#### # VideoData

A model representing metadata gathered for a video.

Field| Type| Description  
---|---|---  
duration| string| A time string representing the duration of the video.  
views| int| The number of views of the video.  
creator| string| The creator of the video.  
publisher| string| The publisher of the video.  
requires_subscription| bool| Whether the video requires a subscription.  
tags| list [ string ]| A list of tags relevant to the video.  
author| Profile| A list of profiles associated with the video.  
  
#### # Profile

A profile of an entity associated with the video.

Field| Type| Description  
---|---|---  
name| string| The name of the profile.  
long_name| string| The long name of the profile.  
url| string| The original url where the profile is available.  
img| string| The served image url representing the profile.  
  
#### # MetaUrl

Aggregated information about a url.

Field| Type| Description  
---|---|---  
scheme| string| The protocol scheme extracted from the url.  
netloc| string| The network location part extracted from the url.  
hostname| string| The lowercased domain name extracted from the url.  
favicon| string| The favicon used for the url.  
path| string| The hierarchical path of the url useful as a display string.

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard) [ Documentation](/app/documentation)

[ Web Search](/app/documentation/web-search) [ Summarizer
Search](/app/documentation/summarizer-search) [ Image
Search](/app/documentation/image-search) [ Video
Search](/app/documentation/video-search)

[ Get Started](/app/documentation/video-search/get-started)[ Query
Parameters](/app/documentation/video-search/query)[ Request
Headers](/app/documentation/video-search/request-headers)[ Response
Headers](/app/documentation/video-search/response-headers)[ Response
Objects](/app/documentation/video-search/responses)[
Codes](/app/documentation/video-search/codes)[ API
Changelog](/app/documentation/video-search/api-changelog)

[ News Search](/app/documentation/news-search) [
Suggest](/app/documentation/suggest) [
Spellcheck](/app/documentation/spellcheck) [
General](/app/documentation/general)

[Login](/login) [Register](/register)

###### Brave Video Search API

## Codes

#### [#](/app/documentation/video-search/query#country) Country Codes

This table lists the country codes supported by the `country` parameter.

Country| Code  
---|---  
All Regions| ALL  
Argentina| AR  
Australia| AU  
Austria| AT  
Belgium| BE  
Brazil| BR  
Canada| CA  
Chile| CL  
Denmark| DK  
Finland| FI  
France| FR  
Germany| DE  
Hong Kong| HK  
India| IN  
Indonesia| ID  
Italy| IT  
Japan| JP  
Korea| KR  
Malaysia| MY  
Mexico| MX  
Netherlands| NL  
New Zealand| NZ  
Norway| NO  
Peoples Republic of China| CN  
Poland| PL  
Portugal| PT  
Republic of the Philippines| PH  
Russia| RU  
Saudi Arabia| SA  
South Africa| ZA  
Spain| ES  
Sweden| SE  
Switzerland| CH  
Taiwan| TW  
Turkey| TR  
United Kingdom| GB  
United States| US  
  
#### [#](/app/documentation/video-search/query#language) Language Codes

This table lists the language codes supported by the `search_lang` parameter.

Language| Code  
---|---  
Arabic| ar  
Basque| eu  
Bengali| bn  
Bulgarian| bg  
Catalan| ca  
Chinese Simplified| zh-hans  
Chinese Traditional| zh-hant  
Croatian| hr  
Czech| cs  
Danish| da  
Dutch| nl  
English| en  
English United Kingdom| en-gb  
Estonian| et  
Finnish| fi  
French| fr  
Galician| gl  
German| de  
Gujarati| gu  
Hebrew| he  
Hindi| hi  
Hungarian| hu  
Icelandic| is  
Italian| it  
Japanese| jp  
Kannada| kn  
Korean| ko  
Latvian| lv  
Lithuanian| lt  
Malay| ms  
Malayalam| ml  
Marathi| mr  
Norwegian Bokmål| nb  
Polish| pl  
Portuguese Brazil| pt-br  
Portuguese Portugal| pt-pt  
Punjabi| pa  
Romanian| ro  
Russian| ru  
Serbian Cyrylic| sr  
Slovak| sk  
Slovenian| sl  
Spanish| es  
Swedish| sv  
Tamil| ta  
Telugu| te  
Thai| th  
Turkish| tr  
Ukrainian| uk  
Vietnamese| vi  
  
#### [#](/app/documentation/video-search/query#market-code) Market Codes

This table lists the country language codes supported by the `ui_lang`
parameter.

Country| Language| Code  
---|---|---  
Argentina| Spanish| es-AR  
Australia| English| en-AU  
Austria| German| de-AT  
Belgium| Dutch| nl-BE  
Belgium| French| fr-BE  
Brazil| Portuguese| pt-BR  
Canada| English| en-CA  
Canada| French| fr-CA  
Chile| Spanish| es-CL  
Denmark| Danish| da-DK  
Finland| Finnish| fi-FI  
France| French| fr-FR  
Germany| German| de-DE  
Hong Kong SAR| Traditional Chinese| zh-HK  
India| English| en-IN  
Indonesia| English| en-ID  
Italy| Italian| it-IT  
Japan| Japanese| ja-JP  
Korea| Korean| ko-KR  
Malaysia| English| en-MY  
Mexico| Spanish| es-MX  
Netherlands| Dutch| nl-NL  
New Zealand| English| en-NZ  
Norway| Norwegian| no-NO  
People's republic of China| Chinese| zh-CN  
Poland| Polish| pl-PL  
Republic of the Philippines| English| en-PH  
Russia| Russian| ru-RU  
South Africa| English| en-ZA  
Spain| Spanish| es-ES  
Sweden| Swedish| sv-SE  
Switzerland| French| fr-CH  
Switzerland| German| de-CH  
Taiwan| Traditional Chinese| zh-TW  
Turkey| Turkish| tr-TR  
United Kingdom| English| en-GB  
United States| English| en-US  
United States| Spanish| es-US

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard) [ Documentation](/app/documentation)

[ Web Search](/app/documentation/web-search) [ Summarizer
Search](/app/documentation/summarizer-search) [ Image
Search](/app/documentation/image-search) [ Video
Search](/app/documentation/video-search)

[ Get Started](/app/documentation/video-search/get-started)[ Query
Parameters](/app/documentation/video-search/query)[ Request
Headers](/app/documentation/video-search/request-headers)[ Response
Headers](/app/documentation/video-search/response-headers)[ Response
Objects](/app/documentation/video-search/responses)[
Codes](/app/documentation/video-search/codes)[ API
Changelog](/app/documentation/video-search/api-changelog)

[ News Search](/app/documentation/news-search) [
Suggest](/app/documentation/suggest) [
Spellcheck](/app/documentation/spellcheck) [
General](/app/documentation/general)

[Login](/login) [Register](/register)

# Brave Search API Changelog - Videos

This changelog lists all updates to the Brave Video Search API in
chronological order.

### 2023-08-15

  * Add video search.

================
File: resources/brave/brave.md
================
[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard) [ Documentation](/app/documentation)

[ Web Search](/app/documentation/web-search)

[ Get Started](/app/documentation/web-search/get-started)[ Query
Parameters](/app/documentation/web-search/query)[ Request
Headers](/app/documentation/web-search/request-headers)[ Response
Headers](/app/documentation/web-search/response-headers)[ Response
Objects](/app/documentation/web-search/responses)[
Codes](/app/documentation/web-search/codes)[ API
Changelog](/app/documentation/web-search/api-changelog)

[ Summarizer Search](/app/documentation/summarizer-search) [ Image
Search](/app/documentation/image-search) [ Video
Search](/app/documentation/video-search) [ News
Search](/app/documentation/news-search) [ Suggest](/app/documentation/suggest)
[ Spellcheck](/app/documentation/spellcheck) [
General](/app/documentation/general)

[Login](/login) [Register](/register)

# Web Search API - Changelog

This changelog outlines all significant changes to the Brave Web Search API in
chronological order.

### 2023-01-01

  * Add Brave Web Search API resource.

### 2023-04-14

  * Change `SearchResult` restaurant property to `location`.

### 2023-10-11

  * Add `spellcheck` flag.

# Local Search API - Changelog

This changelog outlines all significant changes to the Brave Local Search API
in chronological order.

### 2024-06-11

  * Add Brave Local Search API resource.

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard) [ Documentation](/app/documentation)

[ Web Search](/app/documentation/web-search)

[ Get Started](/app/documentation/web-search/get-started)[ Query
Parameters](/app/documentation/web-search/query)[ Request
Headers](/app/documentation/web-search/request-headers)[ Response
Headers](/app/documentation/web-search/response-headers)[ Response
Objects](/app/documentation/web-search/responses)[
Codes](/app/documentation/web-search/codes)[ API
Changelog](/app/documentation/web-search/api-changelog)

[ Summarizer Search](/app/documentation/summarizer-search) [ Image
Search](/app/documentation/image-search) [ Video
Search](/app/documentation/video-search) [ News
Search](/app/documentation/news-search) [ Suggest](/app/documentation/suggest)
[ Spellcheck](/app/documentation/spellcheck) [
General](/app/documentation/general)

[Login](/login) [Register](/register)

###### Brave Web Search API

## Codes

#### [#](/app/documentation/web-search/query#country) Country Codes

This table lists the country codes supported by the `country` parameter.

Country| Code  
---|---  
All Regions| ALL  
Argentina| AR  
Australia| AU  
Austria| AT  
Belgium| BE  
Brazil| BR  
Canada| CA  
Chile| CL  
Denmark| DK  
Finland| FI  
France| FR  
Germany| DE  
Hong Kong| HK  
India| IN  
Indonesia| ID  
Italy| IT  
Japan| JP  
Korea| KR  
Malaysia| MY  
Mexico| MX  
Netherlands| NL  
New Zealand| NZ  
Norway| NO  
Peoples Republic of China| CN  
Poland| PL  
Portugal| PT  
Republic of the Philippines| PH  
Russia| RU  
Saudi Arabia| SA  
South Africa| ZA  
Spain| ES  
Sweden| SE  
Switzerland| CH  
Taiwan| TW  
Turkey| TR  
United Kingdom| GB  
United States| US  
  
#### [#](/app/documentation/web-search/query#language) Language Codes

This table lists the language codes supported by the `search_lang` parameter.

Language| Code  
---|---  
Arabic| ar  
Basque| eu  
Bengali| bn  
Bulgarian| bg  
Catalan| ca  
Chinese Simplified| zh-hans  
Chinese Traditional| zh-hant  
Croatian| hr  
Czech| cs  
Danish| da  
Dutch| nl  
English| en  
English United Kingdom| en-gb  
Estonian| et  
Finnish| fi  
French| fr  
Galician| gl  
German| de  
Gujarati| gu  
Hebrew| he  
Hindi| hi  
Hungarian| hu  
Icelandic| is  
Italian| it  
Japanese| jp  
Kannada| kn  
Korean| ko  
Latvian| lv  
Lithuanian| lt  
Malay| ms  
Malayalam| ml  
Marathi| mr  
Norwegian Bokmål| nb  
Polish| pl  
Portuguese Brazil| pt-br  
Portuguese Portugal| pt-pt  
Punjabi| pa  
Romanian| ro  
Russian| ru  
Serbian Cyrylic| sr  
Slovak| sk  
Slovenian| sl  
Spanish| es  
Swedish| sv  
Tamil| ta  
Telugu| te  
Thai| th  
Turkish| tr  
Ukrainian| uk  
Vietnamese| vi  
  
#### [#](/app/documentation/web-search/query#market-code) Market Codes

This table lists the country language codes supported by the `ui_lang`
parameter.

Country| Language| Code  
---|---|---  
Argentina| Spanish| es-AR  
Australia| English| en-AU  
Austria| German| de-AT  
Belgium| Dutch| nl-BE  
Belgium| French| fr-BE  
Brazil| Portuguese| pt-BR  
Canada| English| en-CA  
Canada| French| fr-CA  
Chile| Spanish| es-CL  
Denmark| Danish| da-DK  
Finland| Finnish| fi-FI  
France| French| fr-FR  
Germany| German| de-DE  
Hong Kong SAR| Traditional Chinese| zh-HK  
India| English| en-IN  
Indonesia| English| en-ID  
Italy| Italian| it-IT  
Japan| Japanese| ja-JP  
Korea| Korean| ko-KR  
Malaysia| English| en-MY  
Mexico| Spanish| es-MX  
Netherlands| Dutch| nl-NL  
New Zealand| English| en-NZ  
Norway| Norwegian| no-NO  
People's republic of China| Chinese| zh-CN  
Poland| Polish| pl-PL  
Republic of the Philippines| English| en-PH  
Russia| Russian| ru-RU  
South Africa| English| en-ZA  
Spain| Spanish| es-ES  
Sweden| Swedish| sv-SE  
Switzerland| French| fr-CH  
Switzerland| German| de-CH  
Taiwan| Traditional Chinese| zh-TW  
Turkey| Turkish| tr-TR  
United Kingdom| English| en-GB  
United States| English| en-US  
United States| Spanish| es-US

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard) [ Documentation](/app/documentation)

[ Web Search](/app/documentation/web-search)

[ Get Started](/app/documentation/web-search/get-started)[ Query
Parameters](/app/documentation/web-search/query)[ Request
Headers](/app/documentation/web-search/request-headers)[ Response
Headers](/app/documentation/web-search/response-headers)[ Response
Objects](/app/documentation/web-search/responses)[
Codes](/app/documentation/web-search/codes)[ API
Changelog](/app/documentation/web-search/api-changelog)

[ Summarizer Search](/app/documentation/summarizer-search) [ Image
Search](/app/documentation/image-search) [ Video
Search](/app/documentation/video-search) [ News
Search](/app/documentation/news-search) [ Suggest](/app/documentation/suggest)
[ Spellcheck](/app/documentation/spellcheck) [
General](/app/documentation/general)

[Login](/login) [Register](/register)

# Brave Web Search API

## Introduction

Brave Web Search API is a REST API to query Brave Search and get back search
results from the web. The following sections describe how to curate requests,
including parameters and headers, to Brave Web Search API and get a JSON
response back.

> To try the API on a Free plan, you’ll still need to subscribe — you simply
> won’t be charged. Once subscribed, you can get an API key in the [API Keys
> section](/app/keys).

## Endpoints

Brave Search API exposes multiple endpoints for specific types of data, based
on the level of your subscription. If you don’t see the endpoint you’re
interested in, you may need to change your subscription.

    
    
    https://api.search.brave.com/res/v1/web/search
    

## Example

A request has to be made to the web search endpoint. An example CURL request
is given below.

    
    
    
    curl -s --compressed "https://api.search.brave.com/res/v1/web/search?q=brave+search" \
      -H "Accept: application/json" \
      -H "Accept-Encoding: gzip" \
      -H "X-Subscription-Token: <YOUR_API_KEY>"
    

The response specification for Web Search API can be seen in the
[WebSearchApiResponse](/app/documentation/web-
search/responses#WebSearchApiResponse) model.

## Next Steps

To learn what parameters are available and what responses can be expected
while querying Brave Web Search API, please review the following pages:

  * [Query Parameters](/app/documentation/web-search/query#WebSearchAPIQueryParameters)
  * [Request Headers](/app/documentation/web-search/request-headers#WebSearchAPIRequestHeaders)
  * [Response Headers](/app/documentation/web-search/response-headers)
  * [Response Objects](/app/documentation/web-search/responses)

# Brave Local Search API

## Introduction

Brave Local Search API provides enrichments for location search results.

> Access to Local API is available through the [Pro
> plans](/app/subscriptions/subscribe?tab=normal).

## Endpoints

Brave Local Search API is currently available at the following endpoints and
exposes an API to get extra information about a location, including pictures
and related web results.

    
    
    https://api.search.brave.com/res/v1/local/pois
    

The endpoint supports batching and retrieval of extra information of up to 20
locations with a single request.

The local API also includes an endpoint to get an AI generated description for
a location.

    
    
    https://api.search.brave.com/res/v1/local/descriptions
    

## Example

An initial request has to be made to web search endpoint with a given query.
An example CURL request is given below.

    
    
    
    curl -s --compressed "https://api.search.brave.com/res/v1/web/search?q=greek+restaurants+in+san+francisco" \
      -H "Accept: application/json" \
      -H "Accept-Encoding: gzip" \
      -H "X-Subscription-Token: <YOUR_API_KEY>"
    

If the query returns a list of locations, as in this case, each location
result has an [id field](/app/documentation/web-
search/responses#LocationResult), which is a temporary ID that can be used to
retrieve extra information about the location. An example from the locations
result is given below.

    
    
    {
      "locations": {
        "results": [
          {
            "id": "1520066f3f39496780c5931d9f7b26a6",
            "title": "Pangea Banquet Mediterranean Food"
          },
          {
            "id": "d00b153c719a427ea515f9eacf4853a2",
            "title": "Park Mediterranean Grill"
          },
          {
            "id": "4b943b378725432aa29f019def0f0154",
            "title": "The Halal Mediterranean Co."
          }
        ]
      }
    }
    

The `id` value can be used to further fetch extra information about the
location. An example request is given below.

    
    
    
    curl -s --compressed "https://api.search.brave.com/res/v1/local/pois?ids=1520066f3f39496780c5931d9f7b26a6&ids=d00b153c719a427ea515f9eacf4853a2" \
      -H "accept: application/json" \
      -H "Accept-Encoding: gzip" \
      -H "x-subscription-token: <YOUR_API_KEY>"
    

An AI generated description associated with a location can be further fetched
using the example below.

    
    
    
    curl -s --compressed "https://api.search.brave.com/res/v1/local/descriptions?ids=1520066f3f39496780c5931d9f7b26a6&ids=d00b153c719a427ea515f9eacf4853a2" \
      -H "accept: application/json" \
      -H "Accept-Encoding: gzip" \
      -H "x-subscription-token: <YOUR_API_KEY>"
    

The response specification for Local Search API can be seen in the
[LocalPoiSearchApiResponse](/app/documentation/web-
search/responses#LocalPoiSearchApiResponse) and
[LocalDescriptionsSearchApiResponse](/app/documentation/web-
search/responses#LocalDescriptionsSearchApiResponse) models.

## Next Steps

To learn what parameters are available and what responses can be expected
while querying Brave Web Search API, please review the following pages:

  * [Query Parameters](/app/documentation/web-search/query#LocalSearchAPIQueryParameters)
  * [Request Headers](/app/documentation/web-search/request-headers#LocalSearchAPIRequestHeaders)
  * [Response Headers](/app/documentation/web-search/response-headers)
  * [Response Objects](/app/documentation/web-search/responses)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard) [ Documentation](/app/documentation)

[ Web Search](/app/documentation/web-search)

[ Get Started](/app/documentation/web-search/get-started)[ Query
Parameters](/app/documentation/web-search/query)[ Request
Headers](/app/documentation/web-search/request-headers)[ Response
Headers](/app/documentation/web-search/response-headers)[ Response
Objects](/app/documentation/web-search/responses)[
Codes](/app/documentation/web-search/codes)[ API
Changelog](/app/documentation/web-search/api-changelog)

[ Summarizer Search](/app/documentation/summarizer-search) [ Image
Search](/app/documentation/image-search) [ Video
Search](/app/documentation/video-search) [ News
Search](/app/documentation/news-search) [ Suggest](/app/documentation/suggest)
[ Spellcheck](/app/documentation/spellcheck) [
General](/app/documentation/general)

[Login](/login) [Register](/register)

###### Brave Web Search API

## Query Parameters

#### # Web Search API

This table lists the query parameters supported by the Web Search API. Some
are required, but most are optional.

Parameter| Required| Type| Default| Description  
---|---|---|---|---  
q| true| string| |  The user’s search query term. Query can not be empty. Maximum of 400 characters and 50 words in the query.  
country| false| string| US|  The search query country, where the results come
from. The country string is limited to 2 character country codes of supported
countries. For a list of supported values, see [Country
Codes](/app/documentation/web-search/codes#country-codes).  
search_lang| false| string| en|  The search language preference. The 2 or more
character language code for which the search results are provided. For a list
of possible values, see [Language Codes](/app/documentation/web-
search/codes#language-codes).  
ui_lang| false| string| en-US|  User interface language preferred in response.
Usually of the format ‘<language_code>-<country_code>’. For more, see [RFC
9110](https://www.rfc-editor.org/rfc/rfc9110.html#name-accept-language). For a
list of supported values, see [UI Language Codes](/app/documentation/web-
search/codes#market-codes).  
count| false| number| 20|  The number of search results returned in response.
The maximum is `20`. The actual number delivered may be less than requested.
Combine this parameter with `offset` to paginate search results.  
offset| false| number| 0|  The zero based offset that indicates number of
search results per page (count) to skip before returning the result. The
maximum is `9`. The actual number delivered may be less than requested based
on the query. In order to paginate results use this parameter together with
`count`. For example, if your user interface displays 20 search results per
page, set `count` to `20` and offset to `0` to show the first page of results.
To get subsequent pages, increment `offset` by 1 (e.g. 0, 1, 2). The results
may overlap across multiple pages.  
safesearch| false| string| moderate|  Filters search results for adult
content. The following values are supported:

  * `off`: No filtering is done.
  * `moderate`: Filters explicit content, like images and videos, but allows adult domains in the search results.
  * `strict`: Drops all adult content from search results.

  
freshness| false| string| |  Filters search results by when they were discovered. The following values are supported: \- `pd`: Discovered within the last 24 hours. \- `pw`: Discovered within the last 7 Days. \- `pm`: Discovered within the last 31 Days. \- `py`: Discovered within the last 365 Days… \- `YYYY-MM-DDtoYYYY-MM-DD`: timeframe is also supported by specifying the date range e.g. `2022-04-01to2022-07-30`.  
text_decorations| false| bool| 1|  Whether display strings (e.g. result
snippets) should include decoration markers (e.g. highlighting characters).  
spellcheck| false| bool| 1|  Whether to spellcheck provided query. If the
spellchecker is enabled, the modified query is always used for search. The
modified query can be found in `altered` key from the
[query](/app/documentation/web-search/responses#Query) response model.  
result_filter| false| string| |  A comma delimited string of result types to include in the search response. Not specifying this parameter will return back all result types in search response where data is available and a plan with the corresponding option is subscribed. The response always includes query and type to identify any query modifications and response type respectively. Available result filter values are: \- discussions \- faq \- infobox \- news \- query \- summarizer \- videos \- web \- locations Example result filter param `result_filter=discussions`, `videos` returns only `discussions`, and videos responses. Another example where only location results are required, set the `result_filter` param to `result_filter=locations`.  
goggles_id| false| string| |  Goggles act as a custom re-ranking on top of Brave’s search index. For more details, refer to the [Goggles repository](https://github.com/brave/goggles-quickstart). This parameter is deprecated. Please use the goggles parameter.  
goggles| false| string| |  Goggles act as a custom re-ranking on top of Brave’s search index. The parameter supports both a url where the Goggle is hosted or the definition of the goggle. For more details, refer to the [Goggles repository](https://github.com/brave/goggles-quickstart).  
units| false| string| |  The measurement units. If not provided, units are derived from search country. Possible values are: \- `metric`: The standardized measurement system \- `imperial`: The British Imperial system of units.  
extra_snippets| false| bool| |  A snippet is an excerpt from a page you get as a result of the query, and extra_snippets allow you to get up to 5 additional, alternative excerpts. Only available under `Free AI`, `Base AI`, `Pro AI`, `Base Data`, `Pro Data` and `Custom plans`.  
summary| false| bool| |  This parameter enables summary key generation in web search results. This is required for summarizer to be enabled.  
  

You can also optimise your search query by using [search
operators](https://search.brave.com/help/operators).

#### # Local Search API

This table lists the query parameters supported by the Local Search API. Some
are required, but most are optional.

Parameter| Required| Type| Default| Description  
---|---|---|---|---  
ids| true| list[string]| |  Unique identifier for the location. Ids can not be empty. Maximum of 20 ids per request. The parameter can be repeated to query for multiple ids.  
search_lang| false| string| en|  The search language preference. The 2 or more
character language code for which the search results are provided. For a list
of possible values, see [Language Codes](/app/documentation/web-
search/codes#language-codes).  
ui_lang| false| string| en-US|  User interface language preferred in response.
Usually of the format ‘<language_code>-<country_code>’. For more, see [RFC
9110](https://www.rfc-editor.org/rfc/rfc9110.html#name-accept-language). For a
list of supported values, see [UI Language Codes](/app/documentation/web-
search/codes#market-codes).  
units| false| string| |  The measurement units. If not provided, units are derived from search country. Possible values are: \- `metric`: The standardized measurement system \- `imperial`: The British Imperial system of units.

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard) [ Documentation](/app/documentation)

[ Web Search](/app/documentation/web-search)

[ Get Started](/app/documentation/web-search/get-started)[ Query
Parameters](/app/documentation/web-search/query)[ Request
Headers](/app/documentation/web-search/request-headers)[ Response
Headers](/app/documentation/web-search/response-headers)[ Response
Objects](/app/documentation/web-search/responses)[
Codes](/app/documentation/web-search/codes)[ API
Changelog](/app/documentation/web-search/api-changelog)

[ Summarizer Search](/app/documentation/summarizer-search) [ Image
Search](/app/documentation/image-search) [ Video
Search](/app/documentation/video-search) [ News
Search](/app/documentation/news-search) [ Suggest](/app/documentation/suggest)
[ Spellcheck](/app/documentation/spellcheck) [
General](/app/documentation/general)

[Login](/login) [Register](/register)

###### Brave Web Search API

## Request Headers

#### # Web Search API Request Headers

This table lists the request headers supported by the Web Search API. Most are
optional, but note that sending more information in headers (such as client
location) will improve search results.

Header| Required| Name| Description  
---|---|---|---  
Accept| false| Accept|  The default supported media type is `application/json`  
Accept-Encoding| false| Accept Encoding|  The supported compression type is
`gzip`.  
Api-Version| false| Web Search API Version|  The Brave Web Search API version
to use. This is denoted by the format `YYYY-MM-DD`. The latest version is used
by default, and the previous ones can be found in the [API Changelog](./api-
changelog).  
Cache-Control| false| Cache Control|  Search will return cached web search
results by default. To prevent caching set the Cache-Control header to `no-
cache`. This is currently done as best effort.  
User-Agent| false| User Agent|  The user agent of the client sending the
request. Search can utilize the user agent to provide a different experience
depending on the client sending the request. The user agent should follow the
commonly used browser agent strings on each platform. For more information on
curating user agents, see [RFC 9110](https://www.rfc-
editor.org/rfc/rfc9110.html#name-user-agent). User agent string examples by
platform:

  * **Android** : Mozilla/5.0 (Linux; Android 13; Pixel 7 Pro) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Mobile Safari/537.36
  * **iOS** : Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Mobile/15E148 Safari/604.1
  * **macOS** : Mozilla/5.0 (Macintosh; Intel Mac OS X 12_0_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/
  * **Windows** : Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/

  
X-Loc-Lat| false| Latitude|  The latitude of the client’s geographical
location in degrees, to provide relevant local results. The latitiude must be
greater than or equal to -90.0 degrees and less than or equal to +90.0
degrees.  
X-Loc-Long| false| Longitude|  The longitude of the client’s geographical
location in degrees, to provide relevant local results. The longitude must be
greater than or equal to -180.0 degrees and less than or equal to +180.0
degrees.  
X-Loc-Timezone| false| Timezone|  The IANA timezone for the client’s device,
for example `America/New_York`. For complete list of IANA timezones and
location mappings see [IANA Database](https://www.iana.org/time-zones) and
[Geonames Database](https://download.geonames.org/export/dump/).  
X-Loc-City| false| City Name|  The generic name of the client city.  
X-Loc-State| false| State Code|  The code representing the client’s
state/region, can be up to 3 characters long. The region is the first-level
subdivision (the broadest or least specific) of the [ISO
3166-2](https://en.wikipedia.org/wiki/ISO_3166-2) code.  
X-Loc-State-Name| false| State Name|  The name of the client’s state/region.
The region is the first-level subdivision (the broadest or least specific) of
the [ISO 3166-2](https://en.wikipedia.org/wiki/ISO_3166-2) code.  
X-Loc-Country| false| Country Code|  The two letter code for the client’s
country. For a list of country codes, see [ISO 3166-1
alpha-2](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2)  
X-Loc-Postal-Code| false| Postal Code|  The postal code of the client’s
location.  
X-Subscription-Token| true| Authentication token|  The secret token for the
subscribed plan to authenticate the request. Can be obtained from [API
Keys](/app/keys).  
  
#### # Local Search API Request Headers

This table lists the request headers supported by the Local Search API, most
of which are optional.

Header| Required| Name| Description  
---|---|---|---  
Accept| false| Accept|  The default supported media type is `application/json`  
Accept-Encoding| false| Accept Encoding|  The supported compression type is
`gzip`.  
Api-Version| false| Web Search API Version|  The Brave Web Search API version
to use. This is denoted by the format `YYYY-MM-DD`. The latest version is used
by default, and the previous ones can be found in the [API Changelog](./api-
changelog).  
Cache-Control| false| Cache Control|  Search will return cached web search
results by default. To prevent caching set the Cache-Control header to `no-
cache`. This is currently done as best effort.  
User-Agent| false| User Agent|  The user agent of the client sending the
request. Search can utilize the user agent to provide a different experience
depending on the client sending the request. The user agent should follow the
commonly used browser agent strings on each platform. For more information on
curating user agents, see [RFC 9110](https://www.rfc-
editor.org/rfc/rfc9110.html#name-user-agent). User agent string examples by
platform:

  * **Android** : Mozilla/5.0 (Linux; Android 13; Pixel 7 Pro) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Mobile Safari/537.36
  * **iOS** : Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Mobile/15E148 Safari/604.1
  * **macOS** : Mozilla/5.0 (Macintosh; Intel Mac OS X 12_0_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/
  * **Windows** : Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/

  
X-Subscription-Token| true| Authentication token|  The secret token for the
subscribed plan to authenticate the request. Can be obtained from [API
Keys](/app/keys).

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard) [ Documentation](/app/documentation)

[ Web Search](/app/documentation/web-search)

[ Get Started](/app/documentation/web-search/get-started)[ Query
Parameters](/app/documentation/web-search/query)[ Request
Headers](/app/documentation/web-search/request-headers)[ Response
Headers](/app/documentation/web-search/response-headers)[ Response
Objects](/app/documentation/web-search/responses)[
Codes](/app/documentation/web-search/codes)[ API
Changelog](/app/documentation/web-search/api-changelog)

[ Summarizer Search](/app/documentation/summarizer-search) [ Image
Search](/app/documentation/image-search) [ Video
Search](/app/documentation/video-search) [ News
Search](/app/documentation/news-search) [ Suggest](/app/documentation/suggest)
[ Spellcheck](/app/documentation/spellcheck) [
General](/app/documentation/general)

[Login](/login) [Register](/register)

###### Brave Web Search API

## Response Headers

#### Global

This table lists the response headers supported by the Web Search API.

Header| Name| Description  
---|---|---  
X-RateLimit-Limit| Rate Limit|  Rate limits associated with the requested
plan. An example rate limit `X-RateLimit-Limit: 1, 15000` means 1 request per
second and 15000 requests per month.  
X-RateLimit-Policy| Rate Limit Policy|  Rate limit policies currently
associated with the requested plan. An example policy `X-RateLimit-Policy:
1;w=1, 15000;w=2592000` means a limit of 1 request over a 1 second window and
15000 requests over a month window. The windows are always given in seconds.  
X-RateLimit-Remaining| Rate Limit Remaining|  Remaining quota units associated
with the expiring limits. An example remaining limit `X-RateLimit-Remaining:
1, 1000` indicates the API is able to be accessed once during the current
second, and 1000 times over the current month. **Note** : Only successful
requests are counted and billed.  
X-RateLimit-Reset| Rate Limit Reset|  The number of seconds until the quota
associated with the expiring limits resets. An example reset limit
`X-RateLimit-Reset: 1, 1419704` means a single request can be done again in a
second and in 1419704 seconds the full monthly quota associated with the plan
will be available again.

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard)

[![Brave](https://cdn.search.brave.com/search-
api/web/v1/client/_app/immutable/assets/brave-
logo.BytqdRrN.svg)](/app/dashboard) [ Documentation](/app/documentation)

[ Web Search](/app/documentation/web-search)

[ Get Started](/app/documentation/web-search/get-started)[ Query
Parameters](/app/documentation/web-search/query)[ Request
Headers](/app/documentation/web-search/request-headers)[ Response
Headers](/app/documentation/web-search/response-headers)[ Response
Objects](/app/documentation/web-search/responses)[
Codes](/app/documentation/web-search/codes)[ API
Changelog](/app/documentation/web-search/api-changelog)

[ Summarizer Search](/app/documentation/summarizer-search) [ Image
Search](/app/documentation/image-search) [ Video
Search](/app/documentation/video-search) [ News
Search](/app/documentation/news-search) [ Suggest](/app/documentation/suggest)
[ Spellcheck](/app/documentation/spellcheck) [
General](/app/documentation/general)

[Login](/login) [Register](/register)

###### Brave Web Search API

## Response Objects

#### # WebSearchApiResponse

Top level response model for successful Web Search API requests. The response
will include the relevant keys based on the plan subscribed, query relevance
or applied result_filter as a query parameter. The API can also respond back
with an error response based on invalid subscription keys and rate limit
events.

Field| Type| Required| Description  
---|---|---|---  
type| "search"| true|  The type of web search API result. The value is always
search.  
discussions| Discussions| false|  Discussions clusters aggregated from forum
posts that are relevant to the query.  
faq| FAQ| false|  Frequently asked questions that are relevant to the search
query.  
infobox| GraphInfobox| false|  Aggregated information on an entity showable as
an infobox.  
locations| Locations| false|  Places of interest (POIs) relevant to location
sensitive queries.  
mixed| MixedResponse| false|  Preferred ranked order of search results.  
news| News| false|  News results relevant to the query.  
query| Query| false|  Search query string and its modifications that are used
for search.  
videos| Videos| false|  Videos relevant to the query.  
web| Search| false|  Web search results relevant to the query.  
summarizer| Summarizer| false|  Summary key to get summary results for the
query.  
  
#### # LocalPoiSearchApiResponse

Top level response model for successful Local Search API request to get extra
information for locations. The response will include a list of location
results corresponding to the ids in the request. The API can also respond back
with an error response in cases like too many ids being requested, invalid
subscription keys, and rate limit events. Access to Local Search API requires
a subscription to a Pro plan.

Field| Type| Required| Description  
---|---|---|---  
type| "local_pois"| true|  The type of local POI search API result. The value
is always local_pois.  
results| list [ LocationResult ]| false|  Location results matching the ids in
the request.  
  
#### # LocalDescriptionsSearchApiResponse

Top level response model for successful Local Search API request to get AI
generated description for locations. The response includes a list of generated
descriptions corresponding to the ids in the request. The API can also respond
back with an error response in cases like too many ids being requested,
invalid subscription keys, and rate limit events. Access to Local Search API
requires a subscription to a Pro plan.

Field| Type| Required| Description  
---|---|---|---  
type| "local_descriptions"| true|  The type of local description search API
result. The value is always local_descriptions.  
results| list [ LocationDescription ]| false|  Location descriptions matching
the ids in the request.  
  
#### # Query

A model representing information gathered around the requested query.

Field| Type| Required| Description  
---|---|---|---  
original| string| true|  The original query that was requested.  
show_strict_warning| bool| false|  Whether there is more content available for
query, but the response was restricted due to safesearch.  
altered| string| false|  The altered query for which the search was performed.  
safesearch| bool| false|  Whether safesearch was enabled.  
is_navigational| bool| false|  Whether the query is a navigational query to a
domain.  
is_geolocal| bool| false|  Whether the query has location relevance.  
local_decision| string| false|  Whether the query was decided to be location
sensitive.  
local_locations_idx| int| false|  The index of the location.  
is_trending| bool| false|  Whether the query is trending.  
is_news_breaking| bool| false|  Whether the query has news breaking articles
relevant to it.  
ask_for_location| bool| false|  Whether the query requires location
information for better results.  
language| Language| false|  The language information gathered from the query.  
spellcheck_off| bool| false|  Whether the spellchecker was off.  
country| string| false|  The country that was used.  
bad_results| bool| false|  Whether there are bad results for the query.  
should_fallback| bool| false|  Whether the query should use a fallback.  
lat| string| false|  The gathered location latitutde associated with the
query.  
long| string| false|  The gathered location longitude associated with the
query.  
postal_code| string| false|  The gathered postal code associated with the
query.  
city| string| false|  The gathered city associated with the query.  
state| string| false|  The gathered state associated with the query.  
header_country| string| false|  The country for the request origination.  
more_results_available| bool| false|  Whether more results are available for
the given query.  
custom_location_label| string| false|  Any custom location labels attached to
the query.  
reddit_cluster| string| false|  Any reddit cluster associated with the query.  
  
#### # Discussions

A model representing a discussion cluster relevant to the query.

Field| Type| Required| Description  
---|---|---|---  
type| "search"| true|  The type identifying a discussion cluster. Currently
the value is always search.  
results| list [ DiscussionResult ]| true|  A list of discussion results.  
mutated_by_goggles| bool| true|  Whether the discussion results are changed by
a Goggle. False by default.  
  
#### # DiscussionResult (SearchResult)

A discussion result. These are forum posts and discussions that are relevant
to the search query.

Field| Type| Required| Description  
---|---|---|---  
type| "discussion"| true|  The discussion result type identifier. The value is
always discussion.  
data| ForumData| false|  The enriched aggregated data for the relevant forum
post.  
  
#### # ForumData

Defines a result from a discussion forum.

Field| Type| Required| Description  
---|---|---|---  
forum_name| string| true|  The name of the forum.  
num_answers| int| false|  The number of answers to the post.  
score| string| false|  The score of the post on the forum.  
title| string| false|  The title of the post on the forum.  
question| string| false|  The question asked in the forum post.  
top_comment| string| false|  The top-rated comment under the forum post.  
  
#### # FAQ

Frequently asked questions relevant to the search query term.

Field| Type| Required| Description  
---|---|---|---  
type| "faq"| true|  The FAQ result type identifier. The value is always faq.  
results| list [ QA ]| true|  A list of aggregated question answer results
relevant to the query.  
  
#### # QA

A question answer result.

Field| Type| Required| Description  
---|---|---|---  
question| string| true|  The question being asked.  
answer| string| true|  The answer to the question.  
title| string| true|  The title of the post.  
url| string| true|  The url pointing to the post.  
meta_url| MetaUrl| false|  Aggregated information about the url.  
  
#### # MetaUrl

Aggregated information about a url.

Field| Type| Required| Description  
---|---|---|---  
scheme| string| true|  The protocol scheme extracted from the url.  
netloc| string| true|  The network location part extracted from the url.  
hostname| string| false|  The lowercased domain name extracted from the url.  
favicon| string| true|  The favicon used for the url.  
path| string| true|  The hierarchical path of the url useful as a display
string.  
  
#### # Search

A model representing a collection of web search results.

Field| Type| Required| Description  
---|---|---|---  
type| "search"| true|  A type identifying web search results. The value is
always search.  
results| list [ SearchResult ]| true|  A list of search results.  
family_friendly| bool| true|  Whether the results are family friendly.  
  
#### # SearchResult (Result)

Aggregated information on a web search result, relevant to the query.

Field| Type| Required| Description  
---|---|---|---  
type| "search_result"| true|  A type identifying a web search result. The
value is always search_result.  
subtype| "generic"| true|  A sub type identifying the web search result type.  
is_live| bool| true|  Whether the web search result is currently live. Default
value is False.  
deep_results| DeepResult| false|  Gathered information on a web search result.  
schemas| list [ list ]| false|  A list of schemas (structured data) extracted
from the page. The schemas try to follow
[schema.org](https://schema.org/docs/schemas.html) and will return anything we
can extract from the HTML that can fit into these models.  
meta_url| MetaUrl| false|  Aggregated information on the url associated with
the web search result.  
thumbnail| Thumbnail| false|  The thumbnail of the web search result.  
age| string| false|  A string representing the age of the web search result.  
language| string| true|  The main language on the web search result.  
location| LocationResult| false|  The location details if the query relates to
a restaurant.  
video| VideoData| false|  The video associated with the web search result.  
movie| MovieData| false|  The movie associated with the web search result.  
faq| FAQ| false|  Any frequently asked questions associated with the web
search result.  
qa| QAPage| false|  Any question answer information associated with the web
search result page.  
book| Book| false|  Any book information associated with the web search result
page.  
rating| Rating| false|  Rating found for the web search result page.  
article| Article| false|  An article found for the web search result page.  
product| ProductReview| false|  The main product and a review that is found on
the web search result page.  
product_cluster| list [ ProductReview ]| false|  A list of products and
reviews that are found on the web search result page.  
cluster_type| string| false|  A type representing a cluster. The value can be
product_cluster.  
cluster| list [ Result ]| false|  A list of web search results.  
creative_work| CreativeWork| false|  Aggregated information on the creative
work found on the web search result.  
music_recording| MusicRecording| false|  Aggregated information on music
recording found on the web search result.  
review| Review| false|  Aggregated information on the review found on the web
search result.  
software| Software| false|  Aggregated information on a software product found
on the web search result page.  
recipe| Recipe| false|  Aggregated information on a recipe found on the web
search result page.  
organization| Organization| false|  Aggregated information on a organization
found on the web search result page.  
content_type| string| false|  The content type associated with the search
result page.  
extra_snippets| list [ string ]| false|  A list of extra alternate snippets
for the web search result.  
  
#### # Result

A model representing a web search result.

Field| Type| Required| Description  
---|---|---|---  
title| string| true|  The title of the web page.  
url| string| true|  The url where the page is served.  
is_source_local| bool| true|  
is_source_both| bool| true|  
description| string| false|  A description for the web page.  
page_age| string| false|  A date representing the age of the web page.  
page_fetched| string| false|  A date representing when the web page was last
fetched.  
profile| Profile| false|  A profile associated with the web page.  
language| string| false|  A language classification for the web page.  
family_friendly| bool| true|  Whether the web page is family friendly.  
  
#### # AbstractGraphInfobox (Result)

Shared aggregated information on an entity from a knowledge graph.

Field| Type| Required| Description  
---|---|---|---  
type| "infobox"| true|  The infobox result type identifier. The value is
always infobox.  
position| int| true|  The position on a search result page.  
label| string| false|  Any label associated with the entity.  
category| string| false|  Category classification for the entity.  
long_desc| string| false|  A longer description for the entity.  
thumbnail| Thumbnail| false|  The thumbnail associated with the entity.  
attributes| list [ list [ string ] ]| false|  A list of attributes about the
entity.  
profiles| list [ Profile ] | list [ DataProvider ]| false|  The profiles associated with the entity.  
website_url| string| false|  The official website pertaining to the entity.  
ratings| list [ Rating ]| false|  Any ratings given to the entity.  
providers| list [ DataProvider ]| false|  A list of data sources for the
entity.  
distance| Unit| false|  A unit representing quantity relevant to the entity.  
images| list [ Thumbnail ]| false|  A list of images relevant to the entity.  
movie| MovieData| false|  Any movie data relevant to the entity. Appears only
when the result is a movie.  
  
#### # GenericInfobox (AbstractGraphInfobox)

Aggregated information on a generic entity from a knowledge graph.

Field| Type| Required| Description  
---|---|---|---  
subtype| "generic"| true|  The infobox subtype identifier. The value is always
generic.  
found_in_urls| list [ string ]| false|  List of urls where the entity was
found.  
  
#### # EntityInfobox (AbstractGraphInfobox)

Aggregated information on an entity from a knowledge graph.

Field| Type| Required| Description  
---|---|---|---  
subtype| "entity"| true|  The infobox subtype identifier. The value is always
entity.  
  
#### # QAInfobox (AbstractGraphInfobox)

A question answer infobox.

Field| Type| Required| Description  
---|---|---|---  
subtype| "code"| true|  The infobox subtype identifier. The value is always
code.  
data| QAPage| true|  The question and relevant answer.  
meta_url| MetaUrl| false|  Detailed information on the page containing the
question and relevant answer.  
  
#### # InfoboxWithLocation (AbstractGraphInfobox)

An infobox with location.

Field| Type| Required| Description  
---|---|---|---  
subtype| "location"| true|  The infobox subtype identifier. The value is
always location.  
is_location| bool| true|  Whether the entity a location.  
coordinates| list [ float ]| false|  The coordinates of the location.  
zoom_level| int| true|  The map zoom level.  
location| LocationResult| false|  The location result.  
  
#### # InfoboxPlace (AbstractGraphInfobox)

An infobox for a place, such as a business.

Field| Type| Required| Description  
---|---|---|---  
subtype| "place"| true|  The infobox subtype identifier. The value is always
place.  
location| LocationResult| true|  The location result.  
  
#### # GraphInfobox

Aggregated information on an entity shown as an infobox.

Field| Type| Required| Description  
---|---|---|---  
type| "graph"| true|  The type identifier for infoboxes. The value is always
graph.  
results| GenericInfoboxQAInfoboxInfoboxPlaceInfoboxWithLocationEntityInfobox|
true|  A list of infoboxes associated with the query.  
  
#### # QAPage

Aggreated result from a question answer page.

Field| Type| Required| Description  
---|---|---|---  
question| string| true|  The question that is being asked.  
answer| Answer| true|  An answer to the question.  
  
#### # Answer

A response representing an answer to a question on a forum.

Field| Type| Required| Description  
---|---|---|---  
text| string| true|  The main content of the answer.  
author| string| false|  The name of the author of the answer.  
upvoteCount| int| false|  Number of upvotes on the answer.  
downvoteCount| int| false|  The number of downvotes on the answer.  
  
#### # Thumbnail

Aggregated details representing a picture thumbnail.

Field| Type| Required| Description  
---|---|---|---  
src| string| true|  The served url of the picture thumbnail.  
original| string| false|  The original url of the image.  
  
#### # LocationWebResult (Result)

A model representing a web result related to a location.

Field| Type| Required| Description  
---|---|---|---  
meta_url| MetaUrl| true|  Aggregated information about the url.  
  
#### # LocationResult (Result)

A result that is location relevant.

Field| Type| Required| Description  
---|---|---|---  
type| "location_result"| true|  Location result type identifier. The value is
always location_result.  
id| string| false|  A Temporary id associated with this result, which can be
used to retrieve extra information about the location. It remains valid for 8
hours…  
provider_url| string| true|  The complete url of the provider.  
coordinates| list [ float ]| false|  A list of coordinates associated with the
location. This is a lat long represented as a floating point.  
zoom_level| int| true|  The zoom level on the map.  
thumbnail| Thumbnail| false|  The thumbnail associated with the location.  
postal_address| PostalAddress| false|  The postal address associated with the
location.  
opening_hours| OpeningHours| false|  The opening hours, if it is a business,
associated with the location .  
contact| Contact| false|  The contact of the business associated with the
location.  
price_range| string| false|  A display string used to show the price
classification for the business.  
rating| Rating| false|  The ratings of the business.  
distance| Unit| false|  The distance of the location from the client.  
profiles| list [ DataProvider ]| false|  Profiles associated with the
business.  
reviews| Reviews| false|  Aggregated reviews from various sources relevant to
the business.  
pictures| PictureResults| false|  A bunch of pictures associated with the
business.  
action| Action| false|  An action to be taken.  
serves_cuisine| list [ string ]| false|  A list of cuisine categories served.  
categories| list [ string ]| false|  A list of categories.  
icon_category| string| false|  An icon category.  
results| LocationWebResult| false|  Web results related to this location.  
timezone| string| false|  IANA timezone identifier.  
timezone_offset| string| false|  The utc offset of the timezone.  
  
#### # LocationDescription

AI generated description of a location result.

Field| Type| Required| Description  
---|---|---|---  
type| "local_description"| true|  The type of a location description. The
value is always local_description.  
id| string| true|  A Temporary id of the location with this description.  
description| string| false|  AI generated description of the location with the
given id.  
  
#### # Locations

A model representing location results.

Field| Type| Required| Description  
---|---|---|---  
type| "locations"| true|  Location type identifier. The value is always
locations.  
results| list [ LocationResult ]| true|  An aggregated list of location
sensitive results.  
  
#### # MixedResponse

The ranking order of results on a search result page.

Field| Type| Required| Description  
---|---|---|---  
type| "mixed"| true|  The type representing the model mixed. The value is
always mixed.  
main| list [ ResultReference ]| false|  The ranking order for the main section
of the search result page.  
top| list [ ResultReference ]| false|  The ranking order for the top section
of the search result page.  
side| list [ ResultReference ]| false|  The ranking order for the side section
of the search result page.  
  
#### # ResultReference

The ranking order of results on a search result page.

Field| Type| Required| Description  
---|---|---|---  
type| string| true|  The type of the result.  
index| int| false|  The 0th based index where the result should be placed.  
all| bool| true|  Whether to put all the results from the type at specific
position.  
  
#### # Videos

A model representing video results.

Field| Type| Required| Description  
---|---|---|---  
type| videos| true|  The type representing the videos. The value is always
videos.  
results| list [ VideoResult ]| true|  A list of video results.  
mutated_by_goggles| bool| false|  Whether the video results are changed by a
Goggle. False by default.  
  
#### # News

A model representing news results.

Field| Type| Required| Description  
---|---|---|---  
type| news| true|  The type representing the news. The value is always news.  
results| list [ NewsResult ]| true|  A list of news results.  
mutated_by_goggles| bool| false|  Whether the news results are changed by a
Goggle. False by default.  
  
#### # NewsResult (Result)

A model representing news results.

Field| Type| Required| Description  
---|---|---|---  
meta_url| MetaUrl| false|  The aggregated information on the url representing
a news result  
source| string| false|  The source of the news.  
breaking| bool| true|  Whether the news result is currently a breaking news.  
is_live| bool| true|  Whether the news result is currently live.  
thumbnail| Thumbnail| false|  The thumbnail associated with the news result.  
age| string| false|  A string representing the age of the news article.  
extra_snippets| list [ string ]| false|  A list of extra alternate snippets
for the news search result.  
  
#### # PictureResults

A model representing a list of pictures.

Field| Type| Required| Description  
---|---|---|---  
viewMoreUrl| string| false|  A url to view more pictures.  
results| list [ Thumbnail ]| true|  A list of thumbnail results.  
  
#### # Action

A model representing an action to be taken.

Field| Type| Required| Description  
---|---|---|---  
type| string| true|  The type representing the action.  
url| string| true|  A url representing the action to be taken.  
  
#### # PostalAddress

A model representing a postal address of a location

Field| Type| Required| Description  
---|---|---|---  
type| "PostalAddress"| true|  The type identifying a postal address. The value
is always PostalAddress.  
country| string| false|  The country associated with the location.  
postalCode| string| false|  The postal code associated with the location.  
streetAddress| string| false|  The street address associated with the
location.  
addressRegion| string| false|  The region associated with the location. This
is usually a state.  
addressLocality| string| false|  The address locality or subregion associated
with the location.  
displayAddress| string| true|  The displayed address string.  
  
#### # OpeningHours

Opening hours of a bussiness at a particular location.

Field| Type| Required| Description  
---|---|---|---  
current_day| list [ DayOpeningHours ]| false|  The current day opening hours.
Can have two sets of opening hours.  
days| list [ list [ DayOpeningHours ] ]| false|  The opening hours for the
whole week.  
  
#### # DayOpeningHours

A model representing the opening hours for a particular day for a business at
a particular location.

Field| Type| Required| Description  
---|---|---|---  
abbr_name| string| true|  A short string representing the day of the week.  
full_name| string| true|  A full string representing the day of the week.  
opens| string| true|  A 24 hr clock time string for the opening time of the
business on a particular day.  
closes| string| true|  A 24 hr clock time string for the closing time of the
business on a particular day.  
  
#### # Contact

A model representing contact information for an entity.

Field| Type| Required| Description  
---|---|---|---  
email| string| false|  The email address.  
telephone| string| false|  The telephone number.  
  
#### # DataProvider

A model representing the data provider associated with the entity.

Field| Type| Required| Description  
---|---|---|---  
type| "external"| true|  The type representing the source of data. This is
usually external.  
name| string| true|  The name of the data provider. This can be a domain.  
url| string| true|  The url where the information is coming from.  
long_name| string| false|  The long name for the data provider.  
img| string| false|  The served url for the image data.  
  
#### # Profile

A profile of an entity.

Field| Type| Required| Description  
---|---|---|---  
name| string| true|  The name of the profile.  
long_name| string| true|  The long name of the profile.  
url| string| false|  The original url where the profile is available.  
img| string| false|  The served image url representing the profile.  
  
#### # Unit

A model representing a unit of measurement.

Field| Type| Required| Description  
---|---|---|---  
value| float| true|  The quantity of the unit.  
units| string| true|  The name of the unit associated with the quantity.  
  
#### # MovieData

Aggregated data for a movie result.

Field| Type| Required| Description  
---|---|---|---  
name| string| false|  Name of the movie.  
description| string| false|  A short plot summary for the movie.  
url| string| false|  A url serving a movie profile page.  
thumbnail| Thumbnail| false|  A thumbnail for a movie poster.  
release| string| false|  The release date for the movie.  
directors| list [ Person ]| false|  A list of people responsible for directing
the movie.  
actors| list [ Person ]| false|  A list of actors in the movie.  
rating| Rating| false|  Rating provided to the movie from various sources.  
duration| string| false|  The runtime of the movie. The format is HH:MM:SS.  
genre| list [ string ]| false|  List of genres in which the movie can be
classified.  
query| string| false|  The query that resulted in the movie result.  
  
#### # Thing

A model describing a generic thing.

Field| Type| Required| Description  
---|---|---|---  
type| "thing"| true|  A type identifying a thing. The value is always thing.  
name| string| true|  The name of the thing.  
url| string| false|  A url for the thing.  
thumbnail| Thumbnail| false|  Thumbnail associated with the thing.  
  
#### # Person (Thing)

A model describing a person entity.

Field| Type| Required| Description  
---|---|---|---  
type| "person"| true|  A type identifying a person. The value is always
person.  
  
#### # Rating

The rating associated with an entity.

Field| Type| Required| Description  
---|---|---|---  
ratingValue| float| true|  The current value of the rating.  
bestRating| float| true|  Best rating received.  
reviewCount| int| false|  The number of reviews associated with the rating.  
profile| Profile| false|  The profile associated with the rating.  
is_tripadvisor| bool| true|  Whether the rating is coming from Tripadvisor.  
  
#### # Book

A model representing a book result.

Field| Type| Required| Description  
---|---|---|---  
title| string| true|  The title of the book.  
author| list [ Person ]| true|  The author of the book.  
date| string| false|  The publishing date of the book.  
price| Price| false|  The price of the book.  
pages| int| false|  The number of pages in the book.  
publisher| Person| false|  The publisher of the book.  
rating| Rating| false|  A gathered rating from different sources associated
with the book.  
  
#### # Price

A model representing the price for an entity.

Field| Type| Required| Description  
---|---|---|---  
price| string| true|  The price value in a given currency.  
price_currency| string| true|  The current of the price value.  
  
#### # Article

A model representing an article.

Field| Type| Required| Description  
---|---|---|---  
author| list [ Person ]| false|  The author of the article.  
date| string| false|  The date when the article was published.  
publisher| Organization| false|  The name of the publisher for the article.  
thumbnail| Thumbnail| false|  A thumbnail associated with the article.  
isAccessibleForFree| bool| false|  Whether the article is free to read or is
behind a paywall.  
  
#### # ContactPoint (Thing)

A way to contact an entity.

Field| Type| Required| Description  
---|---|---|---  
type| "contact_point"| true|  A type string identifying a contact point. The
value is always contact_point.  
telephone| string| false|  The telephone number of the entity.  
email| string| false|  The email address of the entity.  
  
#### # Organization (Thing)

An entity responsible for another entity.

Field| Type| Required| Description  
---|---|---|---  
type| "organization"| true|  A type string identifying an organization. The
value is always organization.  
contact_points| list [ ContactPoint ]| false|  A list of contact points for
the organization.  
  
#### # HowTo

Aggregated information on a how to.

Field| Type| Required| Description  
---|---|---|---  
text| string| true|  The how to text.  
name| string| false|  A name for the how to.  
url| string| false|  A url associated with the how to.  
image| list [ string ]| false|  A list of image urls associated with the how
to.  
  
#### # Recipe

Aggregated information on a recipe.

Field| Type| Required| Description  
---|---|---|---  
title| string| true|  The title of the recipe.  
description| string| true|  The description of the recipe.  
thumbnail| Thumbnail| true|  A thumbnail associated with the recipe.  
url| string| true|  The url of the web page where the recipe was found.  
domain| string| true|  The domain of the web page where the recipe was found.  
favicon| string| true|  The url for the favicon of the web page where the
recipe was found.  
time| string| false|  The total time required to cook the recipe.  
prep_time| string| false|  The preparation time for the recipe.  
cook_time| string| false|  The cooking time for the recipe.  
ingredients| string| false|  Ingredients required for the recipe.  
instructions| list [ HowTo ]| false|  List of instructions for the recipe.  
servings| int| false|  How many people the recipe serves.  
calories| int| false|  Calorie count for the recipe.  
rating| Rating| false|  Aggregated information on the ratings associated with
the recipe.  
recipeCategory| string| false|  The category of the recipe.  
recipeCuisine| string| false|  The cuisine classification for the recipe.  
video| VideoData| false|  Aggregated information on the cooking video
associated with the recipe.  
  
#### # Product

A model representing a product.

Field| Type| Required| Description  
---|---|---|---  
type| "Product"| true|  A string representing a product type. The value is
always product.  
name| string| true|  The name of the product.  
category| string| false|  The category of the product.  
price| string| true|  The price of the product.  
thumbnail| Thumbnail| true|  A thumbnail associated with the product.  
description| string| false|  The description of the product.  
offers| list [ Offer ]| false|  A list of offers available on the product.  
rating| Rating| false|  A rating associated with the product.  
  
#### # Offer

An offer associated with a product.

Field| Type| Required| Description  
---|---|---|---  
url| string| true|  The url where the offer can be found.  
priceCurrency| string| true|  The currency in which the offer is made.  
price| string| true|  The price of the product currently on offer.  
  
#### # Review

A model representing a review for an entity.

Field| Type| Required| Description  
---|---|---|---  
type| "review"| true|  A string representing review type. This is always
review.  
name| string| true|  The review title for the review.  
thumbnail| Thumbnail| true|  The thumbnail associated with the reviewer.  
description| string| true|  A description of the review (the text of the
review itself).  
rating| Rating| true|  The ratings associated with the review.  
  
#### # Reviews

The reviews associated with an entity.

Field| Type| Required| Description  
---|---|---|---  
results| list [ TripAdvisorReview ]| true|  A list of trip advisor reviews for
the entity.  
viewMoreUrl| string| true|  A url to a web page where more information on the
result can be seen.  
reviews_in_foreign_language| bool| true|  Any reviews available in a foreign
language.  
  
#### # TripAdvisorReview

A model representing a Tripadvisor review.

Field| Type| Required| Description  
---|---|---|---  
title| string| true|  The title of the review.  
description| string| true|  A description seen in the review.  
date| string| true|  The date when the review was published.  
rating| Rating| true|  A rating given by the reviewer.  
author| Person| true|  The author of the review.  
review_url| string| true|  A url link to the page where the review can be
found.  
language| string| true|  The language of the review.  
  
#### # CreativeWork

A creative work relevant to the query. An example can be enriched metadata for
an app.

Field| Type| Required| Description  
---|---|---|---  
name| string| true|  The name of the creative work.  
thumbnail| Thumbnail| true|  A thumbnail associated with the creative work.  
rating| Rating| false|  A rating that is given to the creative work.  
  
#### # MusicRecording

Result classified as a music label or a song.

Field| Type| Required| Description  
---|---|---|---  
name| string| true|  The name of the song or album.  
thumbnail| Thumbnail| false|  A thumbnail associated with the music.  
rating| Rating| false|  The rating of the music.  
  
#### # Software

A model representing a software entity.

Field| Type| Required| Description  
---|---|---|---  
name| string| false|  The name of the software product.  
author| string| false|  The author of software product.  
version| string| false|  The latest version of the software product.  
codeRepository| string| false|  The code repository where the software product
is currently available or maintained.  
homepage| string| false|  The home page of the software product.  
datePublisher| string| false|  The date when the software product was
published.  
is_npm| bool| false|  Whether the software product is available on npm.  
is_pypi| bool| false|  Whether the software product is available on pypi.  
stars| int| false|  The number of stars on the repository.  
forks| int| false|  The numbers of forks of the repository.  
ProgrammingLanguage| string| false|  The programming language spread on the
software product.  
  
#### # DeepResult

Aggregated deep results from news, social, videos and images.

Field| Type| Required| Description  
---|---|---|---  
news| list [ NewsResult ]| false|  A list of news results associated with the
result.  
buttons| list [ ButtonResult ]| false|  A list of buttoned results associated
with the result.  
videos| list [ VideoResult ]| false|  Videos associated with the result.  
images| list [ Image ]| false|  Images associated with the result.  
  
#### # VideoResult (Result)

A model representing a video result.

Field| Type| Required| Description  
---|---|---|---  
type| "video_result"| true|  The type identifying the video result. The value
is always video_result.  
video| VideoData| true|  Meta data for the video.  
meta_url| MetaUrl| false|  Aggregated information on the URL  
thumbnail| Thumbnail| false|  The thumbnail of the video.  
age| string| false|  A string representing the age of the video.  
  
#### # VideoData

A model representing metadata gathered for a video.

Field| Type| Required| Description  
---|---|---|---  
duration| string| false|  A time string representing the duration of the
video. The format can be HH:MM:SS or MM:SS.  
views| string| false|  The number of views of the video.  
creator| string| false|  The creator of the video.  
publisher| string| false|  The publisher of the video.  
thumbnail| Thumbnail| false|  A thumbnail associated with the video.  
tags| list [ string ]| false|  A list of tags associated with the video.  
author| Profile| false|  Author of the video.  
requires_subscription| bool| false|  Whether the video requires a subscription
to watch.  
  
#### # ButtonResult

A result which can be used as a button.

Field| Type| Required| Description  
---|---|---|---  
type| "button_result"| true|  A type identifying button result. The value is
always button_result.  
title| string| true|  The title of the result.  
url| string| true|  The url for the button result.  
  
#### # Image

A model describing an image

Field| Type| Required| Description  
---|---|---|---  
thumbnail| Thumbnail| true|  The thumbnail associated with the image.  
url| string| false|  The url of the image.  
properties| ImageProperties| false|  Metadata on the image.  
  
#### # Language

A model representing a language.

Field| Type| Required| Description  
---|---|---|---  
main| string| true|  The main language seen in the string.  
  
#### # ImageProperties

Metadata on an image.

Field| Type| Required| Description  
---|---|---|---  
url| string| true|  The original image URL.  
resized| string| true|  The url for a better quality resized image.  
placeholder| string| true|  The placeholder image url.  
height| int| false|  The image height.  
width| int| false|  The image width.  
format| string| false|  The image format.  
content_size| string| false|  The image size.  
  
#### # Summarizer

Details on getting the summary.

Field| Type| Required| Description  
---|---|---|---  
type| "summarizer"| true|  The value is always summarizer.  
key| string| true|  The key for the summarizer API.

================
File: resources/pplx/pplx_urls.txt
================
https://docs.perplexity.ai/guides/getting-started
https://docs.perplexity.ai/guides/model-cards
https://docs.perplexity.ai/guides/pricing
https://docs.perplexity.ai/guides/structured-outputs
https://docs.perplexity.ai/guides/prompt-guide
https://docs.perplexity.ai/guides/bots
https://docs.perplexity.ai/api-reference/chat-completions

================
File: resources/pplx/pplx.md
================
[Perplexity home page![light logo](https://mintlify.s3.us-
west-1.amazonaws.com/perplexity/logo/SonarByPerplexity.svg)![dark
logo](https://mintlify.s3.us-
west-1.amazonaws.com/perplexity/logo/Sonar_Wordmark_Light.svg)](/home.mdx)

Search docs

  * [Playground](https://labs.perplexity.ai/)
  * [Playground](https://labs.perplexity.ai/)

Search...

Navigation

Guides

Initial Setup

[Home](/home)[Guides](/guides/getting-started)[API Reference](/api-
reference/chat-completions)[Changelog](/changelog/changelog)[System
Status](/system-status/system-
status)[FAQ](/faq/faq)[Discussions](/discussions/discussions)

##### Guides

  * [Initial Setup](/guides/getting-started)
  * [Supported Models](/guides/model-cards)
  * [Pricing](/guides/pricing)
  * [Rate Limits and Usage Tiers](/guides/usage-tiers)
  * [Structured Outputs Guide](/guides/structured-outputs)
  * [Prompt Guide](/guides/prompt-guide)
  * [Perplexity Crawlers](/guides/bots)

Guides

# Initial Setup

Register and make a successful API request

##

​

Registration

  * Start by visiting the [API Settings page](https://www.perplexity.ai/pplx-api)

  * Register your credit card to get started

This step will not charge your credit card. It just stores payment information
for later API usage.

##

​

Generate an API key

  * Every API call needs a valid API key

The API key is a long-lived access token that can be used until it is manually
refreshed or deleted.

Send the API key as a bearer token in the Authorization header with each API
request.

When you run out of credits, your API keys will be blocked until you add to
your credit balance. You can avoid this by configuring “Automatic Top Up”,
which refreshes your balance whenever you drop below $2.

##

​

Make your API call

  * The API is conveniently OpenAI client-compatible for easy integration with existing applications.

cURL

    
    
    curl --location 'https://api.perplexity.ai/chat/completions' \
    --header 'accept: application/json' \
    --header 'content-type: application/json' \
    --header 'Authorization: Bearer {API_KEY}' \
    --data '{
      "model": "sonar-pro ",
      "messages": [
        {
          "role": "system",
          "content": "Be precise and concise."
        },
        {
          "role": "user",
          "content": "How many stars are there in our galaxy?"
        }
      ]
    }'
    

python

    
    
    from openai import OpenAI
    
    YOUR_API_KEY = "INSERT API KEY HERE"
    
    messages = [
        {
            "role": "system",
            "content": (
                "You are an artificial intelligence assistant and you need to "
                "engage in a helpful, detailed, polite conversation with a user."
            ),
        },
        {   
            "role": "user",
            "content": (
                "How many stars are in the universe?"
            ),
        },
    ]
    
    client = OpenAI(api_key=YOUR_API_KEY, base_url="https://api.perplexity.ai")
    
    # chat completion without streaming
    response = client.chat.completions.create(
        model="sonar-pro",
        messages=messages,
    )
    print(response)
    
    # chat completion with streaming
    response_stream = client.chat.completions.create(
        model="sonar-pro",
        messages=messages,
        stream=True,
    )
    for response in response_stream:
        print(response)
    

[Supported Models](/guides/model-cards)

[twitter](https://twitter.com/perplexity_ai)[linkedin](https://www.linkedin.com/company/perplexity-
ai/)[discord](https://discord.com/invite/perplexity-
ai)[website](https://labs.perplexity.ai/)

On this page

  * Registration
  * Generate an API key
  * Make your API call

[Perplexity home page![light logo](https://mintlify.s3.us-
west-1.amazonaws.com/perplexity/logo/SonarByPerplexity.svg)![dark
logo](https://mintlify.s3.us-
west-1.amazonaws.com/perplexity/logo/Sonar_Wordmark_Light.svg)](/home.mdx)

Search docs

  * [Playground](https://labs.perplexity.ai/)
  * [Playground](https://labs.perplexity.ai/)

Search...

Navigation

Guides

Supported Models

[Home](/home)[Guides](/guides/getting-started)[API Reference](/api-
reference/chat-completions)[Changelog](/changelog/changelog)[System
Status](/system-status/system-
status)[FAQ](/faq/faq)[Discussions](/discussions/discussions)

##### Guides

  * [Initial Setup](/guides/getting-started)
  * [Supported Models](/guides/model-cards)
  * [Pricing](/guides/pricing)
  * [Rate Limits and Usage Tiers](/guides/usage-tiers)
  * [Structured Outputs Guide](/guides/structured-outputs)
  * [Prompt Guide](/guides/prompt-guide)
  * [Perplexity Crawlers](/guides/bots)

Guides

# Supported Models

Model| Context Length| Model Type  
---|---|---  
`sonar-deep-research`| 128k| Chat Completion  
`sonar-reasoning-pro`| 128k| Chat Completion  
`sonar-reasoning`| 128k| Chat Completion  
`sonar-pro`| 200k| Chat Completion  
`sonar`| 128k| Chat Completion  
`r1-1776`| 128k| Chat Completion  
  
  1. `sonar-reasoning-pro` and `sonar-pro` have a max output token limit of 8k
  2. The reasoning models output CoTs in their responses as well
  3. `r1-1776` is an offline chat model that does not use our search subsystem

[Initial Setup](/guides/getting-started)[Pricing](/guides/pricing)

[twitter](https://twitter.com/perplexity_ai)[linkedin](https://www.linkedin.com/company/perplexity-
ai/)[discord](https://discord.com/invite/perplexity-
ai)[website](https://labs.perplexity.ai/)

[Perplexity home page![light logo](https://mintlify.s3.us-
west-1.amazonaws.com/perplexity/logo/SonarByPerplexity.svg)![dark
logo](https://mintlify.s3.us-
west-1.amazonaws.com/perplexity/logo/Sonar_Wordmark_Light.svg)](/home.mdx)

Search docs

  * [Playground](https://labs.perplexity.ai/)
  * [Playground](https://labs.perplexity.ai/)

Search...

Navigation

Guides

Pricing

[Home](/home)[Guides](/guides/getting-started)[API Reference](/api-
reference/chat-completions)[Changelog](/changelog/changelog)[System
Status](/system-status/system-
status)[FAQ](/faq/faq)[Discussions](/discussions/discussions)

##### Guides

  * [Initial Setup](/guides/getting-started)
  * [Supported Models](/guides/model-cards)
  * [Pricing](/guides/pricing)
  * [Rate Limits and Usage Tiers](/guides/usage-tiers)
  * [Structured Outputs Guide](/guides/structured-outputs)
  * [Prompt Guide](/guides/prompt-guide)
  * [Perplexity Crawlers](/guides/bots)

Guides

# Pricing

Model| Input Tokens (Per Million Tokens)| Reasoning Tokens (Per Million
Tokens)| Output Tokens (Per Million Tokens)| Price per 1000 searches  
---|---|---|---|---  
`sonar-deep-research`| $2| $3| $8| $5  
`sonar-reasoning-pro`| $2| -| $8| $5  
`sonar-reasoning`| $1| -| $5| $5  
`sonar-pro`| $3| -| $15| $5  
`sonar`| $1| -| $1| $5  
`r1-1776`| $2| -| $8| -  
  
`r1-1776` is an offline chat model that does not use our search subsystem

##

​

Pricing Breakdown

Detailed Pricing Breakdown for Sonar Deep Research

**Input Tokens**

  1. Input tokens are priced at $2/1M tokens

  2. Input tokens comprise of Prompt tokens (user prompt) + Citation tokens (these are processed tokens from running searches)

**Search Queries**

  1. Deep Research runs multiple searches to conduct exhaustive research

  2. Searches are priced at $5/1000 searches

  3. A request that does 30 searches will cost $0.15 in this step.

**Reasoning Tokens**

  1. Reasoning is a distinct step in Deep Research since it does extensive automated reasoning through all the material it gathers during its research phase

  2. Reasoning tokens here are a bit different than the CoTs in the answer - these are tokens that we use to reason through the research material prior to generating the outputs via the CoTs.

  3. Reasoning tokens are priced at $3/1M tokens

**Output Tokens**

  1. Output tokens (Completion tokens) are priced at $8/1M tokens

**Total Price**

Your total price per request finally is a sum of the above 4 components

Detailed Pricing Breakdown for Sonar Reasoning Pro and Sonar Pro

**Input Tokens**

  1. Input tokens are priced at $2/1M tokens and $3/1M tokens respectively

  2. Input tokens comprise of Prompt tokens (user prompt) + Citation tokens (these are processed tokens from running searches)

**Search Queries**

  1. To give detailed answers, both the Pro APIs also run multiple searches on top of the user prompt where necessary for more exhaustive information retrieval

  2. Searches are priced at $5/1000 searches

  3. A request that does 3 searches will cost $0.015 in this step

**Output Tokens**

  1. Output tokens (Completion tokens) are priced at $8/1M tokens and $15/1M tokens respectively

**Total Price**

Your total price per request finally is a sum of the above 3 components

Detailed Pricing Breakdown for Sonar Reasoning and Sonar

**Input Tokens**

  1. Input tokens are priced at $1/1M tokens for both

  2. Input tokens comprise of Prompt tokens (user prompt)

**Search Queries**

  1. Each request does 1 search priced at $5/1000 searches

**Output Tokens**

  1. Output tokens (Completion tokens) are priced at $5/1M tokens and $1/1M tokens respectively

**Total Price**

Your total price per request finally is a sum of the above 2 components

[Supported Models](/guides/model-cards)[Rate Limits and Usage
Tiers](/guides/usage-tiers)

[twitter](https://twitter.com/perplexity_ai)[linkedin](https://www.linkedin.com/company/perplexity-
ai/)[discord](https://discord.com/invite/perplexity-
ai)[website](https://labs.perplexity.ai/)

[Perplexity home page![light logo](https://mintlify.s3.us-
west-1.amazonaws.com/perplexity/logo/SonarByPerplexity.svg)![dark
logo](https://mintlify.s3.us-
west-1.amazonaws.com/perplexity/logo/Sonar_Wordmark_Light.svg)](/home.mdx)

Search docs

  * [Playground](https://labs.perplexity.ai/)
  * [Playground](https://labs.perplexity.ai/)

Search...

Navigation

Guides

Structured Outputs Guide

[Home](/home)[Guides](/guides/getting-started)[API Reference](/api-
reference/chat-completions)[Changelog](/changelog/changelog)[System
Status](/system-status/system-
status)[FAQ](/faq/faq)[Discussions](/discussions/discussions)

##### Guides

  * [Initial Setup](/guides/getting-started)
  * [Supported Models](/guides/model-cards)
  * [Pricing](/guides/pricing)
  * [Rate Limits and Usage Tiers](/guides/usage-tiers)
  * [Structured Outputs Guide](/guides/structured-outputs)
  * [Prompt Guide](/guides/prompt-guide)
  * [Perplexity Crawlers](/guides/bots)

Guides

# Structured Outputs Guide

Structured outputs is currently a beta feature and only available to users in
Tier-3

##

​

Overview

We currently support two types of structured outputs: **JSON Schema** and
**Regex**. LLM responses will work to match the specified format, except for
the following cases:

  * The output exceeds `max_tokens`

Enabling the structured outputs can be done by adding a `response_format`
field in the request:

**JSON Schema**

  * `response_format: { type: "json_schema", json_schema: {"schema": object} }` .

  * The schema should be a valid JSON schema object.

**Regex** (only avilable for `sonar` right now)

  * `response_format: { type: "regex", regex: {"regex": str} }` .

  * The regex is a regular expression string.

We recommend to give the LLM some hints about the output format in the
prompts.

The first request with a new JSON Schema or Regex expects to incur delay on
the first token. Typically, it takes 10 to 30 seconds to prepare the new
schema. Once the schema has been prepared, the subsequent requests will not
see such delay.

##

​

Examples

###

​

1\. Get a response in JSON format

**Request**

python

    
    
    import requests
    from pydantic import BaseModel
    
    class AnswerFormat(BaseModel):
        first_name: str
        last_name: str
        year_of_birth: int
        num_seasons_in_nba: int
    
    url = "https://api.perplexity.ai/chat/completions"
    headers = {"Authorization": "Bearer YOUR_API_KEY"}
    payload = {
        "model": "sonar",
        "messages": [
            {"role": "system", "content": "Be precise and concise."},
            {"role": "user", "content": (
                "Tell me about Michael Jordan. "
                "Please output a JSON object containing the following fields: "
                "first_name, last_name, year_of_birth, num_seasons_in_nba. "
            )},
        ],
        "response_format": {
    		    "type": "json_schema",
            "json_schema": {"schema": AnswerFormat.model_json_schema()},
        },
    }
    response = requests.post(url, headers=headers, json=payload).json()
    print(response["choices"][0]["message"]["content"])
    

**Response**

    
    
    {"first_name":"Michael","last_name":"Jordan","year_of_birth":1963,"num_seasons_in_nba":15}
    

###

​

2\. Use a regex to output the format

**Request**

python

    
    
    import requests
    
    url = "https://api.perplexity.ai/chat/completions"
    headers = {"Authorization": "Bearer YOUR_API_KEY"}
    payload = {
        "model": "sonar",
        "messages": [
            {"role": "system", "content": "Be precise and concise."},
            {"role": "user", "content": "What is the IPv4 address of OpenDNS DNS server?"},
        ],
        "response_format": {
    		    "type": "regex",
            "regex": {"regex": r"(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)"},
        },
    }
    response = requests.post(url, headers=headers, json=payload).json()
    print(response["choices"][0]["message"]["content"])
    

**Response**

    
    
    208.67.222.222
    

##

​

Best Practices

###

​

Generating responses in a JSON Format

For Python users, we recommend using the Pydantic library to [generate JSON
schema](https://docs.pydantic.dev/latest/api/base_model/#pydantic.BaseModel.model_json_schema).

**Unsupported JSON Schemas**

Recursive JSON schema is not supported. As a result of that, unconstrained
objects are not supported either. Here’s a few example of unsupported schemas:

    
    
    # UNSUPPORTED!
    
    from typing import Any
    
    class UnconstrainedDict(BaseModel):
       unconstrained: dict[str, Any]
    
    class RecursiveJson(BaseModel):
       value: str
       child: list["RecursiveJson"]
    

###

​

Generating responses using a regex

**Supported Regex**

  * Characters: `\d`, `\w`, `\s` , `.`
  * Character classes: `[0-9A-Fa-f]` , `[^x]`
  * Quantifiers: `*`, `?` , `+`, `{3}`, `{2,4}` , `{3,}`
  * Alternation: `|`
  * Group: `( ... )`
  * Non-capturing group: `(?: ... )`
  * Positive lookahead: `(?= ... )`
  * Negative lookahead: `(?! ... )`

**Unsupported Regex**

  * Contents of group: `\1`
  * Anchors: `^`, `$`, `\b`
  * Positive look-behind: `(?<= ... )`
  * Negative look-behind: `(?<! ... )`
  * Recursion: `(?R)`

[Rate Limits and Usage Tiers](/guides/usage-tiers)[Prompt
Guide](/guides/prompt-guide)

[twitter](https://twitter.com/perplexity_ai)[linkedin](https://www.linkedin.com/company/perplexity-
ai/)[discord](https://discord.com/invite/perplexity-
ai)[website](https://labs.perplexity.ai/)

On this page

  * Overview
  * Examples
  * 1\. Get a response in JSON format
  * 2\. Use a regex to output the format
  * Best Practices
  * Generating responses in a JSON Format
  * Generating responses using a regex

[Perplexity home page![light logo](https://mintlify.s3.us-
west-1.amazonaws.com/perplexity/logo/SonarByPerplexity.svg)![dark
logo](https://mintlify.s3.us-
west-1.amazonaws.com/perplexity/logo/Sonar_Wordmark_Light.svg)](/home.mdx)

Search docs

  * [Playground](https://labs.perplexity.ai/)
  * [Playground](https://labs.perplexity.ai/)

Search...

Navigation

Guides

Prompt Guide

[Home](/home)[Guides](/guides/getting-started)[API Reference](/api-
reference/chat-completions)[Changelog](/changelog/changelog)[System
Status](/system-status/system-
status)[FAQ](/faq/faq)[Discussions](/discussions/discussions)

##### Guides

  * [Initial Setup](/guides/getting-started)
  * [Supported Models](/guides/model-cards)
  * [Pricing](/guides/pricing)
  * [Rate Limits and Usage Tiers](/guides/usage-tiers)
  * [Structured Outputs Guide](/guides/structured-outputs)
  * [Prompt Guide](/guides/prompt-guide)
  * [Perplexity Crawlers](/guides/bots)

Guides

# Prompt Guide

##

​

System Prompt

You can use the system prompt to provide instructions related to style, tone,
and language of the response.

The real-time search component of our models does not attend to the system
prompt.

**Example of a system prompt**

    
    
    You are a helpful AI assistant.
    
    Rules:
    1. Provide only the final answer. It is important that you do not include any explanation on the steps below.
    2. Do not show the intermediate steps information.
    
    Steps:
    1. Decide if the answer should be a brief sentence or a list of suggestions.
    2. If it is a list of suggestions, first, write a brief and natural introduction based on the original query.
    3. Followed by a list of suggestions, each suggestion should be split by two newlines.
    

##

​

User Prompt

You should use the user prompt to pass in the actual query for which you need
an answer for. The user prompt will be used to kick off a real-time web search
to make sure the answer has the latest and the most relevant information
needed.

**Example of a user prompt**

    
    
    What are the best sushi restaurants in the world currently?
    

[Structured Outputs Guide](/guides/structured-outputs)[Perplexity
Crawlers](/guides/bots)

[twitter](https://twitter.com/perplexity_ai)[linkedin](https://www.linkedin.com/company/perplexity-
ai/)[discord](https://discord.com/invite/perplexity-
ai)[website](https://labs.perplexity.ai/)

[Perplexity home page![light logo](https://mintlify.s3.us-
west-1.amazonaws.com/perplexity/logo/SonarByPerplexity.svg)![dark
logo](https://mintlify.s3.us-
west-1.amazonaws.com/perplexity/logo/Sonar_Wordmark_Light.svg)](/home.mdx)

Search docs

  * [Playground](https://labs.perplexity.ai/)
  * [Playground](https://labs.perplexity.ai/)

Search...

Navigation

Guides

Perplexity Crawlers

[Home](/home)[Guides](/guides/getting-started)[API Reference](/api-
reference/chat-completions)[Changelog](/changelog/changelog)[System
Status](/system-status/system-
status)[FAQ](/faq/faq)[Discussions](/discussions/discussions)

##### Guides

  * [Initial Setup](/guides/getting-started)
  * [Supported Models](/guides/model-cards)
  * [Pricing](/guides/pricing)
  * [Rate Limits and Usage Tiers](/guides/usage-tiers)
  * [Structured Outputs Guide](/guides/structured-outputs)
  * [Prompt Guide](/guides/prompt-guide)
  * [Perplexity Crawlers](/guides/bots)

Guides

# Perplexity Crawlers

We strive to improve our service every day by delivering the best search
experience possible. To achieve this, we collect data using web crawlers
(“robots”) and user agents that gather and index information from the
internet, operating either automatically or in response to user requests.
Webmasters can use the following robots.txt tags to manage how their sites and
content interact with Perplexity. Each setting works independently, and it may
take up to 24 hours for our systems to reflect changes.

User Agent| Description  
---|---  
PerplexityBot| `PerplexityBot` is designed to surface and link websites in
search results on Perplexity. It is not used to crawl content for AI
foundation models. To ensure your site appears in search results, we recommend
allowing `PerplexityBot` in your site’s `robots.txt` file and permitting
requests from our published IP ranges listed below.  
  
Full user-agent string: `Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko;
compatible; PerplexityBot/1.0; +https://perplexity.ai/perplexitybot)`  
  
Published IP addresses: <https://www.perplexity.com/perplexitybot.json>  
Perplexity‑User| `Perplexity-User` supports user actions within Perplexity.
When users ask Perplexity a question, it might visit a web page to help
provide an accurate answer and include a link to the page in its response.
`Perplexity-User` controls which sites these user requests can access. It is
not used for web crawling or to collect content for training AI foundation
models.  
  
Full user-agent string: `Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko;
compatible; Perplexity-User/1.0; +https://perplexity.ai/perplexity-user)`  
  
Published IP addresses: <https://www.perplexity.com/perplexity-user.json>  
  
Since a user requested the fetch, this fetcher generally ignores robots.txt
rules.  
  
[Prompt Guide](/guides/prompt-guide)

[twitter](https://twitter.com/perplexity_ai)[linkedin](https://www.linkedin.com/company/perplexity-
ai/)[discord](https://discord.com/invite/perplexity-
ai)[website](https://labs.perplexity.ai/)

[Perplexity home page![light logo](https://mintlify.s3.us-
west-1.amazonaws.com/perplexity/logo/SonarByPerplexity.svg)![dark
logo](https://mintlify.s3.us-
west-1.amazonaws.com/perplexity/logo/Sonar_Wordmark_Light.svg)](/home.mdx)

Search docs

  * [Playground](https://labs.perplexity.ai/)
  * [Playground](https://labs.perplexity.ai/)

Search...

Navigation

Perplexity API

Chat Completions

[Home](/home)[Guides](/guides/getting-started)[API Reference](/api-
reference/chat-completions)[Changelog](/changelog/changelog)[System
Status](/system-status/system-
status)[FAQ](/faq/faq)[Discussions](/discussions/discussions)

##### Perplexity API

  * [POSTChat Completions](/api-reference/chat-completions)

Perplexity API

# Chat Completions

Generates a model’s response for the given chat conversation.

POST

/

chat

/

completions

Try it

cURL

Python

JavaScript

PHP

Go

Java

    
    
    curl --request POST \
      --url https://api.perplexity.ai/chat/completions \
      --header 'Authorization: Bearer <token>' \
      --header 'Content-Type: application/json' \
      --data '{
      "model": "sonar",
      "messages": [
        {
          "role": "system",
          "content": "Be precise and concise."
        },
        {
          "role": "user",
          "content": "How many stars are there in our galaxy?"
        }
      ],
      "max_tokens": 123,
      "temperature": 0.2,
      "top_p": 0.9,
      "search_domain_filter": null,
      "return_images": false,
      "return_related_questions": false,
      "search_recency_filter": "<string>",
      "top_k": 0,
      "stream": false,
      "presence_penalty": 0,
      "frequency_penalty": 1,
      "response_format": null
    }'

200

422

    
    
    {
      "id": "3c90c3cc-0d44-4b50-8888-8dd25736052a",
      "model": "sonar",
      "object": "chat.completion",
      "created": 1724369245,
      "citations": [
        "https://www.astronomy.com/science/astro-for-kids-how-many-stars-are-there-in-space/",
        "https://www.esa.int/Science_Exploration/Space_Science/Herschel/How_many_stars_are_there_in_the_Universe",
        "https://www.space.com/25959-how-many-stars-are-in-the-milky-way.html",
        "https://www.space.com/26078-how-many-stars-are-there.html",
        "https://en.wikipedia.org/wiki/Milky_Way"
      ],
      "choices": [
        {
          "index": 0,
          "finish_reason": "stop",
          "message": {
            "role": "assistant",
            "content": "The number of stars in the Milky Way galaxy is estimated to be between 100 billion and 400 billion stars. The most recent estimates from the Gaia mission suggest that there are approximately 100 to 400 billion stars in the Milky Way, with significant uncertainties remaining due to the difficulty in detecting faint red dwarfs and brown dwarfs."
          },
          "delta": {
            "role": "assistant",
            "content": ""
          }
        }
      ],
      "usage": {
        "prompt_tokens": 14,
        "completion_tokens": 70,
        "total_tokens": 84
      }
    }

#### Authorizations

​

Authorization

string

header

required

Bearer authentication header of the form `Bearer <token>`, where `<token>` is
your auth token.

#### Body

application/json

​

model

string

required

The name of the model that will complete your prompt. Refer to [Supported
Models](https://docs.perplexity.ai/guides/model-cards) to find all the models
offered.

​

messages

object[]

required

A list of messages comprising the conversation so far.

Show child attributes

​

messages.content

string

required

The contents of the message in this turn of conversation.

​

messages.role

enum<string>

required

The role of the speaker in this turn of conversation. After the (optional)
system message, user and assistant roles should alternate with `user` then
`assistant`, ending in `user`.

Available options:

`system`,

`user`,

`assistant`

​

max_tokens

integer

The maximum number of completion tokens returned by the API. The number of
tokens requested in `max_tokens` plus the number of prompt tokens sent in
messages must not exceed the context window token limit of model requested. If
left unspecified, then the model will generate tokens until either it reaches
its stop token or the end of its context window.

​

temperature

number

default:

0.2

The amount of randomness in the response, valued between 0 inclusive and 2
exclusive. Higher values are more random, and lower values are more
deterministic.

Required range: `0 < x < 2`

​

top_p

number

default:

0.9

The nucleus sampling threshold, valued between 0 and 1 inclusive. For each
subsequent token, the model considers the results of the tokens with top_p
probability mass. We recommend either altering top_k or top_p, but not both.

Required range: `0 < x < 1`

​

search_domain_filter

any[]

Given a list of domains, limit the citations used by the online model to URLs
from the specified domains. Currently limited to only 3 domains for
whitelisting and blacklisting. For **blacklisting** add a `-` to the beginning
of the domain string. **Only available in certain tiers** \- refer to our
usage tiers [here](https://docs.perplexity.ai/guides/usage-tiers).

​

return_images

boolean

default:

false

Determines whether or not a request to an online model should return images.
**Only available in certain tiers** \- refer to our usage tiers
[here](https://docs.perplexity.ai/guides/usage-tiers).

​

return_related_questions

boolean

default:

false

Determines whether or not a request to an online model should return related
questions.**Only available in certain tiers** \- refer to our usage tiers
[here](https://docs.perplexity.ai/guides/usage-tiers).

​

search_recency_filter

string

Returns search results within the specified time interval - does not apply to
images. Values include `month`, `week`, `day`, `hour`.

​

top_k

number

default:

0

The number of tokens to keep for highest top-k filtering, specified as an
integer between 0 and 2048 inclusive. If set to 0, top-k filtering is
disabled. We recommend either altering top_k or top_p, but not both.

Required range: `0 < x < 2048`

​

stream

boolean

default:

false

Determines whether or not to incrementally stream the response with [server-
sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-
sent_events/Using_server-sent_events#event_stream_format) with `content-type:
text/event-stream`.

​

presence_penalty

number

default:

0

A value between -2.0 and 2.0. Positive values penalize new tokens based on
whether they appear in the text so far, increasing the model's likelihood to
talk about new topics. Incompatible with `frequency_penalty`.

Required range: `-2 < x < 2`

​

frequency_penalty

number

default:

1

A multiplicative penalty greater than 0. Values greater than 1.0 penalize new
tokens based on their existing frequency in the text so far, decreasing the
model's likelihood to repeat the same line verbatim. A value of 1.0 means no
penalty. Incompatible with `presence_penalty`.

Required range: `x > 0`

​

response_format

object

Enable structured outputs with a JSON or Regex schema. Refer to the guide
[here](https://docs.perplexity.ai/guides/structured-outputs) for more
information on how to use this parameter. **Only available in certain tiers**
\- refer to our usage tiers [here](https://docs.perplexity.ai/guides/usage-
tiers).

#### Response

200

200422

application/json

application/jsontext/event-stream

OK

​

id

string

An ID generated uniquely for each response.

​

model

string

The model used to generate the response.

​

object

string

The object type, which always equals `chat.completion`.

​

created

integer

The Unix timestamp (in seconds) of when the completion was created.

​

citations

any[]

Citations for the generated answer.

​

choices

object[]

The list of completion choices the model generated for the input prompt.

Show child attributes

​

choices.index

integer

​

choices.finish_reason

enum<string>

The reason the model stopped generating tokens. Possible values include `stop`
if the model hit a natural stopping point, or `length` if the maximum number
of tokens specified in the request was reached.

Available options:

`stop`,

`length`

​

choices.message

object

The message generated by the model.

Show child attributes

​

choices.message.content

string

required

The contents of the message in this turn of conversation.

​

choices.message.role

enum<string>

required

The role of the speaker in this turn of conversation. After the (optional)
system message, user and assistant roles should alternate with `user` then
`assistant`, ending in `user`.

Available options:

`system`,

`user`,

`assistant`

​

choices.delta

object

The incrementally streamed next tokens. Only meaningful when `stream = true`.

Show child attributes

​

choices.delta.content

string

required

The contents of the message in this turn of conversation.

​

choices.delta.role

enum<string>

required

The role of the speaker in this turn of conversation. After the (optional)
system message, user and assistant roles should alternate with `user` then
`assistant`, ending in `user`.

Available options:

`system`,

`user`,

`assistant`

​

usage

object

Usage statistics for the completion request.

Show child attributes

​

usage.prompt_tokens

integer

The number of tokens provided in the request prompt.

​

usage.completion_tokens

integer

The number of tokens generated in the response output.

​

usage.total_tokens

integer

The total number of tokens used in the chat completion (prompt + completion).

[twitter](https://twitter.com/perplexity_ai)[linkedin](https://www.linkedin.com/company/perplexity-
ai/)[discord](https://discord.com/invite/perplexity-
ai)[website](https://labs.perplexity.ai/)

cURL

Python

JavaScript

PHP

Go

Java

    
    
    curl --request POST \
      --url https://api.perplexity.ai/chat/completions \
      --header 'Authorization: Bearer <token>' \
      --header 'Content-Type: application/json' \
      --data '{
      "model": "sonar",
      "messages": [
        {
          "role": "system",
          "content": "Be precise and concise."
        },
        {
          "role": "user",
          "content": "How many stars are there in our galaxy?"
        }
      ],
      "max_tokens": 123,
      "temperature": 0.2,
      "top_p": 0.9,
      "search_domain_filter": null,
      "return_images": false,
      "return_related_questions": false,
      "search_recency_filter": "<string>",
      "top_k": 0,
      "stream": false,
      "presence_penalty": 0,
      "frequency_penalty": 1,
      "response_format": null
    }'

200

422

    
    
    {
      "id": "3c90c3cc-0d44-4b50-8888-8dd25736052a",
      "model": "sonar",
      "object": "chat.completion",
      "created": 1724369245,
      "citations": [
        "https://www.astronomy.com/science/astro-for-kids-how-many-stars-are-there-in-space/",
        "https://www.esa.int/Science_Exploration/Space_Science/Herschel/How_many_stars_are_there_in_the_Universe",
        "https://www.space.com/25959-how-many-stars-are-in-the-milky-way.html",
        "https://www.space.com/26078-how-many-stars-are-there.html",
        "https://en.wikipedia.org/wiki/Milky_Way"
      ],
      "choices": [
        {
          "index": 0,
          "finish_reason": "stop",
          "message": {
            "role": "assistant",
            "content": "The number of stars in the Milky Way galaxy is estimated to be between 100 billion and 400 billion stars. The most recent estimates from the Gaia mission suggest that there are approximately 100 to 400 billion stars in the Milky Way, with significant uncertainties remaining due to the difficulty in detecting faint red dwarfs and brown dwarfs."
          },
          "delta": {
            "role": "assistant",
            "content": ""
          }
        }
      ],
      "usage": {
        "prompt_tokens": 14,
        "completion_tokens": 70,
        "total_tokens": 84
      }
    }

================
File: resources/you/you_news.md
================
[You.com API home page![light logo](https://mintlify.s3.us-
west-1.amazonaws.com/you/logo/light.svg)![dark logo](https://mintlify.s3.us-
west-1.amazonaws.com/you/logo/dark.svg)](/)

Search or ask...

  * [Discord](https://discord.com/invite/youdotcom)
  * [Support](mailto:api@you.com)
  * [Support](mailto:api@you.com)

Search...

Navigation

API Guide

News API

[Welcome](/welcome)[Quickstart](/docs/quickstart)[API Reference](/api-
reference/smart)[API Guide](/api-modes/smart-api)

##### API Guide

  * [Smart API](/api-modes/smart-api)
  * [Research API](/api-modes/research-api)
  * [Search API](/api-modes/search-api)
  * [News API](/api-modes/news-api)

##### Custom

  * [Custom APIs](/api-modes/custom-api)

API Guide

# News API

##

​

Stay Informed with the Latest Global News

When interacting with news results, LLMs are currently facing challlenges:

## Lack of News-Specific Filtering

LLMs lack the capability to focus exclusively on real-time news updates,
limiting their relevance for time-sensitive information.

## Limited Recency Filtering

Models are generally unable to filter news based on specific timeframes,
preventing users from accessing the most relevant information for them.

With our **News API** , we ensure you have reliable insights into global news
and events, keeping you informed about the latest stories and developments
worldwide

## Access to Live News

Our API integrates live news data, providing long snippets from trusted
sources, complete with URLs for verification.

## Customizable Recency and Region

For news results, users can filter data by timeframes such as the past day,
week, or month, enabling access to the insights you need.

## Uniquely Long Snippets

Ensure your responses are trustworthy and contain the information you need.

##

​

Use Cases

## Stay Up to Date

  

Current Developments in Key Areas of Interest

query.py

    
    
    import requests
    
    url = "https://api.ydc-index.io/news"
    
    query = {"query":"Latest News on Chipmakers"}
    
    headers = {"X-API-Key": "YOUR_API_KEY"}
    
    response = requests.request("GET", url, headers=headers, params=query)
    
    print(response.text)
    

Response

    
    
    {
    "news": {
    "query": {
      "original": "Latest News on Chipmakers",
      "show_strict_warning": false,
      "spellcheck_off": false
    },
    "results": [
      {
        "age": "4 days ago",
        "description": "The country that was once the world’s largest producer of semiconductors has embarked on a quest to return to the top.",
        "meta_url": {
          "hostname": "www.bloomberg.com",
          "netloc": "bloomberg.com",
          "path": "› opinion  › articles  › 2024-12-12  › japan-chipmakers-gamble-the-future-of-semiconductors-on-the-past",
          "scheme": "https"
        },
        "page_age": "2024-12-12T22:37:49",
        "source_name": "Bloomberg L.P.",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/qlX4Mz061puuwMyZm1FuXEbSGXuCbqjHIypccDoXGqw/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9hc3Nl/dHMuYndieC5pby9p/bWFnZXMvdXNlcnMv/aXFqV0hCRmRmeElV/L2lYTWNFSDBjMFli/OC92MS8xMjAweDgw/MC5qcGc"
        },
        "title": "Japan Chipmakers Gamble the Future of Semiconductors on the Past - Bloomberg",
        "type": "news_result",
        "url": "https://www.bloomberg.com/opinion/articles/2024-12-12/japan-chipmakers-gamble-the-future-of-semiconductors-on-the-past"
      },
      {
        "age": "2 weeks ago",
        "description": "Chinese companies should buy locally instead, four of the country's top industry associations said.",
        "meta_url": {
          "hostname": "www.reuters.com",
          "netloc": "reuters.com",
          "path": "› technology  › chinese-firms-should-diversify-chip-sources-internet-society-china-says-2024-12-03",
          "scheme": "https"
        },
        "page_age": "2024-12-04T12:28:42",
        "source_name": "reuters.com",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/BuXIjhV4mQs66s0tgItvuiy4x3EBC-ZpqgaeQsLvF-Y/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly93d3cu/cmV1dGVycy5jb20v/cmVzaXplci92Mi9S/WlRXVE9YSkNCUDVS/RzM3RURQRERMWTdM/US5qcGc_YXV0aD02/YTI3MDBmMjJhMGJj/Y2I4NzE0OTM4M2Qz/YTZlODkyODg4YWI4/YTk4MTUwMmI2NGQ1/MDNlNDZmZDY0YjVl/MjRkJmFtcDtoZWln/aHQ9MTAwNSZhbXA7/d2lkdGg9MTkyMCZh/bXA7cXVhbGl0eT04/MCZhbXA7c21hcnQ9/dHJ1ZQ"
        },
        "title": "US chips are 'no longer safe,' Chinese industry bodies say in latest trade salvo | Reuters",
        "type": "news_result",
        "url": "https://www.reuters.com/technology/chinese-firms-should-diversify-chip-sources-internet-society-china-says-2024-12-03/"
      },
      {
        "age": "2 weeks ago",
        "description": "Microchip Technology Inc. is pausing its application for US semiconductor subsidies, making it the first known company to step back from a program designed to revitalize American chipmaking.",
        "meta_url": {
          "hostname": "www.bloomberg.com",
          "netloc": "bloomberg.com",
          "path": "› news  › articles  › 2024-12-03  › microchip-pauses-chips-act-application-after-scaling-back-plans",
          "scheme": "https"
        },
        "page_age": "2024-12-03T22:47:04",
        "source_name": "Bloomberg L.P.",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/va8gs8bDTD44dQtxAjHvM8dB9Trdl_puEs2DlPWJlRM/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9hc3Nl/dHMuYndieC5pby9p/bWFnZXMvdXNlcnMv/aXFqV0hCRmRmeElV/L2llQThZQ3hjNXFp/TS92MC8xMjAweDgw/MC5qcGc"
        },
        "title": "Microchip Pauses Chips Act Application Amid Inventory Woes - Bloomberg",
        "type": "news_result",
        "url": "https://www.bloomberg.com/news/articles/2024-12-03/microchip-pauses-chips-act-application-after-scaling-back-plans"
      },
      {
        "age": "2 weeks ago",
        "description": "The Chinese government has slammed America’s introduction of fresh export controls on US-made semiconductors that Washington fears Beijing could use to make the next generation of weapons and artificial intelligence (AI) systems.",
        "meta_url": {
          "hostname": "www.cnn.com",
          "netloc": "cnn.com",
          "path": "› 2024  › 12  › 02  › tech  › china-us-chips-new-restrictions-intl-hnk  › index.html",
          "scheme": "https"
        },
        "page_age": "2024-12-03T02:18:13",
        "source_name": "CNN",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/Q7u2YWA5NPRkhPy3LpctiPU11jkmoP6uYjggAeJ4J6s/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9tZWRp/YS5jbm4uY29tL2Fw/aS92MS9pbWFnZXMv/c3RlbGxhci9wcm9k/L2FwMjQyOTE1NDEy/Njk1OTEtY29weS5q/cGc_Yz0xNng5JnE9/d184MDAsY19maWxs"
        },
        "title": "AI and semiconductors: China hits out at latest US effort to block Beijing’s access to chip technology | CNN Business",
        "type": "news_result",
        "url": "https://www.cnn.com/2024/12/02/tech/china-us-chips-new-restrictions-intl-hnk/index.html"
      },
      {
        "age": "2 weeks ago",
        "description": "The US will launch its third crackdown in three years on China's semiconductor industry on Monday, restricting exports to 140 companies.",
        "meta_url": {
          "hostname": "www.reuters.com",
          "netloc": "reuters.com",
          "path": "› technology  › latest-us-strike-chinas-chips-hits-semiconductor-toolmakers-2024-12-02",
          "scheme": "https"
        },
        "page_age": "2024-12-03T01:14:17",
        "source_name": "reuters.com",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/UQxYf9pHn51WR53MNkbaa9ieeQZStKI64CetvaW6Tt4/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly93d3cu/cmV1dGVycy5jb20v/cmVzaXplci92Mi9E/VlRNTERBWkRWSkk3/RTdDUzZJUEtEUVRV/RS5qcGc_YXV0aD0x/OGZkYzAxZmNlZmEx/ZDk4MjIxODFhYTY0/YzA2YTBjNWFjMDk4/YWIxMmM2NzBjZmRh/NDlmYmRkY2Y2MDNh/ZjY0JmFtcDtoZWln/aHQ9MTAwNSZhbXA7/d2lkdGg9MTkyMCZh/bXA7cXVhbGl0eT04/MCZhbXA7c21hcnQ9/dHJ1ZQ"
        },
        "title": "Latest US clampdown on China's chips hits semiconductor toolmakers | Reuters",
        "type": "news_result",
        "url": "https://www.reuters.com/technology/latest-us-strike-chinas-chips-hits-semiconductor-toolmakers-2024-12-02/"
      },
      {
        "age": "3 weeks ago",
        "description": "There’s been a sea change among tech investors during the past month: Software stocks are hot, while semiconductor makers are not.",
        "meta_url": {
          "hostname": "www.bloomberg.com",
          "netloc": "bloomberg.com",
          "path": "› news  › articles  › 2024-11-26  › software-is-in-chips-are-out-as-traders-position-for-trump-era",
          "scheme": "https"
        },
        "page_age": "2024-11-26T12:01:25",
        "source_name": "Bloomberg L.P.",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/LxYcJMko0ujutqhct8xx6kD_wc32kwIH0BJJTPle4cM/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9hc3Nl/dHMuYndieC5pby9p/bWFnZXMvdXNlcnMv/aXFqV0hCRmRmeElV/L2k3aDhSbUZuMjcy/US92MC8xMjAweDgw/MC5qcGc"
        },
        "title": "Software Is In, Chips Are Out as Traders Position for Trump Era - Bloomberg",
        "type": "news_result",
        "url": "https://www.bloomberg.com/news/articles/2024-11-26/software-is-in-chips-are-out-as-traders-position-for-trump-era"
      },
      {
        "age": "4 weeks ago",
        "description": "Understanding how AI impacts business. The latest news on advancements in artificial intelligence, how to use AI, and how to invest in AI.",
        "meta_url": {
          "hostname": "www.bloomberg.com",
          "netloc": "bloomberg.com",
          "path": "› ai",
          "scheme": "https"
        },
        "page_age": "2024-11-21T21:05:04",
        "source_name": "Bloomberg L.P.",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/m7Jfen96xPtekEXCptzbwN1D52cE6auJKE9sP5DajpU/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly93d3cu/Ymxvb21iZXJnLmNv/bS9haQ"
        },
        "title": "AI - Bloomberg",
        "type": "news_result",
        "url": "https://www.bloomberg.com/ai"
      },
      {
        "age": "1 month ago",
        "description": "The Biden administration is trying to shore up its CHIPS Act funding agreements before Donald Trump takes office.",
        "meta_url": {
          "hostname": "www.businessinsider.com",
          "netloc": "businessinsider.com",
          "path": "› chips-act-funding-biden-administration-trump-tariffs-repeal-2024-11",
          "scheme": "https"
        },
        "page_age": "2024-11-19T09:15:01",
        "source_name": "Business Insider",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/_16b6eIPhruMXBpV8ovQHxtj5qc_0nTGW5SHlHYseAg/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9pLmlu/c2lkZXIuY29tLzY3/M2JiM2Y0ZmEwMTQw/Y2RkNTY0Mjk5ZT93/aWR0aD0xMjAwJmZv/cm1hdD1qcGVn"
        },
        "title": "The Biden administration is scrambling to send billions to chipmakers before Trump takes over",
        "type": "news_result",
        "url": "https://www.businessinsider.com/chips-act-funding-biden-administration-trump-tariffs-repeal-2024-11"
      },
      {
        "age": "1 month ago",
        "description": "JONATHAN: NVIDIA IS HUGE SO WE ALL HAVE EXPOSURE TO IT ONE WAY OR ANOTHER. EXPLAIN THE STORY THIS MORNING, REPORTING THE CHIPMAKER IS FACING A DESIGN SNACK FOR THE BLACKWELL CHIPS. WE SEE THAT DROPPING IN AND OUT OF THE NEWS REPEATEDLY, SHAKING UP THE STOCK FROM TIME TO TIME.",
        "meta_url": {
          "hostname": "www.bloomberg.com",
          "netloc": "bloomberg.com",
          "path": "› news  › videos  › 2024-11-18  › chipmakers-hope-for-a-boost-from-nvidia-earnigns-video",
          "scheme": "https"
        },
        "page_age": "2024-11-18T14:17:33",
        "source_name": "Bloomberg L.P.",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/BzYM6UHOgK9rvTWBxa_JK-buc5-4LfUelVxMrd_tl6A/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9hc3Nl/dHMuYndieC5pby9p/bWFnZXMvdXNlcnMv/aXFqV0hCRmRmeElV/L2lCejhkVm53dEVo/WS92My8tMXgtMS5q/cGc"
        },
        "title": "Watch Chipmakers Hope for a Boost From Nvidia Earnings - Bloomberg",
        "type": "news_result",
        "url": "https://www.bloomberg.com/news/videos/2024-11-18/chipmakers-hope-for-a-boost-from-nvidia-earnigns-video"
      },
      {
        "age": "1 month ago",
        "description": "Nov 17 (Reuters) - Nvidia's (NVDA.O), ... reported on Sunday. The Blackwell graphics processing units overheat when connected together in server racks designed to hold up to 72 chips, the report said, citing sources familiar with the issue. The chipmaker has asked its suppliers ...",
        "meta_url": {
          "hostname": "www.reuters.com",
          "netloc": "reuters.com",
          "path": "› technology  › artificial-intelligence  › new-nvidia-ai-chips-face-issue-with-overheating-servers-information-reports-2024-11-17",
          "scheme": "https"
        },
        "page_age": "2024-11-17T22:36:10",
        "source_name": "reuters.com",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/16Jgf2p3zxsmMnQUzOrP7EhvynVj9rnuSDEGYINQ-HQ/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly93d3cu/cmV1dGVycy5jb20v/cmVzaXplci92Mi9W/VFhIVkFJM1ZCT1lU/QVFSRzdOQlZMNUVJ/SS5qcGc_YXV0aD1m/ODQxZmExMzY3ODMz/ZGRjMmJhMTQ5MzJi/YTQ5Yzc2ZmY2MTNm/NmI0MWI3ZjM3Y2Yx/ZDU5ZDZiZjA2ZjYw/ZjJhJmFtcDtoZWln/aHQ9MTAwNSZhbXA7/d2lkdGg9MTkyMCZh/bXA7cXVhbGl0eT04/MCZhbXA7c21hcnQ9/dHJ1ZQ"
        },
        "title": "New Nvidia AI chips overheating in servers, the Information reports | Reuters",
        "type": "news_result",
        "url": "https://www.reuters.com/technology/artificial-intelligence/new-nvidia-ai-chips-face-issue-with-overheating-servers-information-reports-2024-11-17/"
      },
      {
        "age": "November 8, 2024",
        "description": "China hardliners in Congress are calling on the world's foremost semiconductor equipment makers - KLA , LAM , Applied Materials , Tokyo Electron and ASML - to provide details of their sales to China.",
        "meta_url": {
          "hostname": "www.reuters.com",
          "netloc": "reuters.com",
          "path": "› technology  › us-lawmakers-press-top-chip-equipment-makers-details-china-sales-2024-11-08",
          "scheme": "https"
        },
        "page_age": "2024-11-08T18:05:21",
        "source_name": "reuters.com",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/UQxYf9pHn51WR53MNkbaa9ieeQZStKI64CetvaW6Tt4/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly93d3cu/cmV1dGVycy5jb20v/cmVzaXplci92Mi9E/VlRNTERBWkRWSkk3/RTdDUzZJUEtEUVRV/RS5qcGc_YXV0aD0x/OGZkYzAxZmNlZmEx/ZDk4MjIxODFhYTY0/YzA2YTBjNWFjMDk4/YWIxMmM2NzBjZmRh/NDlmYmRkY2Y2MDNh/ZjY0JmFtcDtoZWln/aHQ9MTAwNSZhbXA7/d2lkdGg9MTkyMCZh/bXA7cXVhbGl0eT04/MCZhbXA7c21hcnQ9/dHJ1ZQ"
        },
        "title": "US lawmakers press top chip equipment makers for details on China sales | Reuters",
        "type": "news_result",
        "url": "https://www.reuters.com/technology/us-lawmakers-press-top-chip-equipment-makers-details-china-sales-2024-11-08/"
      },
      {
        "age": "November 8, 2024",
        "description": "The Biden administration is racing to complete Chips Act agreements with companies like Intel Corp. and Samsung Electronics Co., aiming to shore up one of its signature initiatives before President-elect Donald Trump enters the White House.",
        "meta_url": {
          "hostname": "www.bloomberg.com",
          "netloc": "bloomberg.com",
          "path": "› news  › articles  › 2024-11-08  › trump-s-win-sets-off-race-to-complete-chips-act-subsidy-deals",
          "scheme": "https"
        },
        "page_age": "2024-11-08T01:42:07",
        "source_name": "Bloomberg L.P.",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/ei0BLNvbyDZ0k8vGo3QdI7Pm8QtABfzqpfz0ucNHOa0/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9hc3Nl/dHMuYndieC5pby9p/bWFnZXMvdXNlcnMv/aXFqV0hCRmRmeElV/L2l4T25CbS4zdFhq/Yy92MC8xMjAweDgw/MC5qcGc"
        },
        "title": "Trump’s Win Has Biden Rushing to Finalize Chips Act Deals With Intel, Samsung - Bloomberg",
        "type": "news_result",
        "url": "https://www.bloomberg.com/news/articles/2024-11-08/trump-s-win-sets-off-race-to-complete-chips-act-subsidy-deals"
      },
      {
        "age": "November 4, 2024",
        "description": "Intel has fallen so far so fast that the chipmaker's stock price is no longer having an impact on the Dow Jones Industrial Average.",
        "meta_url": {
          "hostname": "www.cnbc.com",
          "netloc": "cnbc.com",
          "path": "› 2024  › 11  › 04  › the-dow-needs-nvidia-because-intel-plunge-made-semis-underrepresented.html",
          "scheme": "https"
        },
        "page_age": "2024-11-04T20:59:19",
        "source_name": "CNBC",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/1uMlnbYCbjdLfqxeP9fQOibcd2dVPGJwIDWZ65jYr8k/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9pbWFn/ZS5jbmJjZm0uY29t/L2FwaS92MS9pbWFn/ZS8xMDgwMzI5OTIt/MTcyNjEwNzgwMzAw/NS1nZXR0eWltYWdl/cy0yMTU1MTI3Mzc5/LU5WSURJQV9IVUFO/Ry5qcGVnP3Y9MTcz/MjE5ODI4OCZhbXA7/dz0xOTIwJmFtcDto/PTEwODA"
        },
        "title": "The Dow needs Nvidia to give chipmakers representation in index after Intel's plunge",
        "type": "news_result",
        "url": "https://www.cnbc.com/2024/11/04/the-dow-needs-nvidia-because-intel-plunge-made-semis-underrepresented.html"
      },
      {
        "age": "October 30, 2024",
        "description": "Global stock indexes edged lower on Wednesday as a disappointing forecast from Advanced Micro Devices weighed on chipmakers, while gold prices rose to a record high as uncertainty ahead of next week's U.S. presidential election drove safe-haven demand.",
        "meta_url": {
          "hostname": "www.reuters.com",
          "netloc": "reuters.com",
          "path": "› markets  › global-markets-wrapup-1-2024-10-30",
          "scheme": "https"
        },
        "page_age": "2024-10-30T21:05:08",
        "source_name": "reuters.com",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/nzLyG1gwaIEPPDPWvUB6P6_z5aoPvHTWttuhFJ-AZZ4/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly93d3cu/cmV1dGVycy5jb20v/cmVzaXplci92Mi9O/MkJQN0VHM1RGT0dI/REFMSlFGVDNJRTZa/WS5qcGc_YXV0aD0z/YmE1ZmUyZGZkYzkx/MjAzZmU0YzVjZWE2/MWNhNmJiNjUzYTc0/ZjhlMjQxMjEzNDAx/NTUzYTYyMWZiODdm/M2E3JmFtcDtoZWln/aHQ9MTAwNSZhbXA7/d2lkdGg9MTkyMCZh/bXA7cXVhbGl0eT04/MCZhbXA7c21hcnQ9/dHJ1ZQ"
        },
        "title": "Stocks fall with chipmakers, gold hits record high | Reuters",
        "type": "news_result",
        "url": "https://www.reuters.com/markets/global-markets-wrapup-1-2024-10-30/"
      },
      {
        "age": "October 30, 2024",
        "description": "OpenAI is working with Broadcom and TSMC to build its first in-house chip designed to support its artificial intelligence systems, while adding AMD chips alongside Nvidia chips to meet its surging infrastructure demands, sources told Reuters.",
        "meta_url": {
          "hostname": "www.reuters.com",
          "netloc": "reuters.com",
          "path": "› technology  › artificial-intelligence  › openai-builds-first-chip-with-broadcom-tsmc-scales-back-foundry-ambition-2024-10-29",
          "scheme": "https"
        },
        "page_age": "2024-10-30T15:30:09",
        "source_name": "reuters.com",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/JfpxYjJ_YO4fhT-Ejlkn0j1V4oqVpLdCQ7LIqyAlBOM/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly93d3cu/cmV1dGVycy5jb20v/cmVzaXplci92Mi83/M0I1NTRIS0JCTUZI/S0tNTE82RlZFVTNH/US5qcGc_YXV0aD04/NDlhODM1MDIwZGVj/YjBhMjI1ZDM5MGVk/YmE4NzI0YTNkMWYz/YzI2ZmE1YjM4OGMy/ZWU1Y2NkNWVkZmU3/YWYzJmFtcDtoZWln/aHQ9MTAwNSZhbXA7/d2lkdGg9MTkyMCZh/bXA7cXVhbGl0eT04/MCZhbXA7c21hcnQ9/dHJ1ZQ"
        },
        "title": "Exclusive: OpenAI builds first chip with Broadcom and TSMC, scales back foundry ambition | Reuters",
        "type": "news_result",
        "url": "https://www.reuters.com/technology/artificial-intelligence/openai-builds-first-chip-with-broadcom-tsmc-scales-back-foundry-ambition-2024-10-29/"
      },
      {
        "age": "October 16, 2024",
        "description": "A rally in semiconductor names and strong economic data sent stocks higher on Thursday.",
        "meta_url": {
          "hostname": "www.cnbc.com",
          "netloc": "cnbc.com",
          "path": "› 2024  › 10  › 16  › stock-market-today-live-updates.html",
          "scheme": "https"
        },
        "page_age": "2024-10-16T22:02:35",
        "source_name": "CNBC",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/3z4hAEdUIDVi62nmwGTixjpwr8RNndn7RzgS7AzkBpU/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9pbWFn/ZS5jbmJjZm0uY29t/L2FwaS92MS9pbWFn/ZS8xMDgwNDQ3MjUt/MTcyODM5Nzg5MDg1/NC1nZXR0eWltYWdl/cy0yMTc3NDM3MDE5/LW1zMV8xNzQ0X3ln/cXlwem9oLmpwZWc_/dj0xNzI5MTgxMTAz/JmFtcDt3PTE5MjAm/YW1wO2g9MTA4MA"
        },
        "title": "Dow closes at fresh record, Nasdaq ends higher as chipmakers rally: Live updates",
        "type": "news_result",
        "url": "https://www.cnbc.com/2024/10/16/stock-market-today-live-updates.html"
      },
      {
        "age": "October 16, 2024",
        "description": "ASML’s surprise results have implications for the overall chip industry. But first...",
        "meta_url": {
          "hostname": "www.bloomberg.com",
          "netloc": "bloomberg.com",
          "path": "› news  › newsletters  › 2024-10-16  › asml-s-surprise-results-signal-uncertain-future-for-some-chipmakers",
          "scheme": "https"
        },
        "page_age": "2024-10-16T11:05:28",
        "source_name": "Bloomberg L.P.",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/CCtgpMYTTo_tqrCUjHj2hE4j33_v3pN_12gPUXztUOk/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9hc3Nl/dHMuYndieC5pby9p/bWFnZXMvdXNlcnMv/aXFqV0hCRmRmeElV/L2k1QnRoejdlb1Zt/TS92MS8xMjAweDgw/MC5qcGc"
        },
        "title": "ASML’s Surprise Results Signal Uncertain Future for Some Chipmakers - Bloomberg",
        "type": "news_result",
        "url": "https://www.bloomberg.com/news/newsletters/2024-10-16/asml-s-surprise-results-signal-uncertain-future-for-some-chipmakers"
      },
      {
        "age": "October 16, 2024",
        "description": "ASML Holding NV has lost more than €60 billion ($65.3 billion) in market value since it reported weak orders for its chipmaking machines, forcing investors to reevaluate the health of the industry.",
        "meta_url": {
          "hostname": "www.bloomberg.com",
          "netloc": "bloomberg.com",
          "path": "› news  › articles  › 2024-10-16  › asml-s-tumble-fuels-questions-about-chip-industry-s-prospects",
          "scheme": "https"
        },
        "page_age": "2024-10-16T10:36:26",
        "source_name": "Bloomberg L.P.",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/FRs8PrWQma_nu8Z4wCUNX20wYoNYhRPA5K8oYZWYcL0/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9hc3Nl/dHMuYndieC5pby9p/bWFnZXMvdXNlcnMv/aXFqV0hCRmRmeElV/L2k2dUkwNDdXTWpC/OC92MC8xMjAweDgw/MC5qcGc"
        },
        "title": "ASML’s Plunge Shows the Diverging Fortunes of Chipmakers From AI - Bloomberg",
        "type": "news_result",
        "url": "https://www.bloomberg.com/news/articles/2024-10-16/asml-s-tumble-fuels-questions-about-chip-industry-s-prospects"
      },
      {
        "age": "October 16, 2024",
        "description": "Chip stocks fell Tuesday. ASML slashed its guidance for 2025, and the sector mulled reports of more potential restrictions on chip exports from some US firms.",
        "meta_url": {
          "hostname": "ca.finance.yahoo.com",
          "netloc": "ca.finance.yahoo.com",
          "path": "› news  › chipmakers-tumble-asml-forecast-cut-030444530.html",
          "scheme": "https"
        },
        "page_age": "2024-10-16T03:04:44",
        "source_name": "ca.finance.yahoo.com",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/a2VYr800beR71JP283brHdStO-voqPux2jfJuPjnnuw/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9zLnlp/bWcuY29tL255L2Fw/aS9yZXMvMS4yL1ho/OU5CUEk1WEFLR2Np/S3puREk1TmctLS9Z/WEJ3YVdROWFHbG5h/R3hoYm1SbGNqdDNQ/VEV5TURBN2FEMDVN/REEtL2h0dHBzOi8v/bWVkaWEuemVuZnMu/Y29tL2VuL2J1c2lu/ZXNzX2luc2lkZXJf/YXJ0aWNsZXNfODg4/LzEzMjI5YmQ0OWEz/NjhmYzQ4YmViZWZl/ZjUwZTcwMjU4"
        },
        "title": "Chipmakers tumble as ASML forecast cut issues a growth warning to the sector",
        "type": "news_result",
        "url": "https://ca.finance.yahoo.com/news/chipmakers-tumble-asml-forecast-cut-030444530.html"
      },
      {
        "age": "October 15, 2024",
        "description": "Manage your newsletter preferences anytime here. ASML Holding’s shares plunged the most in 26 years after it booked only about half the orders analysts expected, a startling slowdown for the Dutch company, maker of the world’s most advanced chip-making machines and one of the bellwethers ...",
        "meta_url": {
          "hostname": "www.bloomberg.com",
          "netloc": "bloomberg.com",
          "path": "› news  › newsletters  › 2024-10-15  › asml-shares-plunge-amid-strange-times-for-chipmakers-like-intel-samsung",
          "scheme": "https"
        },
        "page_age": "2024-10-15T22:14:46",
        "source_name": "Bloomberg L.P.",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/iQL7-057RrQw0oDsHMrzhHX5ksoVJeP0D_tecxHpO1E/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9hc3Nl/dHMuYndieC5pby9p/bWFnZXMvdXNlcnMv/aXFqV0hCRmRmeElV/L2lhaWNjSmhPMTc4/US92MC8xMjAweDgw/MC5qcGc"
        },
        "title": "ASML Shares Plunge Amid Strange Times for Chipmakers Like Intel, Samsung - Bloomberg",
        "type": "news_result",
        "url": "https://www.bloomberg.com/news/newsletters/2024-10-15/asml-shares-plunge-amid-strange-times-for-chipmakers-like-intel-samsung"
      }
    ],
    "type": "news"
    }
    }
    

Country-specific Industry Overviews

query.py

    
    
    import requests
    
    url = "https://api.ydc-index.io/news"
    
    querystring = {"query":"News on the Chemical Industry", "country":"IN"}
    
    headers = {"X-API-Key": "YOUR_API_KEY"}
    
    response = requests.request("GET", url, headers=headers, params=querystring)
    
    print(response.text)
    

reponse

    
    
    {
    "news": {
    "query": {
      "original": "Chemical Industry",
      "show_strict_warning": false,
      "spellcheck_off": false
    },
    "results": [
      {
        "age": "2 weeks ago",
        "description": "On December 3 2024 Sudarshan Chemical Industries a midcap company in the dyes and pigments industry saw a 5 02 increase in its stock outperforming the sector by 2 7 This marks the fourth consecutive day of gains with a total increase of 11 34 in the past four days The stock is currently trading ...",
        "meta_url": {
          "hostname": "www.marketsmojo.com",
          "netloc": "marketsmojo.com",
          "path": "› news  › stocks-in-action  › sudarshan-chemical-industries-stock-sees-positive-trend-outperforms-sector-and-market-308679",
          "scheme": "https"
        },
        "page_age": "2024-12-03T05:30:02",
        "source_name": "marketsmojo.com",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/YiUpk2EMKEHCCGwC57406-c57F3JKfEZr6r3jUKE_jk/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9pLm1h/cmtldHNtb2pvLmNv/bS9uZXdzaW1nLzIw/MjQvMTIvU3VkYXJz/aGFuQ2hlbWljX3By/aWNlUmVsYXRlZGZh/Y3RvcnNfMjMxNjI5/LnBuZw"
        },
        "title": "Sudarshan Chemical Industries' Stock Sees Positive Trend, Outperforms Sector and Market",
        "type": "news_result",
        "url": "https://www.marketsmojo.com/news/stocks-in-action/sudarshan-chemical-industries-stock-sees-positive-trend-outperforms-sector-and-market-308679"
      },
      {
        "age": "2 weeks ago",
        "description": "Chemical industry in Tamil Nadu faces challenges, seeks government support for cracker project, growth projections shared at Chemvision 2024.",
        "meta_url": {
          "hostname": "www.thehindu.com",
          "netloc": "thehindu.com",
          "path": "› news  › national  › tamil-nadu  › chemical-industry-calls-for-cracker-project-to-boost-growth-in-tamil-nadu  › article68927363.ece",
          "scheme": "https"
        },
        "page_age": "2024-12-02T15:30:31",
        "source_name": "The Hindu",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/MbaLnkSns7korT9MlnYjmNHGa1WUAhQ3UWPKsATH8sw/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly90aC1p/LnRoZ2ltLmNvbS9w/dWJsaWMvaW5jb21p/bmcvNnppYXh2L2Fy/dGljbGU2ODkzOTQ1/Ni5lY2UvYWx0ZXJu/YXRlcy9MQU5EU0NB/UEVfMTIwMC9CVlJf/NDQ1NS5KUEc"
        },
        "title": "Chemical industry calls for cracker project to boost growth in Tamil Nadu - The Hindu",
        "type": "news_result",
        "url": "https://www.thehindu.com/news/national/tamil-nadu/chemical-industry-calls-for-cracker-project-to-boost-growth-in-tamil-nadu/article68927363.ece"
      },
      {
        "age": "2 weeks ago",
        "description": "Paradeep Phosphates Limited has taken the planned shutdown of the ammonia and urea plants at Goa",
        "meta_url": {
          "hostname": "www.indianchemicalnews.com",
          "netloc": "indianchemicalnews.com",
          "path": "› general  › briefs-paradeep-phosphates-and-upl-24269",
          "scheme": "https"
        },
        "page_age": "2024-12-02T14:35:03",
        "source_name": "indianchemicalnews.com",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/Vl4q-rbHKGG5hwNVyuoaLx_Igp-MdbrerzkvrrHSwGQ/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly93d3cu/aW5kaWFuY2hlbWlj/YWxuZXdzLmNvbS9w/dWJsaWMvdXBsb2Fk/cy9uZXdzLzIwMjQv/MTIvMjQyNjkvVVBM/X2xvZ29fb3JpZ2lu/YWwuanBn"
        },
        "title": "Briefs: Paradeep Phosphates and UPL",
        "type": "news_result",
        "url": "https://www.indianchemicalnews.com/general/briefs-paradeep-phosphates-and-upl-24269"
      },
      {
        "age": "2 weeks ago",
        "description": "Press release - Exactitude Consultancy - Crop Protection Chemicals Market Detailed Industry Report Analysis 2024-2032 | BASF, Syngenta, Bayer Crop Science - published on openPR.com",
        "meta_url": {
          "hostname": "www.openpr.com",
          "netloc": "openpr.com",
          "path": "› news  › 3766044  › crop-protection-chemicals-market-detailed-industry-report",
          "scheme": "https"
        },
        "page_age": "2024-12-02T10:46:15",
        "source_name": "openpr.com",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/pAlDpCwJjcEf6Ypm-K1Tl8JBCNjyRa73ZfUkRYZwVXU/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9jZG4u/b3Blbi1wci5jb20v/TC9jL0xjMDIxNTQ3/NjdfZy5qcGc"
        },
        "title": "Crop Protection Chemicals Market Detailed Industry Report Analysis 2024-2032 | BASF, Syngenta, Bayer Crop Science",
        "type": "news_result",
        "url": "https://www.openpr.com/news/3766044/crop-protection-chemicals-market-detailed-industry-report"
      },
      {
        "age": "5 days ago",
        "description": "CII Visakhapatnam to organise conference on Industrial & Chemical Safety from December 12",
        "meta_url": {
          "hostname": "www.thehindu.com",
          "netloc": "thehindu.com",
          "path": "› news  › cities  › Visakhapatnam  › cii-visakhapatnam-to-organise-conference-on-industrial-chemical-safety-from-december-12  › article68973107.ece",
          "scheme": "https"
        },
        "page_age": "2024-12-11T12:18:13",
        "source_name": "The Hindu",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/llajowBpmgWaVpuUx0Kw-DZmR0pl3GHEUnH8UFCou-g/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly93d3cu/dGhlaGluZHUuY29t/L3RoZW1lL2ltYWdl/cy9vZy1pbWFnZS5w/bmc"
        },
        "title": "CII Visakhapatnam to organise conference on Industrial & Chemical Safety from December 12 - The Hindu",
        "type": "news_result",
        "url": "https://www.thehindu.com/news/cities/Visakhapatnam/cii-visakhapatnam-to-organise-conference-on-industrial-chemical-safety-from-december-12/article68973107.ece"
      },
      {
        "age": "3 weeks ago",
        "description": "CII Tamil Nadu Chemvision 2024 highlights sustainability, safety, investment opportunities, and capability building in the chemical industry.",
        "meta_url": {
          "hostname": "www.thehindu.com",
          "netloc": "thehindu.com",
          "path": "› news  › national  › tamil-nadu  › cii-tamil-nadu-chemvision-2024-to-be-held-on-november-29  › article68923289.ece",
          "scheme": "https"
        },
        "page_age": "2024-11-28T17:55:13",
        "source_name": "The Hindu",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/llajowBpmgWaVpuUx0Kw-DZmR0pl3GHEUnH8UFCou-g/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly93d3cu/dGhlaGluZHUuY29t/L3RoZW1lL2ltYWdl/cy9vZy1pbWFnZS5w/bmc"
        },
        "title": "CII Tamil Nadu Chemvision 2024 to be held on November 29 - The Hindu",
        "type": "news_result",
        "url": "https://www.thehindu.com/news/national/tamil-nadu/cii-tamil-nadu-chemvision-2024-to-be-held-on-november-29/article68923289.ece"
      },
      {
        "age": "1 month ago",
        "description": "Fire officials said the incident occurred when chemical sludge, which was awaiting transport to a cement factory, caught fire. Firefighters doused the flames before fire could spread to the main unit. The reactor and boilers, which are key components of the chemical industry, were not damaged ...",
        "meta_url": {
          "hostname": "timesofindia.indiatimes.com",
          "netloc": "timesofindia.indiatimes.com",
          "path": "› city  › hyderabad  › massive-fire-erupts-at-chemical-facility-near-hyderabad-no-casualties-reported  › articleshow  › 115369330.cms",
          "scheme": "https"
        },
        "page_age": "2024-11-16T19:01:00",
        "source_name": "timesofindia.indiatimes.com",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/2lwy_RjV_cNso39qFr6pfrjPykcR3cRKhHnm_Z6LzJI/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9zdGF0/aWMudG9paW1nLmNv/bS90aHVtYi9tc2lk/LTExNTM2OTMyNyx3/aWR0aC0xMDcwLGhl/aWdodC01ODAsaW1n/c2l6ZS01NjQ2MCxy/ZXNpemVtb2RlLTc1/LG92ZXJsYXktdG9p/X3N3LHB0LTMyLHlf/cGFkLTQwL3Bob3Rv/LmpwZw"
        },
        "title": "Massive Fire Erupts at Chemical Facility Near Hyderabad, No Casualties Reported | - Times of India",
        "type": "news_result",
        "url": "https://timesofindia.indiatimes.com/city/hyderabad/massive-fire-erupts-at-chemical-facility-near-hyderabad-no-casualties-reported/articleshow/115369330.cms"
      },
      {
        "age": "October 24, 2024",
        "description": "সম্প্রতি যে সংস্থা দেশের সেরা কোম্পানির পুরস্কার (তালিকাভুক্ত কোম্পানির মধ্যে) পেয়েছে, সেটিই সিঙ্গুরে ২২০ কোটি ...",
        "meta_url": {
          "hostname": "bangla.hindustantimes.com",
          "netloc": "bangla.hindustantimes.com",
          "path": "› pictures  › big-investment-for-singur-himadri-speciality-chemical-limited-to-invest-rs-220-crore-for-31729757880342.html",
          "scheme": "https"
        },
        "page_age": "2024-10-24T08:29:45",
        "source_name": "bangla.hindustantimes.com",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/P2qC9UIGEGMbr_jLcGcbrqITQpUIY6NaltTu8DBiEB8/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9pbWFn/ZXMuaGluZHVzdGFu/dGltZXMuY29tL2Jh/bmdsYS9pbWcvMjAy/NC8xMC8yNC82MDB4/MzM4L0lORElBLVNU/RUVMLTBfMTcyOTc1/Nzg3MTk5NF8xNzI5/NzU4NTg1MDQ4LkpQ/Rw"
        },
        "title": "₹220 cr investment at Singur: সিঙ্গুরে ২২০ কোটি টাকার বিনিয়োগ আসছে! লগ্নি করছে দেশের সেরা হওয়া সংস্থা, কী হবে? - Big investment for Singur, Himadri Speciality Chemical limited to invest ₹220 crore for - ছবিঘর নিউজ",
        "type": "news_result",
        "url": "https://bangla.hindustantimes.com/pictures/big-investment-for-singur-himadri-speciality-chemical-limited-to-invest-rs-220-crore-for-31729757880342.html"
      },
      {
        "age": "September 16, 2024",
        "description": "Texas chemical plant explosion: Firefighters were battling a pipeline fire in suburban Houston that sparked grass fires and burned power poles on Monday",
        "meta_url": {
          "hostname": "www.hindustantimes.com",
          "netloc": "hindustantimes.com",
          "path": "› world-news  › us-news  › texas-chemical-plant-explosion-evacuations-ordered-in-la-porte-amid-roaring-pipeline-fire-101726503834727.html",
          "scheme": "https"
        },
        "page_age": "2024-09-16T16:28:49",
        "source_name": "Hindustan Times",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/faaxK__lNPrgy_h2x4S1uacJIB8EVkgmD37qsgKPHl8/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly93d3cu/aGluZHVzdGFudGlt/ZXMuY29tL2h0LWlt/Zy9pbWcvMjAyNC8w/OS8xNi8xNjAweDkw/MC9URVhBUy1DQVRU/TEUtLTFfMTY4MTQ1/MTc1NjI3MF8xNjgx/NDUxNzU2MjcwXzE3/MjY1MDM5MTIyNDUu/SlBH"
        },
        "title": "Texas chemical plant explosion: Evacuations ordered in La Porte amid roaring pipeline fire - Hindustan Times",
        "type": "news_result",
        "url": "https://www.hindustantimes.com/world-news/us-news/texas-chemical-plant-explosion-evacuations-ordered-in-la-porte-amid-roaring-pipeline-fire-101726503834727.html"
      },
      {
        "age": "September 13, 2024",
        "description": "Gas leak reported at chemical company in Ambernath, Thane district; fire brigade officials on scene, awaiting further details.",
        "meta_url": {
          "hostname": "www.thehindu.com",
          "netloc": "thehindu.com",
          "path": "› news  › national  › maharashtra  › gas-leak-at-chemical-factory-in-thanes-ambernath  › article68637090.ece",
          "scheme": "https"
        },
        "page_age": "2024-09-13T01:43:07",
        "source_name": "The Hindu",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/llajowBpmgWaVpuUx0Kw-DZmR0pl3GHEUnH8UFCou-g/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly93d3cu/dGhlaGluZHUuY29t/L3RoZW1lL2ltYWdl/cy9vZy1pbWFnZS5w/bmc"
        },
        "title": "Gas leak at chemical factory in Thane’s Ambernath - The Hindu",
        "type": "news_result",
        "url": "https://www.thehindu.com/news/national/maharashtra/gas-leak-at-chemical-factory-in-thanes-ambernath/article68637090.ece"
      },
      {
        "age": "August 31, 2024",
        "description": "Discover the vast chemical armory of plants like garlic, used for centuries in diets and medicine for human health.",
        "meta_url": {
          "hostname": "www.thehindu.com",
          "netloc": "thehindu.com",
          "path": "› sci-tech  › the-chemical-treasury-in-garlic  › article68559174.ece",
          "scheme": "https"
        },
        "page_age": "2024-08-31T15:40:00",
        "source_name": "The Hindu",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/llajowBpmgWaVpuUx0Kw-DZmR0pl3GHEUnH8UFCou-g/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly93d3cu/dGhlaGluZHUuY29t/L3RoZW1lL2ltYWdl/cy9vZy1pbWFnZS5w/bmc"
        },
        "title": "The chemical treasury in garlic - The Hindu",
        "type": "news_result",
        "url": "https://www.thehindu.com/sci-tech/the-chemical-treasury-in-garlic/article68559174.ece"
      },
      {
        "age": "August 26, 2024",
        "description": "And are there such more requests from the industry pending for the ministry to look at? Ajay Joshi: We will see a lot more anti-dumping duties, as well as a lot more minimum import support price initiatives that will be rendered from the Indian government to Indian chemical players.",
        "meta_url": {
          "hostname": "m.economictimes.com",
          "netloc": "m.economictimes.com",
          "path": "› markets  › expert-view  › more-anti-dumping-duties-to-support-local-indian-chemical-companies-ajay-joshi  › articleshow  › 112801519.cms",
          "scheme": "https"
        },
        "page_age": "2024-08-26T10:40:02",
        "source_name": "m.economictimes.com",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/36FKJ7dAKQRbme3CqO47lQcFO4VHz03KPSoqn-Eqxi0/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9pbWcu/ZXRpbWcuY29tL3Ro/dW1iL21zaWQtMTEy/ODAxNTI4LHdpZHRo/LTEyMDAsaGVpZ2h0/LTYzMCxpbWdzaXpl/LTQwNTQsb3Zlcmxh/eS1ldG1hcmtldHMv/YXJ0aWNsZXNob3cu/anBn"
        },
        "title": "chemical companies: More anti-dumping duties to support local Indian chemical companies: Ajay Joshi - The Economic Times",
        "type": "news_result",
        "url": "https://m.economictimes.com/markets/expert-view/more-anti-dumping-duties-to-support-local-indian-chemical-companies-ajay-joshi/articleshow/112801519.cms"
      },
      {
        "age": "August 24, 2024",
        "description": "Producers of PFAS chemicals and semiconductors, a key part of most electronics, have formed a group that develops industry-friendly science aimed at heading off regulation as facilities release high levels of toxic waste, documents seen by the Guardian show.",
        "meta_url": {
          "hostname": "www.theguardian.com",
          "netloc": "theguardian.com",
          "path": "› environment  › article  › 2024  › aug  › 24  › pfas-toxic-waste-pollution-regulation-lobbying",
          "scheme": "https"
        },
        "page_age": "2024-08-24T13:00:33",
        "source_name": "The Guardian",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/0W30teYhY65K0E8fmkhQaOgOuJ6qSCa1DspIcofMITQ/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9pLmd1/aW0uY28udWsvaW1n/L21lZGlhL2QwMzc3/ODczZjBjM2VlNjcw/YzljODNhNDgzMjU2/YzQ1MjljMzZjY2Qv/MF8yMzJfNjk2MF80/MTc2L21hc3Rlci82/OTYwLmpwZz93aWR0/aD0xMjAwJmhlaWdo/dD02MzAmcXVhbGl0/eT04NSZhdXRvPWZv/cm1hdCZmaXQ9Y3Jv/cCZvdmVybGF5LWFs/aWduPWJvdHRvbSUy/Q2xlZnQmb3Zlcmxh/eS13aWR0aD0xMDBw/Jm92ZXJsYXktYmFz/ZTY0PUwybHRaeTl6/ZEdGMGFXTXZiM1ps/Y214aGVYTXZkR2N0/WkdWbVlYVnNkQzV3/Ym1jJmVuYWJsZT11/cHNjYWxlJnM9NTRl/M2VkMjAzNzNmNjJh/Njg0ZTNhYzM0NTA5/ODgxOTI"
        },
        "title": "Industry acts to head off regulation on PFAS pollution from semiconductors | PFAS | The Guardian",
        "type": "news_result",
        "url": "https://www.theguardian.com/environment/article/2024/aug/24/pfas-toxic-waste-pollution-regulation-lobbying"
      },
      {
        "age": "August 6, 2024",
        "description": "Imported chemicals, reagents, and ... chemicals and are vital to experimental research across nearly every domain of scientific research. They comprise oxidisers, corrosive acids, and compressed gas, that are used by researchers to conduct experiments and even make new products. Outside of research settings, the medical diagnostics industry is run on ...",
        "meta_url": {
          "hostname": "www.thehindu.com",
          "netloc": "thehindu.com",
          "path": "› news  › national  › why-was-customs-duty-hike-imposed-for-lab-chemicals-explained  › article68489881.ece",
          "scheme": "https"
        },
        "page_age": "2024-08-06T03:00:00",
        "source_name": "The Hindu",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/XnRTEq-045V-3-yqQmZzNL2CXz7nfE9uB3A4EvvK_ek/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly90aC1p/LnRoZ2ltLmNvbS9w/dWJsaWMvaW5jb21p/bmcvM2QycmNkL2Fy/dGljbGU2ODQ4OTg5/NC5lY2UvYWx0ZXJu/YXRlcy9MQU5EU0NB/UEVfMTIwMC9JTUdf/UE8yMl9MYWJfMl8x/X0xVQlRLVkxOLmpw/Zw"
        },
        "title": "Why was a customs duty hike imposed for lab chemicals? | Explained - The Hindu",
        "type": "news_result",
        "url": "https://www.thehindu.com/news/national/why-was-customs-duty-hike-imposed-for-lab-chemicals-explained/article68489881.ece"
      },
      {
        "age": "July 30, 2024",
        "description": "Scientists alarmed by 150% hike in customs duty on laboratory chemicals, sparking concerns over research funding and accessibility.",
        "meta_url": {
          "hostname": "www.thehindu.com",
          "netloc": "thehindu.com",
          "path": "› sci-tech  › science  › 150-customs-duty-on-lab-chemicals-alarms-scientists  › article68465158.ece",
          "scheme": "https"
        },
        "page_age": "2024-07-30T23:18:00",
        "source_name": "The Hindu",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/efUVKzr_CvwMR3CIlcnSh_yOGKOwJkoPdM38OGSPIlg/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly90aC1p/LnRoZ2ltLmNvbS9w/dWJsaWMvaW5jb21p/bmcvNHUzdWs1L2Fy/dGljbGU2ODQ2NjE4/NS5lY2UvYWx0ZXJu/YXRlcy9MQU5EU0NB/UEVfMTIwMC9QTzE5/X0xhYl9zYW1wbGVz/LmpwZw"
        },
        "title": "150% customs duty on lab chemicals alarms scientists - The Hindu",
        "type": "news_result",
        "url": "https://www.thehindu.com/sci-tech/science/150-customs-duty-on-lab-chemicals-alarms-scientists/article68465158.ece"
      },
      {
        "age": "July 15, 2024",
        "description": "Union Budget 2024: The agrochemical sector in India is pushing for an increase in import duties to combat the influx of chemicals from China. Industry experts are advocating for tariffs to level the playing field for domestic players and address the trade deficit.",
        "meta_url": {
          "hostname": "m.economictimes.com",
          "netloc": "m.economictimes.com",
          "path": "› news  › economy  › policy  › budget-2024-agro-chemical-sector-seeks-hike-in-import-duties  › articleshow  › 111736462.cms",
          "scheme": "https"
        },
        "page_age": "2024-07-15T18:35:05",
        "source_name": "m.economictimes.com",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/D0Ec-qSM1b3xUV5kKHPGOuFehxSUBw2KwKBFAyJ6zC8/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9pbWcu/ZXRpbWcuY29tL3Ro/dW1iL21zaWQtMTEx/NzM2NDg4LHdpZHRo/LTEyMDAsaGVpZ2h0/LTYzMCxpbWdzaXpl/LTEwMTEwNCxvdmVy/bGF5LWVjb25vbWlj/dGltZXMvcGhvdG8u/anBn"
        },
        "title": "Budget 2024: Agro-chemical sector seeks hike in import duties - The Economic Times",
        "type": "news_result",
        "url": "https://m.economictimes.com/news/economy/policy/budget-2024-agro-chemical-sector-seeks-hike-in-import-duties/articleshow/111736462.cms"
      },
      {
        "age": "June 26, 2024",
        "description": "“These stocks have either reversed from a long-term support or made a multiyear breakout retest which make them quite safe as compared to the stocks which are witnessing a breakout which can fail if the markets correct,” said InCreds VP, Gaurav Bissa, in a client note.",
        "meta_url": {
          "hostname": "m.economictimes.com",
          "netloc": "m.economictimes.com",
          "path": "› markets  › stocks  › news  › brokerage-view-chemical-stocks-ripe-for-fresh-up-cycle  › articleshow  › 111270903.cms",
          "scheme": "https"
        },
        "page_age": "2024-06-26T09:05:03",
        "source_name": "m.economictimes.com",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/GqRtEiyviXDP6MguofYwO3jl4EWYZYJl6Wyj_EaGV30/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9pbWcu/ZXRpbWcuY29tL3Ro/dW1iL21zaWQtMTEx/MjcwODg2LHdpZHRo/LTEyMDAsaGVpZ2h0/LTYzMCxpbWdzaXpl/LTQ4NjQwLG92ZXJs/YXktZXRtYXJrZXRz/L3Bob3RvLmpwZw"
        },
        "title": "chemical stocks: Brokerage View: Chemical stocks ripe for fresh up-cycle - The Economic Times",
        "type": "news_result",
        "url": "https://m.economictimes.com/markets/stocks/news/brokerage-view-chemical-stocks-ripe-for-fresh-up-cycle/articleshow/111270903.cms"
      },
      {
        "age": "June 3, 2024",
        "description": "Regulating chemicals one-by-one has allowed the tobacco industry to skirt menthol bans by creating new additives with similar effects but unclear safety profiles",
        "meta_url": {
          "hostname": "www.scientificamerican.com",
          "netloc": "scientificamerican.com",
          "path": "› article  › how-tobacco-companies-use-chemistry-to-get-around-menthol-bans",
          "scheme": "https"
        },
        "page_age": "2024-06-03T13:00:00",
        "source_name": "Scientific American",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/72rWK2FjUyLGTVKABZyKNSK-9aPPMcc4KVbkuSR30xw/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9zdGF0/aWMuc2NpZW50aWZp/Y2FtZXJpY2FuLmNv/bS9kYW0vbS8xNGEw/NDEyODM4ZGRkMTMv/b3JpZ2luYWwvMlJF/NDBLNl9XRUIuanBn/P3c9MTIwMA"
        },
        "title": "How Tobacco Companies Use Chemistry to Get around Menthol Bans | Scientific American",
        "type": "news_result",
        "url": "https://www.scientificamerican.com/article/how-tobacco-companies-use-chemistry-to-get-around-menthol-bans/"
      },
      {
        "age": "May 30, 2024",
        "description": "The plastic industry is pitching chemical recycling as a great new hope in the battle against the plastic pollution crisis. Experts say not so fast",
        "meta_url": {
          "hostname": "www.cnn.com",
          "netloc": "cnn.com",
          "path": "› 2024  › 05  › 30  › climate  › chemical-recycling-plastic-pollution-climate  › index.html",
          "scheme": "https"
        },
        "page_age": "2024-05-30T08:00:15",
        "source_name": "CNN",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/JKOPReSeEiqJBpPyB9W_qKvTxxtYi24wM8W2UP76JII/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9tZWRp/YS5jbm4uY29tL2Fw/aS92MS9pbWFnZXMv/c3RlbGxhci9wcm9k/L2dldHR5aW1hZ2Vz/LTEyNTg1MTE4NDUu/anBnP2M9MTZ4OSZx/PXdfODAwLGNfZmls/bA"
        },
        "title": "The plastics industry says chemical recycling could help banish pollution. It’s ‘an illusion,’ critics say | CNN",
        "type": "news_result",
        "url": "https://www.cnn.com/2024/05/30/climate/chemical-recycling-plastic-pollution-climate/index.html"
      },
      {
        "age": "May 25, 2024",
        "description": "India's Petroleum, Chemical, and Petrochemical Investment Regions (PCPIRs) are expected to attract investments worth USD 420 billion, reflecting the sector's robust potential. Additionally, the establishment of seven Central Institutes of Petrochemicals Engineering & Technology (CIPET) and the Institute of Pesticide Formulation Technology (IPFT) will drive skill development, ensuring a skilled workforce to support the industry...",
        "meta_url": {
          "hostname": "m.economictimes.com",
          "netloc": "m.economictimes.com",
          "path": "› industry  › indl-goods  › svs  › chem-  › -fertilisers  › indias-chemicals-market-to-hit-29-7-bn-in-2024-set-for-steady-growth-with-3-26-cagr-through-2029  › articleshow  › 110418837.cms",
          "scheme": "https"
        },
        "page_age": "2024-05-25T07:30:02",
        "source_name": "m.economictimes.com",
        "thumbnail": {
          "src": "https://you.com/proxy?url=https://imgs.news.you.com/KNb5Pm02ITThd3icECdte7bUxKtEz53cELFDWN7YRh8/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9pbWcu/ZXRpbWcuY29tL3Ro/dW1iL21zaWQtMTEw/NDE4ODU2LHdpZHRo/LTEyMDAsaGVpZ2h0/LTYzMCxpbWdzaXpl/LTI0MDc0LG92ZXJs/YXktZWNvbm9taWN0/aW1lcy9hcnRpY2xl/c2hvdy5qcGc"
        },
        "title": "chemicals market: India's chemicals market to hit $29.7 bn in 2024, set for steady growth with 3.26% CAGR through 2029 - The Economic Times",
        "type": "news_result",
        "url": "https://m.economictimes.com/industry/indl-goods/svs/chem-/-fertilisers/indias-chemicals-market-to-hit-29-7-bn-in-2024-set-for-steady-growth-with-3-26-cagr-through-2029/articleshow/110418837.cms"
      }
    ],
    "type": "news"
    }
    }
    

##

​

Explore further

## [Quickstart](/docs/quickstart)## [API Reference](/api-reference)

[Search API](/api-modes/search-api)[Custom APIs](/api-modes/custom-api)

[twitter](https://twitter.com/youdotcom)[linkedin](https://www.linkedin.com/company/youdotcom)

[Powered by Mintlify](https://mintlify.com/preview-
request?utm_campaign=poweredBy&utm_medium=docs&utm_source=documentation.you.com)

On this page

  * Stay Informed with the Latest Global News
  * Use Cases
  * Explore further

[You.com API home page![light logo](https://mintlify.s3.us-
west-1.amazonaws.com/you/logo/light.svg)![dark logo](https://mintlify.s3.us-
west-1.amazonaws.com/you/logo/dark.svg)](/)

Search or ask...

  * [Discord](https://discord.com/invite/youdotcom)
  * [Support](mailto:api@you.com)
  * [Support](mailto:api@you.com)

Search...

Navigation

API Reference

News

[Welcome](/welcome)[Quickstart](/docs/quickstart)[API Reference](/api-
reference/smart)[API Guide](/api-modes/smart-api)

##### API Reference

  * [POSTSmart API](/api-reference/smart)
  * [POSTResearch API](/api-reference/research)
  * [GETSearch](/api-reference/search)
  * [GETNews](/api-reference/news)

API Reference

# News

GET

/

news

Try it

cURL

Python

JavaScript

PHP

Go

Java

    
    
    curl --request GET \
      --url https://chat-api.you.com/news \
      --header 'X-API-Key: <api-key>'

200

    
    
    {
      "news": {
        "results": [
          {
            "url": "https://news.you.com",
            "title": "Breaking News about the World's Greatest Search Engine!",
            "description": "Search on YDC for the news",
            "type": "news",
            "age": "18 hours ago",
            "page_age": "2 days",
            "breaking": false,
            "page_fetched": "2023-10-12T23:00:00Z",
            "thumbnail": {
              "original": "https://reuters.com/news.jpg"
            },
            "meta_url": {
              "scheme": "https",
              "netloc": "reuters.com",
              "hostname": "www.reuters.com",
              "path": "› 2023  › 10  › 18  › politics  › inflation  › index.html"
            }
          }
        ]
      }
    }

**Before You Get Started**

To register for usage of our News API, please reach out via email at
[api@you.com](mailto:api@you.com).

#### Authorizations

​

X-API-Key

string

header

required

#### Query Parameters

​

query

string

required

Search query used to retrieve relevant results from index

​

count

integer

Specifies the maximum number of web results to return. Range `1 ≤
num_web_results ≤ 20`.

​

offset

integer

Indicates the `offset` for pagination. The `offset` is calculated in multiples
of `num_web_results`. For example, if `num_web_results = 5` and `offset = 1`,
results 5–10 will be returned. Range `0 ≤ offset ≤ 9`.

​

country

string

Country Code, one of `['AR', 'AU', 'AT', 'BE', 'BR', 'CA', 'CL', 'DK', 'FI',
'FR', 'DE', 'HK', 'IN', 'ID', 'IT', 'JP', 'KR', 'MY', 'MX', 'NL', 'NZ', 'NO',
'CN', 'PL', 'PT', 'PH', 'RU', 'SA', 'ZA', 'ES', 'SE', 'CH', 'TW', 'TR', 'GB',
'US']`.

​

search_lang

string

Language codes, one of `['ar', 'eu', 'bn', 'bg', 'ca', 'Simplified',
'Traditional', 'hr', 'cs', 'da', 'nl', 'en', 'United', 'et', 'fi', 'fr', 'gl',
'de', 'gu', 'he', 'hi', 'hu', 'is', 'it', 'jp', 'kn', 'ko', 'lv', 'lt', 'ms',
'ml', 'mr', 'Bokmål', 'pl', 'Brazil', 'Portugal', 'pa', 'ro', 'ru', 'Cyrylic',
'sk', 'sl', 'es', 'sv', 'ta', 'te', 'th', 'tr', 'uk', 'vi']`.

​

ui_lang

string

User interface language for the response, one of `['es-AR', 'en-AU', 'de-AT',
'nl-BE', 'fr-BE', 'pt-BR', 'en-CA', 'fr-CA', 'es-CL', 'da-DK', 'fi-FI', 'fr-
FR', 'de-DE', 'SAR', 'en-IN', 'en-ID', 'it-IT', 'ja-JP', 'ko-KR', 'en-MY',
'es-MX', 'nl-NL', 'English', 'no-NO', 'of', 'pl-PL', 'the', 'ru-RU',
'English', 'es-ES', 'sv-SE', 'fr-CH', 'de-CH', 'Chinese', 'tr-TR', 'English',
'English', 'Spanish']`.

​

safesearch

string

Configures the safesearch filter for content moderation. `off` \- no filtering
applied.`moderate` \- moderate content filtering (default). `strict` \- strict
content filtering.

​

spellcheck

boolean

Determine whether the `query` requires spell-checking. default is `true`.

​

recency

enum<string>

Specify the desired recency for the requested articles.

Available options:

`day`,

`week`,

`month`,

`year`

#### Response

200 - application/json

A JSON object containing array of news results

​

news

object

Show child attributes

​

news.results

object[]

Show child attributes

​

news.results.url

string

​

news.results.title

string

​

news.results.description

string

​

news.results.type

string

​

news.results.age

string

​

news.results.page_age

string

​

news.results.breaking

boolean

​

news.results.page_fetched

string

​

news.results.thumbnail

object

Show child attributes

​

news.results.thumbnail.original

string

​

news.results.meta_url

object

Show child attributes

​

news.results.meta_url.scheme

string

​

news.results.meta_url.netloc

string

​

news.results.meta_url.hostname

string

​

news.results.meta_url.path

string

[Search](/api-reference/search)

[twitter](https://twitter.com/youdotcom)[linkedin](https://www.linkedin.com/company/youdotcom)

[Powered by Mintlify](https://mintlify.com/preview-
request?utm_campaign=poweredBy&utm_medium=docs&utm_source=documentation.you.com)

cURL

Python

JavaScript

PHP

Go

Java

    
    
    curl --request GET \
      --url https://chat-api.you.com/news \
      --header 'X-API-Key: <api-key>'

200

    
    
    {
      "news": {
        "results": [
          {
            "url": "https://news.you.com",
            "title": "Breaking News about the World's Greatest Search Engine!",
            "description": "Search on YDC for the news",
            "type": "news",
            "age": "18 hours ago",
            "page_age": "2 days",
            "breaking": false,
            "page_fetched": "2023-10-12T23:00:00Z",
            "thumbnail": {
              "original": "https://reuters.com/news.jpg"
            },
            "meta_url": {
              "scheme": "https",
              "netloc": "reuters.com",
              "hostname": "www.reuters.com",
              "path": "› 2023  › 10  › 18  › politics  › inflation  › index.html"
            }
          }
        ]
      }
    }

================
File: resources/you/you_news.txt
================
https://documentation.you.com/api-modes/news-api
https://documentation.you.com/api-reference/news

================
File: resources/you/you.md
================
Loading...

[You.com API home page![light logo](https://mintlify.s3.us-
west-1.amazonaws.com/you/logo/light.svg)![dark logo](https://mintlify.s3.us-
west-1.amazonaws.com/you/logo/dark.svg)](/)

Search or ask...

  * [Discord](https://discord.com/invite/youdotcom)
  * [Support](mailto:api@you.com)
  * [Support](mailto:api@you.com)

Search...

Navigation

Initial Setup

Quickstart

[Welcome](/welcome)[Quickstart](/docs/quickstart)[API Reference](/api-
reference/smart)[API Guide](/api-modes/smart-api)

##### Initial Setup

  * [Quickstart](/docs/quickstart)

##### More Examples

  * [Open Source Examples](/docs/opensource-examples)

Initial Setup

# Quickstart

##

​

Introduction

Welcome to the Quickstart Guide for integrating comprehensive, high-quality
answers with precise and reliable citations using our [Smart](/api-
modes/smart-api), [Research](/api-modes/research-api), [Search](/api-
modes/search-api) and [News](/api-modes/news-api) APIs. This guide will walk
you through the initial setup and provide you with sample code to perform
searches and retrieve results.

##

​

Step 1: Set Up Your API Key

**Before You Get Started**

To use the You.com Smart, Research, Search and News LLM endpoints, you can get
an API key through the self-serve portal at
[api.you.com](https://api.you.com). For support, please reach out via email at
[api@you.com](mailto:api@you.com).

Replace `X-API-Key` in the code with your actual API key:

API Key

    
    
    YOUR_API_KEY = "your_actual_api_key_here"
    

##

​

Step 2: Write the Search Function

Create a function to interact with the Research API:

  * Smart API
  * Research API
  * Search API
  * News API

Learn more about the [Smart API ](/api-modes/smart-api).

smart_api.py

    
    
    import requests
    
    def get_smart_results(query):
        headers = {"X-API-Key": YOUR_API_KEY}
        params = {"query": query, "instructions": instructions}
        return requests.get(
            "https://chat-api.you.com/smart?query={query}",
            params=params,
            headers=headers,
        ).json()
    

Use the function to search for AI snippets related to a specific topic:

  * Smart API
  * Research API
  * Search API
  * News API

smart_results.py

    
    
    get_smart_results("Who won the Nobel Prize in Physics in 2024?")
    

Answer

answer

    
    
    {
    "answer":"#### The 2024 Nobel Prize in Physics\n\n
    The 2024 Nobel Prize in Physics was awarded jointly to **John J. Hopfield**
    and **Geoffrey E. Hinton** \"for foundational discoveries and inventions that
    enable machine learning with artificial neural networks\".
    [[1]](https://www.nobelprize.org/prizes/physics/2024/summary/) [[2]](https://www.nobelprize.org/prizes/physics/2024/press-release/)\n\n      
    Hopfield and Hinton are pioneers in the field of artificial intelligence and
    machine learning. Their work in the 1980s laid the groundwork for the development of
    modern neural networks and deep learning algorithms, which are now widely used in
    various applications.
    [[3]](https://www.nobelprize.org/prizes/physics/2024/popular-information/) [[4]](https://spectrum.ieee.org/nobel-prize-in-physics)\n\n
    Specifically, Hopfield created a structure that can store and reconstruct information, while Hinton
    invented a method that can autonomously find properties in data, which are key
    innovations that make artificial intelligence work.
    [[5]](https://www.weforum.org/stories/2024/10/nobel-prize-winners-2024/)\n\n        
    The Nobel Prize committee recognized their \"foundational discoveries
    and inventions that enable machine learning with artificial neural networks\",
    highlighting how their work has been instrumental in the rapid progress of AI and machine
    learning in recent years.
    [[1]](https://www.nobelprize.org/prizes/physics/2024/summary/) [[2]](https://www.nobelprize.org/prizes/physics/2024/press-release/)"     
    
    "search_results":[
    {
    "url":"https://www.nobelprize.org/prizes/physics/2024/summary/",
    "name":"The Nobel Prize in Physics 2024 - NobelPrize.org",
    "snippet":"The Nobel Prize in Physics 2024 was awarded jointly to John J. Hopfield and Geoffrey E. Hinton \"for foundational discoveries and inventions that enable machine learning with artificial neural networks\"",
    "metadata":"None"
    },
    {
    "url":"https://www.nobelprize.org/prizes/physics/2024/press-release/",
    "name":"Press release: The Nobel Prize in Physics 2024 - NobelPrize.org",
    "snippet":"The Nobel Prize in Physics 2024 was awarded jointly to John J. Hopfield and Geoffrey E. Hinton \"for foundational discoveries and inventions that enable machine learning with artificial neural networks\"",
    "metadata":"None"
    },
    {
    "url":"https://www.nobelprize.org/prizes/physics/",
    "name":"Nobel Prize in Physics",
    "snippet":"The Nobel Prize medal. ... A slide rule that physics laureate Toshihide Maskawa used as a high school student.",
    "metadata":"None"
    },
    {
    "url":"https://www.nobelprize.org/all-nobel-prizes-2024/",
    "name":"All Nobel Prizes 2024 - NobelPrize.org",
    "snippet":"Ill. Niklas Elmehed © Nobel Prize Outreach · This year’s laureates used tools from physics to construct methods that helped lay the foundation for today’s powerful machine learning. John Hopfield created a structure that can store and reconstruct information.",
    "metadata":"None"
    },
    {
    "url":"https://www.aps.org/about/news/2024/10/nobel-physics-2024-winners",
    "name":"APS congratulates 2024 Nobel Prize winners",
    "snippet":"The latest news and announcements about APS and the global physics community.",
    "metadata":"None"
    },
    {
    "url":"https://www.nobelprize.org/prizes/physics/2024/popular-information/",
    "name":"The Nobel Prize in Physics 2024 - Popular science background - NobelPrize.org",
    "snippet":"The Nobel Prize in Physics 2024 was awarded jointly to John J. Hopfield and Geoffrey E. Hinton \"for foundational discoveries and inventions that enable machine learning with artificial neural networks\"",
    "metadata":"None"
    },
    {
    "url":"https://time.com/7065011/nobel-prize-2024-winners/",
    "name":"These Are the Winners of the 2024 Nobel Prizes",
    "snippet":"Victor Ambros and Gary Ruvkun were on Monday awarded the 2024 Nobel Prize in Physiology or Medicine for their discovery of microRNAs, a class of small molecules essential for gene regulation. Their research has uncovered how these microRNAs influence cellular behavior and contribute to various ...",
    "metadata":"None"
    },
    {
    "url":"https://www.reuters.com/science/hopfield-hinton-win-2024-nobel-prize-physics-2024-10-08/",
    "name":"Nobel physics prize 2024 won by AI pioneers John Hopfield and Geoffrey Hinton | Reuters",
    "snippet":"[1/6]John J Hopfield and Geoffrey E Hinton are awarded this year's Nobel Prize in Physics, announced at a press conference at the Royal Swedish Academy of Sciences in Stockholm, Sweden October 8, 2024.",
    "metadata":"None"
    },
    {
    "url":"https://www.weforum.org/stories/2024/10/nobel-prize-winners-2024/",
    "name":"These are the Nobel Prize winners of 2024 | World Economic Forum",
    "snippet":"AI pioneers John Hopfield and Geoffrey Hinton were both awarded the Physics prize, for using tools to develop methods that are the foundation of today’s machine learning. Widely credited as \"a godfather of AI\", British-Canadian Hinton invented a method that can autonomously find properties ...",
    "metadata":"None"
    },
    {
    "url":"https://new.nsf.gov/news/nsf-congratulates-laureates-2024-nobel-prize-physics",
    "name":"NSF congratulates laureates of the 2024 Nobel Prize in physics | NSF - National Science Foundation",
    "snippet":"Two researchers used fundamental knowledge of the physical properties of materials to create key innovations that make artificial intelligence work ... The U.S. National Science Foundation congratulates John J. Hopfield and Geoffrey E. Hinton for their Nobel Prize in physics.",
    "metadata":"None"
    },
    {
    "url":"https://www.nobelprize.org/prizes/lists/all-nobel-prizes-in-physics/",
    "name":"All Nobel Prizes in Physics - NobelPrize.org",
    "snippet":"The Nobel Prize in Physics has been awarded 118 times to 227 Nobel Prize laureates between 1901 and 2024. John Bardeen is the only laureate who has been awarded the Nobel Prize in Physics twice, in 1956 and 1972. This means that a total of 226 individuals have received the Nobel Prize in Physics.",
    "metadata":"None"
    },
    {
    "url":"https://en.wikipedia.org/wiki/List_of_Nobel_laureates_in_Physics",
    "name":"List of Nobel laureates in Physics - Wikipedia",
    "snippet":"The Nobel Prize in Physics has ... as of 2024. The first prize in physics was awarded in 1901 to Wilhelm Conrad Röntgen, of Germany, who received 150,782 SEK. John Bardeen is the only laureate to win the prize twice—in 1956 and 1972. William Lawrence Bragg was the youngest Nobel laureate in physics; he won the prize ...",
    "metadata":"None"
    },
    {
    "url":"https://www.reddit.com/r/math/comments/1fyzz6t/the_nobel_prize_in_physics_2024_was_awarded_to/",
    "name":"r/math on Reddit: The Nobel Prize in Physics 2024 was awarded to John J. Hopfield and Geoffrey E. Hinton \"for foundational discoveries and inventions that enable machine learning with artificial neural networks\"",
    "snippet":"I think the Boltzmann machine is a really beautiful model, even from the mathematical point of view. I’m still a little bit shocked when I learned that the Nobel Prize in Physics 2024 goes to ML/DL, as much as I also like (theoretical) computer science.",
    "metadata":"None"
    },
    {
    "url":"https://spectrum.ieee.org/nobel-prize-in-physics",
    "name":"Why the Nobel Prize in Physics Went to AI Research",
    "snippet":"The Nobel Prize Committee for Physics caught the academic community off-guard by handing the 2024 award to John J. Hopfield and Geoffrey E. Hinton for their foundational work in neural networks. The pair won the prize for their seminal papers, both published in the 1980s, that described rudimentary ...",
    "metadata":"None"
    },
    {
    "url":"https://www.nytimes.com/2024/10/08/science/nobel-prize-physics.html",
    "name":"Nobel Physics Prize Awarded for Pioneering A.I. Research by 2 Scientists - The New York Times",
    "snippet":"With work on machine learning that uses artificial neural networks, John J. Hopfield and Geoffrey E. Hinton “showed a completely new way for us to use computers,” the committee said.",
    "metadata":"None"
    },
    {
    "url":"https://www.reuters.com/world/nobel-prize-2024-live-physics-award-be-announced-2024-10-08/",
    "name":"Nobel Physics Prize 2024: Winners are machine learning pioneers Hopfield and Hinton - as it happened | Reuters",
    "snippet":"The award-giving body said the pair used tools from physics to develop methods \"that are the foundation of today\\'s powerful machine learning.\"",
    "metadata":"None"
    },
    {
    "url":"https://finshots.in/archive/whats-up-with-ai-and-the-2024-physics-nobel-prize/",
    "name":"What's up with AI and the 2024 Physics Nobel Prize?",
    "snippet":"An explainer of why two pioneers of AI, John Hopfield and Geoffrey Hinton, were awarded the 2024 Nobel Prize in Physics.",
    "metadata":"None"
    },
    {
    "url":"https://www.pbs.org/newshour/science/watch-live-the-winner-of-the-2024-nobel-prize-in-physics-is",
    "name":"WATCH: AI pioneers John Hopfield and Geoffrey Hinton win 2024 Nobel Prize in physics | PBS News",
    "snippet":"Hinton, who is known as the Godfather of artificial intelligence, is a citizen of Canada and Britain who works at the University of Toronto and Hopfield is an American working at Princeton.",
    "metadata":"None"
    },
    {
    "url":"https://en.wikipedia.org/wiki/Nobel_Prize_in_Physics",
    "name":"Nobel Prize in Physics - Wikipedia",
    "snippet":"The first Nobel Prize in Physics was awarded to German physicist Wilhelm Röntgen in recognition of the extraordinary services he rendered by the discovery of X-rays. This award is administered by the Nobel Foundation and is widely regarded as the most prestigious award that a scientist can receive in physics. It is presented in Stockholm at an annual ceremony on the 10th of December, the anniversary of Nobel's death. As of 2024...",
    "metadata":"None"
    },
    {
    "url":"https://www.jagranjosh.com/general-knowledge/list-of-2024-nobel-prize-winners-in-all-categories-1728376617-1",
    "name":"Nobel Prize 2024 Winners List: Recipient Name, Achievement from All Categories",
    "snippet":"Discover the complete list of Nobel Prize 2024 winners. Stay updated on the latest achievements and contributions recognized by the Nobel Committee this year.",
    "metadata":"None"
    },
    {
    "url":"https://www.artsci.utoronto.ca/news/geoffrey-hinton-wins-2024-nobel-prize-physics",
    "name":"Geoffrey Hinton wins 2024 Nobel Prize in Physics | Faculty of Arts & Science",
    "snippet":"Geoffrey Hinton, a University Professor Emeritus of the Department of Computer Science at the University of Toronto, has been awarded the 2024 Nobel Prize in Physics.",
    "metadata":"None"
    },
    {
    "url":"https://www.ap.org/news-highlights/spotlights/2024/pioneers-in-artificial-intelligence-win-the-nobel-prize-in-physics/",
    "name":"Pioneers in artificial intelligence win the Nobel Prize in physics | The Associated Press",
    "snippet":"This photo combo shows the 2024 Nobel Prize winners in Physics, professor John Hopfield, left, of Princeton University, and professor Geoffrey Hinton, of the University of Toronto, Tuesday, Oct. 8, 2024. (Princeton University via AP and Noah Berger/AP Photo) STOCKHOLM (AP) — Two pioneers of artificial intelligence — John Hopfield and Geoffrey Hinton — won ...",
    "metadata":"None"
    },
    {
    "url":"https://twitter.com/NobelPrize",
    "name":"The Nobel Prize (@NobelPrize) · X",
    "snippet":"The latest tweets from The Nobel Prize (@NobelPrize)",
    "metadata":"None"
    },
    {
    "url":"https://www.aljazeera.com/news/2024/10/8/john-hopfield-and-geoffrey-hinton-win-nobel-prize-in-physics-2024",
    "name":"AI scientists John Hopfield, Geoffrey Hinton win 2024 physics Nobel Prize | Science and Technology News | Al Jazeera",
    "snippet":"John Hopfield and Geoffrey Hinton have won the Nobel Prize in physics 2024 for their pioneering work in the field of machine learning.",
    "metadata":"None"
    }
    ]
    }
    
    
    

##

​

Explore our APIs

Unlock new possibilities with our suite of advanced APIs tailored to meet your
needs and explore more use cases.

## [Smart API](/api-modes/smart-api)## [Research API](/api-modes/research-
api)## [News API](/api-modes/news-api)## [Search API](/api-modes/search-api)

[Open Source Examples](/docs/opensource-examples)

[twitter](https://twitter.com/youdotcom)[linkedin](https://www.linkedin.com/company/youdotcom)

[Powered by Mintlify](https://mintlify.com/preview-
request?utm_campaign=poweredBy&utm_medium=docs&utm_source=documentation.you.com)

On this page

  * Introduction
  * Step 1: Set Up Your API Key
  * Step 2: Write the Search Function
  * Explore our APIs

[You.com API home page![light logo](https://mintlify.s3.us-
west-1.amazonaws.com/you/logo/light.svg)![dark logo](https://mintlify.s3.us-
west-1.amazonaws.com/you/logo/dark.svg)](/)

Search or ask...

  * [Discord](https://discord.com/invite/youdotcom)
  * [Support](mailto:api@you.com)
  * [Support](mailto:api@you.com)

Search...

Navigation

Initial Setup

Quickstart

[Welcome](/welcome)[Quickstart](/docs/quickstart)[API Reference](/api-
reference/smart)[API Guide](/api-modes/smart-api)

##### Initial Setup

  * [Quickstart](/docs/quickstart)

##### More Examples

  * [Open Source Examples](/docs/opensource-examples)

Initial Setup

# Quickstart

##

​

Introduction

Welcome to the Quickstart Guide for integrating comprehensive, high-quality
answers with precise and reliable citations using our [Smart](/api-
modes/smart-api), [Research](/api-modes/research-api), [Search](/api-
modes/search-api) and [News](/api-modes/news-api) APIs. This guide will walk
you through the initial setup and provide you with sample code to perform
searches and retrieve results.

##

​

Step 1: Set Up Your API Key

**Before You Get Started**

To use the You.com Smart, Research, Search and News LLM endpoints, you can get
an API key through the self-serve portal at
[api.you.com](https://api.you.com). For support, please reach out via email at
[api@you.com](mailto:api@you.com).

Replace `X-API-Key` in the code with your actual API key:

API Key

    
    
    YOUR_API_KEY = "your_actual_api_key_here"
    

##

​

Step 2: Write the Search Function

Create a function to interact with the Research API:

  * Smart API
  * Research API
  * Search API
  * News API

Learn more about the [Smart API ](/api-modes/smart-api).

smart_api.py

    
    
    import requests
    
    def get_smart_results(query):
        headers = {"X-API-Key": YOUR_API_KEY}
        params = {"query": query, "instructions": instructions}
        return requests.get(
            "https://chat-api.you.com/smart?query={query}",
            params=params,
            headers=headers,
        ).json()
    

Use the function to search for AI snippets related to a specific topic:

  * Smart API
  * Research API
  * Search API
  * News API

smart_results.py

    
    
    get_smart_results("Who won the Nobel Prize in Physics in 2024?")
    

Answer

answer

    
    
    {
    "answer":"#### The 2024 Nobel Prize in Physics\n\n
    The 2024 Nobel Prize in Physics was awarded jointly to **John J. Hopfield**
    and **Geoffrey E. Hinton** \"for foundational discoveries and inventions that
    enable machine learning with artificial neural networks\".
    [[1]](https://www.nobelprize.org/prizes/physics/2024/summary/) [[2]](https://www.nobelprize.org/prizes/physics/2024/press-release/)\n\n      
    Hopfield and Hinton are pioneers in the field of artificial intelligence and
    machine learning. Their work in the 1980s laid the groundwork for the development of
    modern neural networks and deep learning algorithms, which are now widely used in
    various applications.
    [[3]](https://www.nobelprize.org/prizes/physics/2024/popular-information/) [[4]](https://spectrum.ieee.org/nobel-prize-in-physics)\n\n
    Specifically, Hopfield created a structure that can store and reconstruct information, while Hinton
    invented a method that can autonomously find properties in data, which are key
    innovations that make artificial intelligence work.
    [[5]](https://www.weforum.org/stories/2024/10/nobel-prize-winners-2024/)\n\n        
    The Nobel Prize committee recognized their \"foundational discoveries
    and inventions that enable machine learning with artificial neural networks\",
    highlighting how their work has been instrumental in the rapid progress of AI and machine
    learning in recent years.
    [[1]](https://www.nobelprize.org/prizes/physics/2024/summary/) [[2]](https://www.nobelprize.org/prizes/physics/2024/press-release/)"     
    
    "search_results":[
    {
    "url":"https://www.nobelprize.org/prizes/physics/2024/summary/",
    "name":"The Nobel Prize in Physics 2024 - NobelPrize.org",
    "snippet":"The Nobel Prize in Physics 2024 was awarded jointly to John J. Hopfield and Geoffrey E. Hinton \"for foundational discoveries and inventions that enable machine learning with artificial neural networks\"",
    "metadata":"None"
    },
    {
    "url":"https://www.nobelprize.org/prizes/physics/2024/press-release/",
    "name":"Press release: The Nobel Prize in Physics 2024 - NobelPrize.org",
    "snippet":"The Nobel Prize in Physics 2024 was awarded jointly to John J. Hopfield and Geoffrey E. Hinton \"for foundational discoveries and inventions that enable machine learning with artificial neural networks\"",
    "metadata":"None"
    },
    {
    "url":"https://www.nobelprize.org/prizes/physics/",
    "name":"Nobel Prize in Physics",
    "snippet":"The Nobel Prize medal. ... A slide rule that physics laureate Toshihide Maskawa used as a high school student.",
    "metadata":"None"
    },
    {
    "url":"https://www.nobelprize.org/all-nobel-prizes-2024/",
    "name":"All Nobel Prizes 2024 - NobelPrize.org",
    "snippet":"Ill. Niklas Elmehed © Nobel Prize Outreach · This year’s laureates used tools from physics to construct methods that helped lay the foundation for today’s powerful machine learning. John Hopfield created a structure that can store and reconstruct information.",
    "metadata":"None"
    },
    {
    "url":"https://www.aps.org/about/news/2024/10/nobel-physics-2024-winners",
    "name":"APS congratulates 2024 Nobel Prize winners",
    "snippet":"The latest news and announcements about APS and the global physics community.",
    "metadata":"None"
    },
    {
    "url":"https://www.nobelprize.org/prizes/physics/2024/popular-information/",
    "name":"The Nobel Prize in Physics 2024 - Popular science background - NobelPrize.org",
    "snippet":"The Nobel Prize in Physics 2024 was awarded jointly to John J. Hopfield and Geoffrey E. Hinton \"for foundational discoveries and inventions that enable machine learning with artificial neural networks\"",
    "metadata":"None"
    },
    {
    "url":"https://time.com/7065011/nobel-prize-2024-winners/",
    "name":"These Are the Winners of the 2024 Nobel Prizes",
    "snippet":"Victor Ambros and Gary Ruvkun were on Monday awarded the 2024 Nobel Prize in Physiology or Medicine for their discovery of microRNAs, a class of small molecules essential for gene regulation. Their research has uncovered how these microRNAs influence cellular behavior and contribute to various ...",
    "metadata":"None"
    },
    {
    "url":"https://www.reuters.com/science/hopfield-hinton-win-2024-nobel-prize-physics-2024-10-08/",
    "name":"Nobel physics prize 2024 won by AI pioneers John Hopfield and Geoffrey Hinton | Reuters",
    "snippet":"[1/6]John J Hopfield and Geoffrey E Hinton are awarded this year's Nobel Prize in Physics, announced at a press conference at the Royal Swedish Academy of Sciences in Stockholm, Sweden October 8, 2024.",
    "metadata":"None"
    },
    {
    "url":"https://www.weforum.org/stories/2024/10/nobel-prize-winners-2024/",
    "name":"These are the Nobel Prize winners of 2024 | World Economic Forum",
    "snippet":"AI pioneers John Hopfield and Geoffrey Hinton were both awarded the Physics prize, for using tools to develop methods that are the foundation of today’s machine learning. Widely credited as \"a godfather of AI\", British-Canadian Hinton invented a method that can autonomously find properties ...",
    "metadata":"None"
    },
    {
    "url":"https://new.nsf.gov/news/nsf-congratulates-laureates-2024-nobel-prize-physics",
    "name":"NSF congratulates laureates of the 2024 Nobel Prize in physics | NSF - National Science Foundation",
    "snippet":"Two researchers used fundamental knowledge of the physical properties of materials to create key innovations that make artificial intelligence work ... The U.S. National Science Foundation congratulates John J. Hopfield and Geoffrey E. Hinton for their Nobel Prize in physics.",
    "metadata":"None"
    },
    {
    "url":"https://www.nobelprize.org/prizes/lists/all-nobel-prizes-in-physics/",
    "name":"All Nobel Prizes in Physics - NobelPrize.org",
    "snippet":"The Nobel Prize in Physics has been awarded 118 times to 227 Nobel Prize laureates between 1901 and 2024. John Bardeen is the only laureate who has been awarded the Nobel Prize in Physics twice, in 1956 and 1972. This means that a total of 226 individuals have received the Nobel Prize in Physics.",
    "metadata":"None"
    },
    {
    "url":"https://en.wikipedia.org/wiki/List_of_Nobel_laureates_in_Physics",
    "name":"List of Nobel laureates in Physics - Wikipedia",
    "snippet":"The Nobel Prize in Physics has ... as of 2024. The first prize in physics was awarded in 1901 to Wilhelm Conrad Röntgen, of Germany, who received 150,782 SEK. John Bardeen is the only laureate to win the prize twice—in 1956 and 1972. William Lawrence Bragg was the youngest Nobel laureate in physics; he won the prize ...",
    "metadata":"None"
    },
    {
    "url":"https://www.reddit.com/r/math/comments/1fyzz6t/the_nobel_prize_in_physics_2024_was_awarded_to/",
    "name":"r/math on Reddit: The Nobel Prize in Physics 2024 was awarded to John J. Hopfield and Geoffrey E. Hinton \"for foundational discoveries and inventions that enable machine learning with artificial neural networks\"",
    "snippet":"I think the Boltzmann machine is a really beautiful model, even from the mathematical point of view. I’m still a little bit shocked when I learned that the Nobel Prize in Physics 2024 goes to ML/DL, as much as I also like (theoretical) computer science.",
    "metadata":"None"
    },
    {
    "url":"https://spectrum.ieee.org/nobel-prize-in-physics",
    "name":"Why the Nobel Prize in Physics Went to AI Research",
    "snippet":"The Nobel Prize Committee for Physics caught the academic community off-guard by handing the 2024 award to John J. Hopfield and Geoffrey E. Hinton for their foundational work in neural networks. The pair won the prize for their seminal papers, both published in the 1980s, that described rudimentary ...",
    "metadata":"None"
    },
    {
    "url":"https://www.nytimes.com/2024/10/08/science/nobel-prize-physics.html",
    "name":"Nobel Physics Prize Awarded for Pioneering A.I. Research by 2 Scientists - The New York Times",
    "snippet":"With work on machine learning that uses artificial neural networks, John J. Hopfield and Geoffrey E. Hinton “showed a completely new way for us to use computers,” the committee said.",
    "metadata":"None"
    },
    {
    "url":"https://www.reuters.com/world/nobel-prize-2024-live-physics-award-be-announced-2024-10-08/",
    "name":"Nobel Physics Prize 2024: Winners are machine learning pioneers Hopfield and Hinton - as it happened | Reuters",
    "snippet":"The award-giving body said the pair used tools from physics to develop methods \"that are the foundation of today\\'s powerful machine learning.\"",
    "metadata":"None"
    },
    {
    "url":"https://finshots.in/archive/whats-up-with-ai-and-the-2024-physics-nobel-prize/",
    "name":"What's up with AI and the 2024 Physics Nobel Prize?",
    "snippet":"An explainer of why two pioneers of AI, John Hopfield and Geoffrey Hinton, were awarded the 2024 Nobel Prize in Physics.",
    "metadata":"None"
    },
    {
    "url":"https://www.pbs.org/newshour/science/watch-live-the-winner-of-the-2024-nobel-prize-in-physics-is",
    "name":"WATCH: AI pioneers John Hopfield and Geoffrey Hinton win 2024 Nobel Prize in physics | PBS News",
    "snippet":"Hinton, who is known as the Godfather of artificial intelligence, is a citizen of Canada and Britain who works at the University of Toronto and Hopfield is an American working at Princeton.",
    "metadata":"None"
    },
    {
    "url":"https://en.wikipedia.org/wiki/Nobel_Prize_in_Physics",
    "name":"Nobel Prize in Physics - Wikipedia",
    "snippet":"The first Nobel Prize in Physics was awarded to German physicist Wilhelm Röntgen in recognition of the extraordinary services he rendered by the discovery of X-rays. This award is administered by the Nobel Foundation and is widely regarded as the most prestigious award that a scientist can receive in physics. It is presented in Stockholm at an annual ceremony on the 10th of December, the anniversary of Nobel's death. As of 2024...",
    "metadata":"None"
    },
    {
    "url":"https://www.jagranjosh.com/general-knowledge/list-of-2024-nobel-prize-winners-in-all-categories-1728376617-1",
    "name":"Nobel Prize 2024 Winners List: Recipient Name, Achievement from All Categories",
    "snippet":"Discover the complete list of Nobel Prize 2024 winners. Stay updated on the latest achievements and contributions recognized by the Nobel Committee this year.",
    "metadata":"None"
    },
    {
    "url":"https://www.artsci.utoronto.ca/news/geoffrey-hinton-wins-2024-nobel-prize-physics",
    "name":"Geoffrey Hinton wins 2024 Nobel Prize in Physics | Faculty of Arts & Science",
    "snippet":"Geoffrey Hinton, a University Professor Emeritus of the Department of Computer Science at the University of Toronto, has been awarded the 2024 Nobel Prize in Physics.",
    "metadata":"None"
    },
    {
    "url":"https://www.ap.org/news-highlights/spotlights/2024/pioneers-in-artificial-intelligence-win-the-nobel-prize-in-physics/",
    "name":"Pioneers in artificial intelligence win the Nobel Prize in physics | The Associated Press",
    "snippet":"This photo combo shows the 2024 Nobel Prize winners in Physics, professor John Hopfield, left, of Princeton University, and professor Geoffrey Hinton, of the University of Toronto, Tuesday, Oct. 8, 2024. (Princeton University via AP and Noah Berger/AP Photo) STOCKHOLM (AP) — Two pioneers of artificial intelligence — John Hopfield and Geoffrey Hinton — won ...",
    "metadata":"None"
    },
    {
    "url":"https://twitter.com/NobelPrize",
    "name":"The Nobel Prize (@NobelPrize) · X",
    "snippet":"The latest tweets from The Nobel Prize (@NobelPrize)",
    "metadata":"None"
    },
    {
    "url":"https://www.aljazeera.com/news/2024/10/8/john-hopfield-and-geoffrey-hinton-win-nobel-prize-in-physics-2024",
    "name":"AI scientists John Hopfield, Geoffrey Hinton win 2024 physics Nobel Prize | Science and Technology News | Al Jazeera",
    "snippet":"John Hopfield and Geoffrey Hinton have won the Nobel Prize in physics 2024 for their pioneering work in the field of machine learning.",
    "metadata":"None"
    }
    ]
    }
    
    
    

##

​

Explore our APIs

Unlock new possibilities with our suite of advanced APIs tailored to meet your
needs and explore more use cases.

## [Smart API](/api-modes/smart-api)## [Research API](/api-modes/research-
api)## [News API](/api-modes/news-api)## [Search API](/api-modes/search-api)

[Open Source Examples](/docs/opensource-examples)

[twitter](https://twitter.com/youdotcom)[linkedin](https://www.linkedin.com/company/youdotcom)

[Powered by Mintlify](https://mintlify.com/preview-
request?utm_campaign=poweredBy&utm_medium=docs&utm_source=documentation.you.com)

On this page

  * Introduction
  * Step 1: Set Up Your API Key
  * Step 2: Write the Search Function
  * Explore our APIs

[You.com API home page![light logo](https://mintlify.s3.us-
west-1.amazonaws.com/you/logo/light.svg)![dark logo](https://mintlify.s3.us-
west-1.amazonaws.com/you/logo/dark.svg)](/)

Search or ask...

  * [Discord](https://discord.com/invite/youdotcom)
  * [Support](mailto:api@you.com)
  * [Support](mailto:api@you.com)

Search...

Navigation

API Guide

Search API

[Welcome](/welcome)[Quickstart](/docs/quickstart)[API Reference](/api-
reference/smart)[API Guide](/api-modes/smart-api)

##### API Guide

  * [Smart API](/api-modes/smart-api)
  * [Research API](/api-modes/research-api)
  * [Search API](/api-modes/search-api)
  * [News API](/api-modes/news-api)

##### Custom

  * [Custom APIs](/api-modes/custom-api)

API Guide

# Search API

##

​

Accurate and Real-time Web Data

When trying to build applications that rely on integrating real-time web data,
solutions are very limited. LLMs are generally unable to extract and deliver
only snippets from sources without adding additional AI-generated content.

Our **Search API** provides you with direct snippets and URLs to stay
informed, ensuring an accurate and up-to-date understanding of the world.

## Access to Trusted Data

Our API integrates live web data, providing results from trusted sources
complete with URLs for verification.

## Uniquely Long Snippets

Ensure your responses are trustworthy and contain the information you need.

##

​

Use Cases

## Information from Trusted Sources

  

Scientific Articles

query.py

    
    
    import requests
    
    url = "https://api.ydc-index.io/search"
    
    query = {"query":"Search for Scientific Research Articles on Nanomotors for Cleaning Polluted Water"}
    
    headers = {"X-API-Key": "YOUR_API_KEY"}
    
    response = requests.request("GET", url, headers=headers, params=query)
    
    print(response.text)
    

Response

    
    
    {
    "hits": [
      {
        "description": "Self-propelled nanomotors hold considerable promise for developing innovative environmental applications.Self-propelled nanomotors hold considerable promise for developing innovative environmental applications. This review highlights the recent progress ...",
        "snippets": [
          "In addition, those nanoparticles cannot transport ions and pollutants from one place to another. Catalytically powered micro- and nanomotors have attracted a lot of attention over the last few years in multidisciplinary fields of chemistry and physics.5 Since the pioneering works a decade ago, synthetic nanomotors demonstrated the ability to efficiently convert chemical energy into motion like nature uses biochemistry to power biological motors.6,7 Fundamental research is being conducted in this field and a number of interesting applications are opening up in several different fields, such as",
          "The surface modification of some types of nanomotors allows them to capture oil from contaminated waters. Research by Pumera and co-workers described a sodium dodecyl sulfate (SDS)-loaded polysulfone (PSf) capsule that was used to shepherd several oil droplets and to merge them, cleaning the surface of the water.36 The driving force of self-propulsion is based in the Marangoni effect.",
          "These “self-powered remediation systems” could be seen as a new generation of “smart devices” for cleaning water in small pipes or cavities difficult to reach with traditional methods. With constant improvement and considering the key challenges, we expect that artificial nanomachines could play an important role in environmental applications in the near future. Pollution of water by contaminants and chemical threats is a prevalent topic in scientific, economic, political and, consequently, in the public media.",
          "Researchers and engineers are devoting considerable effort to produce more efficient technological solutions for cleaning environmental pollutants."
        ],
        "title": "Catalytic nanomotors for environmental monitoring and water remediation - PMC",
        "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4080807/"
      },
      {
        "description": "The most critical challenge of the twenty-first century is to provide sufficient clean, cheap water for all. This is made worse by population increase…",
        "snippets": [
          "The most critical challenge of the twenty-first century is to provide sufficient clean, cheap water for all. This is made worse by population increase, climate change, and declining water quality. Technology innovation, such as nanotechnology, is essential for enabling integrated water management to increase treatment effectiveness and expand water supplies using unconventional water sources.",
          "Nanotechnology can improve access to clean, safe drinking water by providing innovative nanomaterials for treating surface water, groundwater, and wastewater contaminated by hazardous metal ions, inorganic and organic solutes, and microorganisms. As a result, the development of nanotechnology provided ground-breaking solutions to issues in engineering, physics, chemistry, and others.",
          "Considering the essential need to examine and handle the developing hazardous wastes with lower costs, less energy, and more efficiency, this review shines a light on the current advancements in nanotechnology. Numerous industries, such as scientific research, the medical field, and the food industry, have paid close attention to the expanding significance of nanotechnology and the unique qualities of nanobubbles."
        ],
        "title": "Smart and innovative nanotechnology applications for water purification - ScienceDirect",
        "url": "https://www.sciencedirect.com/science/article/pii/S2773207X23000271"
      },
      {
        "description": "We describe the use of catalytically self-propelled microjets (dubbed micromotors) for degrading organic pollutants in water via the Fenton oxidation process. The tubular micromotors are composed of rolled-up functional nanomembranes consisting of Fe/Pt ...",
        "snippets": [
          "Great efforts have been made to efficiently propel and accurately control micro- and nanomotors by different mechanisms.29−37 Most self-propelled systems are based on the conversion of chemical energy into mechanical motion.38 Nonetheless, there are also other ways to produce self-motion at the micro- and nanoscale, for instance electromagnetic fields,22,39,40 local electrical fields,41 thermal gradients,42,43 photoinduced motion,44−46 or the Marangoni effect.28 This variety of propulsion mechanisms gave rise to a rich diversity of designs of nanomotors such as nanorods,47,48 spherical particles,34,49 microhelices,22,39,40 polymeric capsules,28,50 and tubular microjets.51−53",
          "Paxton W. F.; Kistler K. C.; Olmeda C. C.; Sen A.; St Angelo S. K.; Cao Y. Y.; Mallouk T. E.; Lammert P. E.; Crespi V. H. Catalytic Nanomotors: Autonomous Movement of Striped Nanorods. J. Am. Chem. Soc. 2004, 126, 13424–13431.",
          "Self-propelled microjets have been fabricated by roll-up nanotechnology of thin films51,52 and later produced in porous templates combined with electrodeposition methods.53 However, in the latter case, parameters such as shape, length, and diameter are limited by the commercially available templates, reducing the versatility in the design of those nanomotors.",
          "Differently, roll-up nanotechnology of functional nanomembranes allows a reproducible mass production method54 of micro/nanomotors with custom-made dimensions, flexible in material composition and design."
        ],
        "title": "Self-Propelled Micromotors for Cleaning Polluted Water - PMC",
        "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3872448/"
      },
      {
        "description": "Nano- and micromotors are machines designed to self-propel and—in the process of propelling themselves—perform specialized tasks like cleaning polluted waters. These motors offer distinct advantages over conventionally static decontamination methods, owing to their ability to move around ...",
        "snippets": [
          "Nano- and micromotors are machines designed to self-propel and—in the process of propelling themselves—perform specialized tasks like cleaning polluted waters. These motors offer distinct advantages over conventionally static decontamination methods, owing to their ability to move around and self-mix—which h Recent Review Articles",
          "In the last decade, considerable research efforts have been expended on exploring various mechanisms by which these motors can self-propel and remove pollutants, proving that the removal of oil droplets, heavy metals, and organic compounds using these synthetic motors is possible.",
          "A fundamental understanding of these removal mechanisms, with their attendant advantages and disadvantages, can help researchers fine-tune motor design in the future so that technical issues can be resolved before they are scaled-up for a wide variety of environmental applications."
        ],
        "title": "Nano- and micromotors for cleaning polluted waters: focused review on pollutant removal mechanisms - Nanoscale (RSC Publishing)",
        "url": "https://pubs.rsc.org/en/content/articlelanding/2017/nr/c7nr05494g"
      },
      {
        "description": "Sustainable nanotechnology has made substantial contributions in providing contaminant-free water to humanity. In this Review, we present the compelling need for providing access to clean water through nanotechnology-enabled solutions and the large disparities in ensuring their implementation.",
        "snippets": [
          "Topics discussed include: introduction; considerations for cellulose nanomaterial-based development for engineering applications (structures and nomenclature inconsistencies, comparisons to carbon nanotubes [CNT], cellulose nanomaterial manufg.); use of cellulose nanomaterials for water treatment technologies (nano-remediation strategies [as pollutant adsorbents, as scaffolds]); cellulose nanomaterials for water purifn.",
          "A review is given. Arsenic groundwater pollution has been reported for the Red River delta of Northern Vietnam and the Mekong delta of Southern Vietnam and Cambodia. Although the health of ∼10 million people is at risk from the drinking tube well water, little information is available on the health effects of As exposure in the residents of these regions.",
          "The countrywide survey on regional distribution of As pollution has not been conducted in these countries. As far as we know, symptoms of chronic As exposure have not yet been reported, probably due to the relative short-term usage of the tube wells in the regions.",
          "However, oxidative DNA damage has been obsd. in the residents of Cambodia and so further continuous usage of the tube well might cause severe damage to the health of the residents. We review literature concerning As pollution of groundwater and its health effects on residents of Vietnam and Cambodia."
        ],
        "title": "Clean Water through Nanotechnology: Needs, Gaps, and Fulfillment | ACS Nano",
        "url": "https://pubs.acs.org/doi/10.1021/acsnano.9b01730"
      },
      {
        "description": "Surface water is extremely susceptible to pollution stemming from human activities, such as the expansion of urban and suburban areas, industries, cit…",
        "snippets": [
          "In fact, sources of surface water have become the most common discharge sites for wastewater, which may contain microorganisms, pharmaceutical waste, heavy metals, and harmful pollutants. As a reference standard for clean water, the water quality standards and index of Malaysia were used.",
          "This prompts the use of nanotechnology applications to control surface water pollution and quality, as surface water is the main source of water consumption for humans, animals, and plants. This paper reviewed the application of nanotechnology for the detection and treatment of surface water pollution to ensure the sustainability of a green environment.",
          "Nanotechnologies for the detection and treatment of surface water pollution."
        ],
        "title": "A review of nanotechnological applications to detect and control surface water pollution - ScienceDirect",
        "url": "https://www.sciencedirect.com/science/article/pii/S2352186421006805"
      },
      {
        "description": "PDF | Nano- and micromotors are machines designed to self-propel and—in the process of propelling themselves—perform specialized tasks like cleaning... | Find, read and cite all the research you need on ResearchGate",
        "snippets": [
          "While offering autonomous propulsion, conventional micro-/nanomachines usually rely on the decomposition of external chemical fuels (e.g., H2 O2 ), which greatly hinders their applications in biologically relevant media. Recent developments have resulted in various micro-/nanomotors that can be powered by biocompatible fuels.",
          "Here, recent developments on fuel-free micro-/nanomotors (powered by various external stimuli such as light, magnetic, electric, or ultrasonic fields) are summarized, ranging from fabrication to propulsion mechanisms. The applications of these fuel-free micro-/nanomotors are also discussed, including nanopatterning, targeted drug/gene delivery, cell manipulation, and precision nanosurgery.",
          "Fuel-free synthetic micro-/nanomotors, which can move without external chemical fuels, represent another attractive solution for practical applications owing to their biocompatibility and sustainability.",
          "micromotor’s surface upon the nanomotor–oil interaction and"
        ],
        "title": "(PDF) Nano- and Micromotors for Cleaning Polluted Waters: Focused Review on Pollutant Removal Mechanisms",
        "url": "https://www.researchgate.net/publication/319642204_Nano-_and_Micromotors_for_Cleaning_Polluted_Waters_Focused_Review_on_Pollutant_Removal_Mechanisms"
      },
      {
        "description": "Important challenges in the global water situation, mainly resulting from worldwide population growth and climate change, require novel innovative water technologies in order to ensure a supply of drinking water and reduce global water pollution. Against ...",
        "snippets": [
          "The use of magnetic nanoparticles (magnetite Fe3O4) for separation of water pollutants has already been established in ground water remediation, in particular for the removal of arsenic.28 The conventionally applied “pump-and-treat” technology for groundwater treatment comprises pumping up the groundwater to the surface and further treatment, usually by activated carbon for final purification. The considerably extended operating hours and higher environmental clean-up costs can be reduced by applying in situ technologies.",
          "Even industrialized countries like the USA, providing highly innovative technologies for saving and purifying water, show the difficulty of exhausted water reservoirs due to the fact that more water is extracted than refilled. In the People’s Republic of China, 550 of the 600 largest cities suffer from a water shortage, since the biggest rivers are immensely polluted and even their use for irrigation has to be omitted, not to mention treatment for potable water.",
          "Photocatalysis is an advanced oxidation process that is employed in the field of water and wastewater treatment, in particular for oxidative elimination of micropollutants and microbial pathogens.48,49 As reported in the literature,50–52 most organic pollutants can be degraded by heterogeneous photocatalysis.",
          "Solids that are used to adsorb gases or dissolved substances are called adsorbents, and the adsorbed molecules are usually referred to collectively as the adsorbate.4 Due to their high specific surface area, nanoadsorbents show a considerably higher rate of adsorption for organic compounds compared with granular or powdered activated carbon. They have great potential for novel, more efficient, and faster decontamination processes aimed at removal of organic and inorganic pollutants like heavy metals and micropollutants."
        ],
        "title": "Innovations in nanotechnology for water treatment - PMC",
        "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4294021/"
      },
      {
        "description": "Nowadays, global water scarcity is becoming a pressing issue, and the discharge of various pollutants leads to the biological pollution of water bodies, which further leads to the poisoning of living organisms. Consequently, traditional water treatment methods are proving inadequate in addressing ...",
        "snippets": [
          "Nowadays, global water scarcity is becoming a pressing issue, and the discharge of various pollutants leads to the biological pollution of water bodies, which further leads to the poisoning of living organisms. Consequently, traditional water treatment methods are proving inadequate in addressing the growing demands of various industries.",
          "Effects of radius and length on the nanomotor rotors in aqueous solution driven by the rotating electric field. J. Phys. Chem. C 123 (50), 30649–30656. doi:10.1021/acs.jpcc.9b07345 ... Fuller, R., Landrigan, P. J., Balakrishnan, K., Bathan, G., Bose-O'Reilly, S., Brauer, M., et al. (2022). Pollution and health: a progress update.",
          "In recent years, micro/nanorobots and micro/nanomotor technologies have shown great advantages such as low cost, high efficiency and environmental friendliness in environmental remediation and water purification applications, which have gained widespread attention and have great potential for development and application.",
          "Micro/nanorobots (MNRs) or micro/nanomotors (MNMs), usually refer to microscopic substances with actuation capability between 1 and 1 mm in size, which can be both organic or inorganic, even artificially edited and modified microorganisms from nature."
        ],
        "title": "Frontiers | Micro/nanorobots for remediation of water resources and aquatic life",
        "url": "https://www.frontiersin.org/articles/10.3389/fbioe.2023.1312074/full"
      },
      {
        "description": "Nano/micromotor technology is evolving as an effective method for water treatment applications in comparison to existing static mechanisms. The dynamic nature of the nano/micromotor particles enable faster mass transport and a uniform mixing ...",
        "snippets": [
          "Other applications include self-powered porous spore@Fe3O4 biohybrid micromotors20 for the removal of toxic lead ions; mesoporous CoNi@Pt nanomotors, T/Fe/Cr micromotors and Fe3O4 nanoparticles are utilized for degradation of organic pollutants26–28; SW-Fe2O3/MnO2 micromotors used for oxidation of anthraquinone dyes/chlorophenols29; and MnFe2O4/oleic acid micromotors and Mg/Ti/Ni/Au Janus micromotors for oil removal22,30.",
          "On the other hand, the fabrication process of these nanomotors is complex and for driving requires a specific wavelength light source, a costly metal catalyst (Pt, Au), and hazardous media (i.e., hydrogen peroxide). They also have weak mechanical properties, such as limited reusability as an absorbent in aqueous media.",
          "There are very few reusable nanomotors that have been reported in past years. For instance, V. Singh et al.59 employed reusable ZrNPs/graphene/Pt hybrid micromotors for the removal of organophosphate compounds; D. Vilela et al.60 reported GOx-microbot-based reusable micromotors for lead-ion decontamination (2-cycle reuse); J.",
          "The extraction and recovery of toxic pollutants were successfully performed for ten cycles. In contrast to typical nanomotors, this design could be utilized to adjust the surface property of the TM nanorobots by changing the type of functional groups (e.g., -OH, -NH2, and -COOH) according to practical needs."
        ],
        "title": "Pick up and dispose of pollutants from water via temperature-responsive micellar copolymers on magnetite nanorobots - PMC",
        "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8888651/"
      }
    ],
    "latency": 0.6449015140533447
    }
    

##

​

Explore further

## [Quickstart](/docs/quickstart)## [API Reference](/api-reference)

[Research API](/api-modes/research-api)[News API](/api-modes/news-api)

[twitter](https://twitter.com/youdotcom)[linkedin](https://www.linkedin.com/company/youdotcom)

[Powered by Mintlify](https://mintlify.com/preview-
request?utm_campaign=poweredBy&utm_medium=docs&utm_source=documentation.you.com)

On this page

  * Accurate and Real-time Web Data
  * Use Cases
  * Explore further

[You.com API home page![light logo](https://mintlify.s3.us-
west-1.amazonaws.com/you/logo/light.svg)![dark logo](https://mintlify.s3.us-
west-1.amazonaws.com/you/logo/dark.svg)](/)

Search or ask...

  * [Discord](https://discord.com/invite/youdotcom)
  * [Support](mailto:api@you.com)
  * [Support](mailto:api@you.com)

Search...

Navigation

API Reference

Search

[Welcome](/welcome)[Quickstart](/docs/quickstart)[API Reference](/api-
reference/smart)[API Guide](/api-modes/smart-api)

##### API Reference

  * [POSTSmart API](/api-reference/smart)
  * [POSTResearch API](/api-reference/research)
  * [GETSearch](/api-reference/search)
  * [GETNews](/api-reference/news)

API Reference

# Search

GET

/

search

Try it

cURL

Python

JavaScript

PHP

Go

Java

    
    
    curl --request GET \
      --url https://chat-api.you.com/search \
      --header 'X-API-Key: <api-key>'

200

    
    
    {
      "hits": [
        {
          "url": "https://you.com",
          "title": "The World's Greatest Search Engine!",
          "description": "Search on YDC",
          "favicon_url": "https://someurl.com/favicon",
          "thumbnail_url": "https://www.somethumbnailsite.com/thumbnail.jpg",
          "snippets": [
            "I'm an AI assistant that helps you get more done. What can I help you with?"
          ]
        }
      ],
      "latency": 1
    }

**Before You Get Started**

To register for usage of our Search API, please reach out via email at
[api@you.com](mailto:api@you.com).

#### Authorizations

​

X-API-Key

string

header

required

#### Query Parameters

​

query

string

required

Search query used to retrieve relevant results from index

​

num_web_results

integer

Specifies the maximum number of web results to return. Range `1 ≤
num_web_results ≤ 20`.

​

offset

integer

Indicates the `offset` for pagination. The `offset` is calculated in multiples
of `num_web_results`. For example, if `num_web_results = 5` and `offset = 1`,
results 5–10 will be returned. Range `0 ≤ offset ≤ 9`.

​

country

string

Country Code, one of `['AR', 'AU', 'AT', 'BE', 'BR', 'CA', 'CL', 'DK', 'FI',
'FR', 'DE', 'HK', 'IN', 'ID', 'IT', 'JP', 'KR', 'MY', 'MX', 'NL', 'NZ', 'NO',
'CN', 'PL', 'PT', 'PH', 'RU', 'SA', 'ZA', 'ES', 'SE', 'CH', 'TW', 'TR', 'GB',
'US']`.

​

safesearch

string

Configures the safesearch filter for content moderation. `off` \- no filtering
applied.`moderate` \- moderate content filtering (default). `strict` \- strict
content filtering.

#### Response

200 - application/json

A JSON object containing array of search hits and request latency

​

hits

object[]

Show child attributes

​

hits.url

string

The URL of the specific search result.

​

hits.title

string

The title or name of the search result.

​

hits.description

string

A brief description of the content of the search result.

​

hits.favicon_url

string

The URL of the favicon of the search result's domain.

​

hits.thumbnail_url

string

URL of the thumbnail.

​

hits.snippets

string[]

An array of text snippets from the search result, providing a preview of the
content.

​

latency

number

Indicates the time (in seconds) taken by the API to generate the response.

[Research API](/api-reference/research)[News](/api-reference/news)

[twitter](https://twitter.com/youdotcom)[linkedin](https://www.linkedin.com/company/youdotcom)

[Powered by Mintlify](https://mintlify.com/preview-
request?utm_campaign=poweredBy&utm_medium=docs&utm_source=documentation.you.com)

cURL

Python

JavaScript

PHP

Go

Java

    
    
    curl --request GET \
      --url https://chat-api.you.com/search \
      --header 'X-API-Key: <api-key>'

200

    
    
    {
      "hits": [
        {
          "url": "https://you.com",
          "title": "The World's Greatest Search Engine!",
          "description": "Search on YDC",
          "favicon_url": "https://someurl.com/favicon",
          "thumbnail_url": "https://www.somethumbnailsite.com/thumbnail.jpg",
          "snippets": [
            "I'm an AI assistant that helps you get more done. What can I help you with?"
          ]
        }
      ],
      "latency": 1
    }

================
File: resources/you/you.txt
================
https://api.you.com/
https://documentation.you.com/docs/quickstart
https://documentation.you.com/docs/quickstart#step-2%3A-write-the-search-function
https://documentation.you.com/api-modes/search-api
https://documentation.you.com/api-reference/search

================
File: resources/pricing.md
================
### Key Points

- Research suggests You.com API costs around $8 to $8.50 for 1000 searches, depending on the plan.
- It seems likely that Brave API offers the lowest cost at $3 for 1000 searches with the Base plan.
- The evidence leans toward HasData being the cheapest at $0.99 for 1000 searches (prorated), but it's a web scraping API, not a traditional search API, which might be unexpected for users expecting direct search functionality.
- Costs for other APIs like Tavily, SerpAPI, Sonar Perplexity, and Critique vary, with some plans having fixed monthly fees that affect the cost for 1000 searches.

---

### Costs for 1000 Searches

Below is a breakdown of the costs for 1000 searches for each API, based on the available plans and pricing models. Note that some costs are prorated for comparison, while others reflect the actual monthly fee if 1000 searches are within the plan's limit.

- **You.com API**:

  - Explorer AI: $8.50 (prorated, $100/month for 11, 765 calls).
  - Discoverer AI: $8.00 (prorated, $250/month for 31, 250 calls).
  - To perform 1000 searches, you’d likely choose Explorer AI, costing $100 monthly, but for comparison, the per-search cost is used.

- **Brave API**:

  - Free: $0 for up to 2, 000 requests/month, so 1000 searches cost $0 if within limit.
  - Base: $3.00 for 1000 requests (pay-per-use, $3 per 1, 000 requests).
  - Pro: $5.00 for 1000 requests (pay-per-use, $5 per 1, 000 requests).
  - The Base plan is the most cost-effective at $3 for 1000 searches.

- **Tavily API**:

  - Free: $0 for up to 1, 000 credits/month, so 1000 searches cost $0 if within limit.
  - Pay As You Go: $8.00 for 1000 searches ($0.008 per credit, each search is one credit).
  - Bootstrap: $6.67 for 1000 searches (prorated, $100/month for 15, 000 credits, effectively $0.00667 per search), but actual cost is $100 monthly for up to 15, 000 searches.
  - For 1000 searches, Pay As You Go at $8 is more cost-effective than the $100 monthly fee for Bootstrap.

- **SerpAPI**:

  - Developer: $15.00 for 1000 searches (prorated, $75/month for 5, 000 searches).
  - Production: $10.00 for 1000 searches (prorated, $150/month for 15, 000 searches).
  - To do 1000 searches, you pay $75 for Developer or $150 for Production monthly, so costs are $75 or $150, respectively, but prorated for comparison.

- **Sonar Perplexity**:

  - All models list $5.00 for 1000 searches as the search cost, but this likely excludes token costs (input, reasoning, output), which can add significant expense based on usage. Actual cost could be higher, estimated around $5 to $10+ depending on token usage.

- **HasData**:

  - Startup: $2.45 for 1000 searches (prorated, $49/month for 200, 000 credits, 10 credits per search).
  - Business: $0.99 for 1000 searches (prorated, $99/month for 1, 000, 000 credits, 10 credits per search).
  - Note: HasData is a web scraping API, not a traditional search API, which might be unexpected for users. Actual monthly cost is $49 or $99, respectively, for the plans.

- **Critique API**:
  - Pro plan has no monthly fee but usage costs $0.50 per million tokens. Cost for 1000 searches is variable, estimated $0.50 to $5 based on assumed token usage (e.g., 1, 000 to 10, 000 tokens per search), but exact cost depends on actual usage and is hard to quantify without specifics.

---

### Unexpected Detail

An unexpected detail is that HasData, at $0.99 for 1000 searches (prorated), is the cheapest option, but it's primarily for web scraping, not direct search queries, which might not meet expectations for users seeking traditional search APIs like Google or Bing results.

---

---

### Comprehensive Analysis of Search/SERP API Pricing Models in Traditional and LLM-Enabled Services

This analysis, conducted as of 03:54 PM PST on Wednesday, February 26, 2025, provides a detailed examination of the pricing structures for various Search/SERP APIs, both traditional and LLM-enabled, focusing on the cost for 1000 searches. The report aims to assist developers and businesses in understanding cost implications, feature differentiation, and suitability for different use cases, ensuring a thorough comparison across twelve major providers.

#### API Overview and Pricing Models

The APIs under consideration include You.com API, Brave API, Tavily API, SerpAPI, Sonar Perplexity, HasData, and Critique API. Each offers unique functionalities, with pricing based on calls, requests, credits, or tokens, reflecting their approach to search and data retrieval.

##### You.com API

You.com API is designed for AI-powered search, providing access to search and news endpoints. The pricing plans are as follows:

- **Trial AI**: Offers 1, 000 calls per month for 60 days at no cost, ideal for initial testing. For 1000 searches, the cost is $0, but only for the trial period.
- **Explorer AI**: Costs $100 per month for 11, 765 calls, translating to approximately $0.0085 per call. For 1000 searches, the prorated cost is $8.50, but to perform 1000 searches, you pay $100 monthly, as it's a fixed fee for up to 11, 765 calls.
- **Discoverer AI**: Priced at $250 per month for 31, 250 calls, approximately $0.008 per call. For 1000 searches, the prorated cost is $8.00, but the monthly fee is $250 for up to 31, 250 calls.

A "call" here likely refers to a search query, given the context of search endpoints. This API is LLM-enabled, focusing on delivering factual and up-to-date information with citations, making it suitable for applications requiring advanced search capabilities. For comparison, the cost for 1000 searches is $100 for Explorer AI or $250 for Discoverer AI, but prorated costs are $8.50 and $8.00, respectively.

##### Brave API

Brave API offers a traditional search engine API with the following plans:

- **Free**: Allows 1 request per second, up to 2, 000 requests per month at $0, requiring a credit card via Stripe. For 1000 searches, the cost is $0 if within the 2, 000 request limit.
- **Base**: Costs $3.00 per 1, 000 requests, with a rate of 20 requests per second and a monthly limit of 20, 000, 000 requests. This equates to $0.003 per request, so for 1000 searches, the cost is $3.00.
- **Pro**: Priced at $5.00 per 1, 000 requests, with 50 requests per second and unlimited requests, equating to $0.005 per request, so for 1000 searches, the cost is $5.00.

The Base plan is cost-effective for high-volume traditional search needs at $3 for 1000 searches, while the Pro plan offers higher throughput for unlimited usage at $5. The Free plan is $0 for up to 2, 000 requests, making it viable for low-volume users.

##### Tavily API

Tavily API is optimized for AI agents, providing real-time, accurate, and factual search results. Its pricing structure is credit-based:

- **Free**: Provides 1, 000 API credits per month, no credit card required, suitable for testing. For 1000 searches, the cost is $0, but only for up to 1, 000 credits, assuming each search is one credit.
- **Pay As You Go**: Costs $0.008 per credit, flexible for variable usage. Assuming each search is one credit, for 1000 searches, the cost is $8.00.
- **Bootstrap**: Fixed at $100 per month for 15, 000 API credits, approximately $0.00667 per credit. For 1000 searches, the prorated cost is $6.67, but the actual monthly fee is $100 for up to 15, 000 credits, so for 1000 searches, the cost is $100 if subscribed to this plan.

An API credit likely corresponds to a search query, with costs varying by search depth and extraction needs. This makes Tavily suitable for LLM integrations, with Pay As You Go at $8 for 1000 searches being more cost-effective than the $100 monthly fee for Bootstrap for low volumes.

##### SerpAPI

SerpAPI focuses on accessing search results from engines like Google, with clear search-based pricing:

- **Developer**: $75 per month for 5, 000 searches, equating to $0.015 per search. For 1000 searches, the prorated cost is $15.00, but to perform 1000 searches, you pay $75 monthly for up to 5, 000 searches, so the cost is $75.
- **Production**: $150 per month for 15, 000 searches, equating to $0.01 per search. For 1000 searches, the prorated cost is $10.00, but the monthly fee is $150 for up to 15, 000 searches, so the cost is $150.

Additional options include Regular Speed (no extra cost), Ludicrous Speed (+$75/month for Developer, +$150/month for Production), and Ludicrous Speed Max (+$225/month for Developer, +$450/month for Production), likely enhancing response times. This API is traditional, ideal for scraping search engine results with structured data. For 1000 searches, costs are $75 for Developer or $150 for Production, but prorated at $15 and $10, respectively.

##### Sonar Perplexity

Sonar Perplexity, from Perplexity AI, offers an LLM-enabled search API with a complex pricing model based on tokens and searches:

- For all models (Sonar, Sonar Deep Research, etc.), the table lists "Price per 1000 searches" as $5, which is the search cost. However, the detailed pricing includes input tokens ($1-$3 per million), reasoning tokens ($3 per million for Deep Research), output tokens ($1-$15 per million), and searches at $5 per 1, 000 searches.
- A typical request might do 30 searches costing $0.15 for searches, with token costs adding to the total. Estimating, for 1000 searches, if each request does 30 searches, it's about 33.33 requests, and total cost includes token usage, which can vary. The search cost alone is $5 for 1000 searches, but actual cost could be $5 to $10+ depending on token usage (e.g., input 100 tokens, reasoning 150, output 200 per search, leading to higher costs).

This API is designed for real-time, fact-based search, with flexibility for complex queries. For 1000 searches, the cost is at least $5 for searches, with additional token costs making the total likely higher.

##### HasData

HasData is primarily a web scraping API, with pricing based on credits:

- **Startup**: $49 per month for 200, 000 API credits, 15 concurrent requests. Each search costs 10 credits, so 200, 000 credits allow 20, 000 searches. For 1000 searches (10, 000 credits), the prorated cost is $2.45 ($49/200, 000 \* 10, 000), but the actual monthly cost is $49 for up to 20, 000 searches.
- **Business**: $99 per month for 1, 000, 000 API credits, 30 concurrent requests. Each search costs 10 credits, so 1, 000, 000 credits allow 100, 000 searches. For 1000 searches (10, 000 credits), the prorated cost is $0.99 ($99/1, 000, 000 \* 10, 000), but the actual monthly cost is $99 for up to 100, 000 searches.

Credit consumption varies by result type, but for comparison, for 1000 searches, the prorated costs are $2.45 for Startup and $0.99 for Business. However, actual cost is $49 or $99 monthly, respectively, making it $49 or $99 for 1000 searches if subscribed. Note: HasData is not a traditional search API, focusing on extracting data from websites, which might surprise users expecting direct search functionality.

##### Critique API

Critique API appears to be a platform for publishing and using APIs, with pricing:

- **Starter**: Free, 10 API calls per minute, no token limit, includes streaming search endpoint. For 1000 searches, cost is $0 if within rate limits.
- **Pro**: $0.00 per month (usage costs only), 100 API calls per minute, $0.50 per million tokens total, with priority support and websocket streaming. Given the streaming search endpoint, it may offer search capabilities, but details on what constitutes an "API call" or "token" are unclear. Assuming each search uses tokens, for 1000 searches, if each uses 1, 000 to 10, 000 tokens, total tokens are 1, 000, 000 to 10, 000, 000, costing $0.50 to $5, respectively. Exact cost depends on usage and is hard to quantify without specifics.

#### Comparative Analysis

To compare, we standardize costs for 1000 searches, noting both prorated costs for comparison and actual costs if subscribed to plans:

| API Provider | Plan | Cost for 1000 Searches (Prorated) | Actual Cost for 1000 Searches (If Within Plan) |
| --- | --- | --- | --- |
| You.com API | Explorer AI | $8.50 | $100 (monthly fee for 11, 765 calls) |
| You.com API | Discoverer AI | $8.00 | $250 (monthly fee for 31, 250 calls) |
| Brave API | Free | $0.00 (up to 2, 000 requests) | $0 (if within limit) |
| Brave API | Base | $3.00 | $3 (pay-per-use) |
| Brave API | Pro | $5.00 | $5 (pay-per-use) |
| Tavily API | Free | $0.00 (up to 1, 000 credits) | $0 (if within limit) |
| Tavily API | Pay As You Go | $8.00 | $8 (pay-per-use) |
| Tavily API | Bootstrap | $6.67 | $100 (monthly fee for 15, 000 credits) |
| SerpAPI | Developer | $15.00 | $75 (monthly fee for 5, 000 searches) |
| SerpAPI | Production | $10.00 | $150 (monthly fee for 15, 000 searches) |
| Sonar Perplexity | Any | $5.00 (search cost only) | $5+ (includes token costs, estimated $5-$10+) |
| HasData | Startup | $2.45 | $49 (monthly fee for 20, 000 searches) |
| HasData | Business | $0.99 | $99 (monthly fee for 100, 000 searches) |
| Critique API | Pro | $0.50-$5 (estimated, token-based) | Variable (depends on token usage) |

#### Considerations and Recommendations

- **Traditional Search Needs**: For cost efficiency, Brave API's Base plan at $3 for 1000 searches is competitive, especially for pay-per-use models. The Free plan at $0 is viable for low volumes up to 2, 000 requests.
- **LLM-Enabled Search**: Sonar Perplexity offers a base search cost of $5 for 1000 searches, but token costs can increase the total, estimated at $5 to $10+. You.com API at $8.50 to $8.00 (prorated) is another option, with actual costs at $100 to $250 monthly.
- **Web Scraping for Search**: HasData at $0.99 prorated for 1000 searches (Business plan) is the cheapest, but its focus on scraping may not meet direct search API expectations. Actual cost is $99 monthly for up to 100, 000 searches, making it $99 for 1000 searches if subscribed.
- **Fixed vs. Pay-Per-Use**: Plans with fixed monthly fees (e.g., You.com, SerpAPI, HasData, Tavily Bootstrap) may be cost-effective for high volumes but expensive for low usage like 1000 searches. Pay-per-use models (Brave, Tavily Pay As You Go) are better for variable or low volumes.

Users should consider their specific use case, such as volume, complexity, and integration needs, and verify details on official websites like [You API Plans](https://api.you.com/plans), [Brave Search API](https://brave.com/search/api/), [Tavily](https://tavily.com/), [SerpApi](https://serpapi.com/), [Sonar Perplexity Docs](https://docs.perplexity.ai/), and [HasData Prices](https://hasdata.com/prices).

#### Key Citations

- [You API Plans Web LLM & Web Search Pricing](https://api.you.com/plans)
- [Brave Search API Power your search and AI apps](https://brave.com/search/api/)
- [Tavily Fast, reliable access with high rate limits](https://tavily.com/)
- [SerpApi Get Google results from anywhere in the world](https://serpapi.com/)
- [Sonar by Perplexity Power your products with real-time research](https://docs.perplexity.ai/)
- [HasData Our Prices Experience the incredible accuracy](https://hasdata.com/prices)

================
File: src/twat_search/web/engines/__init__.py
================
    __all__.extend(
    __all__.extend(["SerpApiSearchEngine", "google_serpapi"])
    __all__.extend(["TavilySearchEngine", "tavily"])
    __all__.extend(["PerplexitySearchEngine", "pplx"])
    __all__.extend(["CritiqueSearchEngine", "critique"])
    __all__.extend(["DuckDuckGoSearchEngine", "duckduckgo"])
    __all__.extend(["BingScraperSearchEngine", "bing_scraper"])
    logger.warning(f"Failed to import bing_scraper: {e}")
    __all__.extend(["GoogleScraperEngine", "google_scraper"])
    logger.warning(f"Failed to import google_scraper module: {e}")
    logger.warning(f"Failed to import searchit module: {e}")
def is_engine_available(engine_name: str) -> bool:
    standardized_name = standardize_engine_name(engine_name)
def get_engine_function(
    return available_engine_functions.get(standardized_name)
def get_available_engines() -> list[str]:
    return list(available_engine_functions.keys())

================
File: src/twat_search/web/engines/base.py
================
logger = logging.getLogger(__name__)
class SearchEngine(abc.ABC):
    def __init__(self, config: EngineConfig, **kwargs: Any) -> None:
        self.num_results = kwargs.get("num_results", 5)
        self.country = kwargs.get("country", None)
        self.language = kwargs.get("language", None)
        self.safe_search = kwargs.get("safe_search", True)
        self.time_frame = kwargs.get("time_frame", None)
        self.timeout = kwargs.get("timeout", 10)
        self.retries = kwargs.get("retries", 2)
        self.retry_delay = kwargs.get("retry_delay", 1.0)
        self.use_random_user_agent = kwargs.get("use_random_user_agent", True)
            raise EngineError(self.engine_code, msg)
            logger.debug(f"Engine '{self.engine_code}' requires an API key")
            logger.debug(f"Config API key: {self.config.api_key is not None}")
                key_prefix = key_value[:4] if len(key_value) > 4 else "****"
                key_suffix = key_value[-4:] if len(key_value) > 4 else "****"
                logger.debug(f"Config API key value: {key_prefix}...{key_suffix}")
                env_value = os.environ.get(env_var)
                logger.debug(f"Environment variable {env_var}: {env_value is not None}")
                    env_prefix = env_value[:4] if len(env_value) > 4 else "****"
                    env_suffix = env_value[-4:] if len(env_value) > 4 else "****"
                    logger.debug(f"Env var {env_var} value: {env_prefix}...{env_suffix}")
            logger.debug(f"Final API key check before validation: {self.config.api_key is not None}")
                    f"Please set it via one of these env vars: {', '.join(self.env_api_key_names)}"
                logger.error(f"API key validation failed: {msg}")
            logger.debug(f"API key validation passed for {self.engine_code}")
    def _get_num_results(self, param_name: str = "num_results", min_value: int = 1) -> int:
        value = self.kwargs.get(param_name)
                return max(min_value, int(value))
                logger.warning(f"Invalid value for '{param_name}' ({value!r}) in {self.engine_code}, using default.")
        default = self.config.default_params.get(param_name) or self.config.default_params.get("num_results")
                return max(min_value, int(default))
                logger.warning(
    def max_results(self) -> int:
        return self._get_num_results(param_name="num_results", min_value=1)
    def limit_results(self, results: list[SearchResult]) -> list[SearchResult]:
        logger.debug(f"limit_results: Got {len(results)} results, self.num_results={self.num_results}")
        logger.debug(f"limit_results: Using max_results={max_results}")
        if max_results > 0 and len(results) > max_results:
            logger.debug(f"limit_results: Limiting to {len(limited)} results")
        logger.debug(f"limit_results: Returning all {len(results)} results (no limiting needed)")
    async def make_http_request(
        if self.use_random_user_agent and "user-agent" not in {k.lower() for k in headers}:
            headers["User-Agent"] = random.choice(USER_AGENTS)
        for attempt in range(1, actual_retries + 2):  # +2 because we want actual_retries+1 attempts
                async with httpx.AsyncClient(timeout=actual_timeout) as client:
                    response = await client.request(
                    response.raise_for_status()
                jitter = random.uniform(0.5, 1.5)
                await asyncio.sleep(actual_delay)
            last_error = httpx.RequestError("Unknown error occurred during HTTP request")
        raise EngineError(self.engine_code, msg) from last_error
    async def search(self, query: str) -> list[SearchResult]:
    def _get_api_key(self) -> str:
            raise EngineError(
                f"API key is required. Set it via one of these env vars: {', '.join(self.env_api_key_names)}",
def register_engine(engine_class: type[SearchEngine]) -> type[SearchEngine]:
        if not hasattr(engine_class, "engine_code") or not engine_class.engine_code:
            raise AttributeError(error_msg)
        if not hasattr(engine_class, "env_api_key_names"):
                engine_class.friendly_engine_name = ENGINE_FRIENDLY_NAMES.get(
        logger.error(f"Failed to register engine {engine_class.__name__}: {e}")
def get_engine(engine_name: str, config: EngineConfig, **kwargs: Any) -> SearchEngine:
    engine_class = _engine_registry.get(engine_name)
        available_engines = ", ".join(sorted(_engine_registry.keys()))
        raise EngineError(engine_name, msg)
    if hasattr(engine_class, "env_api_key_names") and engine_class.env_api_key_names:
            logger.debug(f"No API key in config for {engine_name}, checking environment variables")
                    logger.debug(f"Found API key in environment variable {env_var}")
        return engine_class(config, **kwargs)
        raise EngineError(engine_name, msg) from e
def get_registered_engines() -> dict[str, type[SearchEngine]]:
    return _engine_registry.copy()

================
File: src/twat_search/web/engines/bing_scraper.py
================
logger = logging.getLogger(__name__)
    class BingScraper:  # type: ignore
        def __init__(
        def search(self, query: str, num_results: int = 10) -> list[Any]:
class BingScraperResult(BaseModel):
class BingScraperSearchEngine(SearchEngine):
        super().__init__(config, **kwargs)
        self.max_retries: int = kwargs.get(
        ) or self.config.default_params.get("max_retries", 3)
        self.delay_between_requests: float = kwargs.get(
        ) or self.config.default_params.get("delay_between_requests", 1.0)
            unused_params.append(f"country='{country}'")
            unused_params.append(f"language='{language}'")
            unused_params.append(f"safe_search={safe_search}")
            unused_params.append(f"time_frame='{time_frame}'")
            logger.debug(
                f"Parameters {', '.join(unused_params)} set but not used by Bing Scraper",
    def _convert_result(self, result: Any) -> SearchResult | None:
            logger.warning("Empty result received from Bing Scraper")
        if not hasattr(result, "title") or not hasattr(result, "url"):
            logger.warning(f"Invalid result format: {result}")
            validated = BingScraperResult(
                description=result.description if hasattr(result, "description") else None,
            return SearchResult(
                    "url": str(result.url),
                    "description": result.description if hasattr(result, "description") else None,
            logger.warning(f"Validation error for result: {exc}")
            logger.warning(f"Unexpected error converting result: {exc}")
    async def search(self, query: str) -> list[SearchResult]:
            raise EngineError(self.engine_code, "Search query cannot be empty")
        logger.info(f"Searching Bing with query: '{query}'")
            scraper = BingScraper(
            raw_results = scraper.search(query, num_results=self.num_results)
                logger.info("No results returned from Bing Scraper")
                f"Received {len(raw_results)} raw results from Bing Scraper",
                search_result = self._convert_result(result)
                    results.append(search_result)
                    if len(results) >= self.num_results:
            logger.info(
                f"Returning {len(results)} validated results from Bing Scraper",
            return self.limit_results(results)
            logger.error(error_msg)
            raise EngineError(self.engine_code, error_msg) from exc
async def bing_scraper(
    config = EngineConfig(enabled=True)
    engine = BingScraperSearchEngine(
    return await engine.search(query)

================
File: src/twat_search/web/engines/brave.py
================
class BraveResult(BaseModel):
class BraveNewsResult(BaseModel):
class BaseBraveEngine(SearchEngine):
    def __init__(
        super().__init__(
            raise EngineError(self.engine_code, "API key is required")
    async def search(self, query: str) -> list[SearchResult]:
        count_param = min(self.max_brave_results, self.num_results)
        print(f"DEBUG: Sending API request with count={count_param}, num_results={self.num_results}")
        async with httpx.AsyncClient() as client:
                response = await client.get(
                response.raise_for_status()
                data = response.json()
                section = data.get(self.response_key, {})
                if section.get("results"):
                            parsed = self.result_model(**result)
                            results.append(self.convert_result(parsed, result))
                            if len(results) >= self.num_results:
                return self.limit_results(results)
                raise EngineError(
    def convert_result(self, parsed: BaseModel, raw: dict[str, Any]) -> SearchResult:
        snippet = getattr(parsed, "description", "")
            publisher = getattr(parsed, "publisher", None)
            published_time = getattr(parsed, "published_time", None)
        url = getattr(parsed, "url", None)
            url = HttpUrl("https://brave.com")
        return SearchResult(
            title=getattr(parsed, "title", ""),
    async def _make_api_call(self, query: str) -> dict[str, Any]:
            raise EngineError(self.engine_code, "Search query cannot be empty")
            "count": min(
        print(f"DEBUG: In _make_api_call, using count={params['count']}")
        if hasattr(self, "freshness") and self.freshness:
        response = await self.make_http_request(
            data: dict[str, Any] = response.json()
    def limit_results(self, results: list[SearchResult]) -> list[SearchResult]:
        print(f"DEBUG: In limit_results, self.num_results={self.num_results}")
        print(f"DEBUG: Limiting {len(results)} results to {self.num_results} results")
        print(f"DEBUG: Returned {len(limited_results)} results")
class BraveSearchEngine(BaseBraveEngine):
        data = await self._make_api_call(query)
        print(f"DEBUG: Raw API response from Brave Web: {data}")
            web_result = data.get("web", {})
            items = web_result.get("results", [])
            print(f"DEBUG: Found {len(items)} web search items")
            for _idx, item in enumerate(items, start=1):
                title = item.get("title", "")
                url = item.get("url", "")
                description = item.get("description", "")
                results.append(
                    SearchResult(
class BraveNewsSearchEngine(BaseBraveEngine):
        print(f"DEBUG: Raw API response from Brave News: {data}")
            items = data.get("results", [])
            print(f"DEBUG: Found {len(items)} news items")
                if item.get("age") or item.get("source", {}).get("name"):
                    source_name = item.get("source", {}).get("name", "")
                    age = item.get("age", "")
                elif item.get("age"):
                    description = f"{description} - {item.get('age')}"
            print(f"DEBUG: Exception in BraveNewsSearchEngine.search: {e!s}")
async def brave(
    config = EngineConfig(api_key=api_key, enabled=True)
    engine = BraveSearchEngine(
    return await engine.search(query)
async def brave_news(
    engine = BraveNewsSearchEngine(

================
File: src/twat_search/web/engines/critique.py
================
class CritiqueResult(BaseModel):
    url: str = Field(default="")  # URL of the result source
    title: str = Field(default="")  # Title of the result
    summary: str = Field(default="")  # Summary or snippet from the result
    source: str = Field(default="")  # Source of the result
class CritiqueResponse(BaseModel):
    results: list[CritiqueResult] = Field(default_factory=list)
class CritiqueSearchEngine(SearchEngine):
    def __init__(
        super().__init__(config)
        self.num_results = self._get_num_results(param_name="num_results", min_value=1)
        self.image_url = image_url or kwargs.get("image_url")
        self.image_base64 = image_base64 or kwargs.get("image_base64")
        self.source_whitelist = source_whitelist or kwargs.get(
        self.source_blacklist = source_blacklist or kwargs.get(
        self.output_format = output_format or kwargs.get("output_format")
        api_key_from_kwargs = kwargs.get("api_key")
            raise EngineError(
                f"API key is required for critique. Set it via one of these env vars: {', '.join(self.env_api_key_names)} or use the --api-key parameter",
    async def _convert_image_url_to_base64(self, image_url: str) -> str:
            async with httpx.AsyncClient() as client:
                response = await client.get(image_url, timeout=30)
                response.raise_for_status()
                encoded = base64.b64encode(response.content).decode("utf-8")
            raise EngineError(self.engine_code, f"Error processing image: {e}")
    async def _build_payload(self, query: str) -> dict[str, Any]:
            payload["image"] = await self._convert_image_url_to_base64(self.image_url)
    def _build_result(self, item: CritiqueResult, rank: int) -> SearchResult:
                HttpUrl(item.url)
                else HttpUrl(
            url_obj = HttpUrl("https://critique-labs.ai")
        return SearchResult(
            raw=item.model_dump(),
    def _parse_results(self, data: dict[str, Any]) -> list[SearchResult]:
        critique_data = CritiqueResponse(
            results=data.get("results", []),
            response=data.get("response"),
            structured_output=data.get("structured_output"),
            results.append(
                SearchResult(
                    url=HttpUrl("https://critique-labs.ai"),
        for idx, item in enumerate(critique_data.results, 1):
                results.append(self._build_result(item, idx))
    async def search(self, query: str) -> list[SearchResult]:
        payload = await self._build_payload(query)
                response = await client.post(
                data = response.json()
                results = self._parse_results(data)
                return self.limit_results(results)
async def critique(
    config = EngineConfig(api_key=api_key, enabled=True)
    engine = CritiqueSearchEngine(
    return await engine.search(query)

================
File: src/twat_search/web/engines/duckduckgo.py
================
logger = logging.getLogger(__name__)
class DuckDuckGoResult(BaseModel):
class DuckDuckGoSearchEngine(SearchEngine):
    def __init__(
        super().__init__(config, **kwargs)
        ) = self._map_init_params(
            logger.debug(
    def _map_init_params(
        num_results = kwargs.get(
        ) or config.default_params.get("num_results", 10)
        region = kwargs.get("region", country) or config.default_params.get(
        lang = language or config.default_params.get("language", None)
        timelimit = kwargs.get("timelimit", time_frame) or config.default_params.get(
        if timelimit and not kwargs.get("timelimit"):
            timelimit = time_mapping.get(timelimit.lower(), timelimit)
        safesearch = kwargs.get("safesearch", safe_search)
        if isinstance(safesearch, str):
                if safesearch.lower()
        proxy = kwargs.get("proxy") or config.default_params.get("proxy", None)
        timeout = kwargs.get(
        ) or config.default_params.get("timeout", 10)
    def _convert_result(self, raw: dict[str, Any]) -> SearchResult | None:
            ddg_result = DuckDuckGoResult(
            return SearchResult(
            logger.warning(f"Validation error for result: {exc}")
    async def search(self, query: str) -> list[SearchResult]:
            ddgs = DDGS(proxy=self.proxy, timeout=self.timeout)
            raw_results = ddgs.text(
                converted = self._convert_result(raw)
                    results.append(converted)
                    if len(results) >= self.num_results:
            raise EngineError(
async def duckduckgo(
    config = EngineConfig(enabled=True)
    engine = DuckDuckGoSearchEngine(
    return await engine.search(query)

================
File: src/twat_search/web/engines/google_scraper.py
================
    class GoogleSearchResult:  # type: ignore
        def __init__(self, url: str, title: str, description: str):
    def google_search(*args, **kwargs):  # type: ignore
logger = logging.getLogger(__name__)
class GoogleScraperResult(BaseModel):
class GoogleScraperEngine(SearchEngine):
    def __init__(
        super().__init__(config, **kwargs)
        self.num_results: int = num_results or self.config.default_params.get(
        self.language: str = language or self.config.default_params.get(
        self.region: str | None = country or self.config.default_params.get(
            else self.config.default_params.get("safe", "active")
        self.sleep_interval: float = kwargs.get(
        ) or self.config.default_params.get("sleep_interval", 0.0)
        self.ssl_verify: bool | None = kwargs.get(
        ) or self.config.default_params.get("ssl_verify", None)
        self.proxy: str | None = kwargs.get("proxy") or self.config.default_params.get(
        self.unique: bool = kwargs.get("unique") or self.config.default_params.get(
            unused_params.append(f"time_frame='{time_frame}'")
            logger.debug(
                f"Parameters {', '.join(unused_params)} set but not used by Google Scraper",
    def _convert_result(self, result: GoogleSearchResult) -> SearchResult | None:
            logger.warning("Empty result received from Google Scraper")
            title = getattr(result, "title", "") or ""
            description = getattr(result, "description", "") or ""
            validated = GoogleScraperResult(
                url=HttpUrl(result.url),
            return SearchResult(
                    "url": str(result.url),
            logger.warning(f"Validation error for result: {exc}")
            logger.warning(f"Unexpected error converting result: {exc}")
    async def search(self, query: str) -> list[SearchResult]:
            raise EngineError(self.engine_code, "Search query cannot be empty")
        logger.info(f"Searching Google with query: '{query}'")
            raw_results = list(
                google_search(
                logger.info("No results returned from Google Scraper")
                f"Received {len(raw_results)} raw results from Google Scraper",
            logger.error(error_msg)
            raise EngineError(self.engine_code, error_msg) from exc
            for search_result in (self._convert_result(cast(GoogleSearchResult, result)) for result in raw_results)
        logger.info(
            f"Returning {len(results)} validated results from Google Scraper",
        return self.limit_results(results)
async def google_scraper(
    return await search(

================
File: src/twat_search/web/engines/hasdata.py
================
class HasDataGoogleResult(BaseModel):
    def from_api_result(cls, result: dict[str, Any]) -> HasDataGoogleResult:
        return cls(
            title=result.get("title", ""),
            url=HttpUrl(result.get("link", "")),
            snippet=result.get("snippet", ""),
class HasDataBaseEngine(SearchEngine):
    def __init__(
        super().__init__(config)
        self.location = location or kwargs.get("location") or self.config.default_params.get("location")
            device_type or kwargs.get("device_type") or self.config.default_params.get("device_type", "desktop")
            raise EngineError(
                f"HasData API key is required. Set it via one of these env vars: {', '.join(self.env_api_key_names)}",
    async def search(self, query: str) -> list[SearchResult]:
        async with httpx.AsyncClient() as client:
                response = await client.get(
                response.raise_for_status()
                data = response.json()
                organic_results = data.get("organicResults", [])
                for i, result in enumerate(organic_results):
                        parsed = HasDataGoogleResult.from_api_result(result)
                        results.append(
                            SearchResult(
class HasDataGoogleEngine(HasDataBaseEngine):
class HasDataGoogleLightEngine(HasDataBaseEngine):
async def hasdata_google_full(
    config = EngineConfig(api_key=api_key, enabled=True)
    engine = HasDataGoogleEngine(
    return await engine.search(query)
async def hasdata_google(
    engine = HasDataGoogleLightEngine(

================
File: src/twat_search/web/engines/pplx.py
================
logger = logging.getLogger(__name__)
class PerplexityResult(BaseModel):
    answer: str = Field(default="")
    url: str = Field(default="https://perplexity.ai")
    title: str = Field(default="Perplexity AI Response")  # Default title
class PerplexitySearchEngine(SearchEngine):
    def __init__(
        super().__init__(config, **kwargs)
        self.model = self.kwargs.get("model") or self.config.default_params.get("model", "sonar")
    async def search(self, query: str) -> list[SearchResult]:
            raise EngineError(self.engine_code, "Search query cannot be empty")
            "search_recency_filter": self.kwargs.get("time_frame", ""),
            response = await self.make_http_request(
            data = response.json()
            raise EngineError(
                f"API request failed: {str(e)!s}",
                f"Unexpected error: {str(e)!s}",
        for choice in data.get("choices", []):
            answer = choice.get("message", {}).get("content", "")
                pr = PerplexityResult(answer=answer, url=url, title=title)
                url_obj = HttpUrl(pr.url)  # Validate URL format
                results.append(
                    SearchResult(
                if len(results) >= self.max_results:
                logger.warning(f"Invalid result from Perplexity: {e}")
        return self.limit_results(results)
async def pplx(
    logger.debug(f"pplx function called with api_key parameter: {api_key is not None}")
            env_value = os.environ.get(env_var)
            logger.debug(f"Environment variable {env_var} in pplx function: {env_value is not None}")
                logger.debug(f"Using API key from environment variable {env_var}")
        key_prefix = api_key[:4] if len(api_key) > 4 else "****"
        key_suffix = api_key[-4:] if len(api_key) > 4 else "****"
        logger.debug(f"Final API key value: {key_prefix}...{key_suffix}")
        logger.error(
    config = EngineConfig(
    logger.debug(f"Created EngineConfig with api_key: {config.api_key is not None}")
        logger.debug("Creating PerplexitySearchEngine instance")
        engine = PerplexitySearchEngine(
        logger.debug("PerplexitySearchEngine instance created successfully")
        logger.debug("Executing search with PerplexitySearchEngine")
        results = await engine.search(query)
        logger.debug(f"Search completed, got {len(results)} results")
        logger.error(f"Error in pplx function: {e}")

================
File: src/twat_search/web/engines/searchit.py
================
    class SearchitResult:  # type: ignore
        def __init__(self, rank: int, url: str, title: str, description: str):
    class ScrapeRequest:  # type: ignore
        def __init__(
    class GoogleScraper:  # type: ignore
        def __init__(self, max_results_per_page: int = 100):
        async def scrape(request: ScrapeRequest) -> list[SearchitResult]:
    class YandexScraper:  # type: ignore
        def __init__(self, max_results_per_page: int = 10):
    class QwantScraper:  # type: ignore
    class BingScraper:  # type: ignore
logger = logging.getLogger(__name__)
class SearchitScraperResult(BaseModel):
    @field_validator("title", "description")
    def validate_non_empty(cls, v: str) -> str:
class SearchitEngine(SearchEngine):
        super().__init__(config, **kwargs)
        self.num_results: int = num_results or self.config.default_params.get(
        self.language: str = language or self.config.default_params.get(
        self.domain: str | None = country or self.config.default_params.get(
        self.geo: str | None = country or self.config.default_params.get(
        self.sleep_interval: int = kwargs.get(
        ) or self.config.default_params.get("sleep_interval", 0)
        self.proxy: str | None = kwargs.get("proxy") or self.config.default_params.get(
            unused_params.append(f"safe_search={safe_search}")
            unused_params.append(f"time_frame='{time_frame}'")
            logger.debug(
                f"Parameters {', '.join(unused_params)} set but not used by {self.engine_code}",
    def _convert_result(self, result: SearchitResult) -> SearchResult | None:
            logger.warning(f"Empty result received from {self.engine_code}")
            validated = SearchitScraperResult(
                url=HttpUrl(result.url),
                description=result.description if hasattr(result, "description") else "",
            return SearchResult(
                    "url": str(result.url),
                    "description": result.description if hasattr(result, "description") else "",
            logger.warning(f"Validation error for result: {exc}")
            logger.warning(f"Unexpected error converting result: {exc}")
    async def _run_scraper(
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(
                lambda: asyncio.run(scraper.scrape(request)),
            logger.error(f"Error running searchit scraper: {exc}")
            raise EngineError(
class GoogleSearchitEngine(SearchitEngine):
    async def search(self, query: str) -> list[SearchResult]:
            raise EngineError(self.engine_code, "Search query cannot be empty")
        logger.info(f"Searching Google (searchit) with query: '{query}'")
            request = ScrapeRequest(
            scraper = GoogleScraper(
                max_results_per_page=min(100, self.num_results),
            raw_results = await self._run_scraper(scraper, request)
                logger.info("No results returned from Google (searchit)")
                f"Received {len(raw_results)} raw results from Google (searchit)",
                for search_result in (self._convert_result(result) for result in raw_results)
            logger.info(
                f"Returning {len(results)} validated results from Google (searchit)",
            logger.error(error_msg)
            raise EngineError(self.engine_code, error_msg) from exc
class YandexSearchitEngine(SearchitEngine):
        logger.info(f"Searching Yandex (searchit) with query: '{query}'")
            scraper = YandexScraper(
                max_results_per_page=min(10, self.num_results),
                logger.info("No results returned from Yandex (searchit)")
                f"Received {len(raw_results)} raw results from Yandex (searchit)",
                f"Returning {len(results)} validated results from Yandex (searchit)",
class QwantSearchitEngine(SearchitEngine):
        logger.info(f"Searching Qwant (searchit) with query: '{query}'")
            scraper = QwantScraper(
                logger.info("No results returned from Qwant (searchit)")
                f"Received {len(raw_results)} raw results from Qwant (searchit)",
                f"Returning {len(results)} validated results from Qwant (searchit)",
class BingSearchitEngine(SearchitEngine):
        logger.info(f"Searching Bing (searchit) with query: '{query}'")
            scraper = BingScraper(
                max_results_per_page=min(30, self.num_results),
                logger.info("No results returned from Bing (searchit)")
                f"Received {len(raw_results)} raw results from Bing (searchit)",
                f"Returning {len(results)} validated results from Bing (searchit)",
async def google_searchit(
    return await search(
async def yandex_searchit(
async def qwant_searchit(
async def bing_searchit(

================
File: src/twat_search/web/engines/serpapi.py
================
class SerpApiResult(BaseModel):
class SerpApiResponse(BaseModel):
class SerpApiSearchEngine(SearchEngine):
    def __init__(
        super().__init__(config)
            "num": kwargs.get("num", num_results) or self.config.default_params.get("num", 10),
            "google_domain": kwargs.get("google_domain")
            or self.config.default_params.get("google_domain", "google.com"),
            "gl": kwargs.get("gl", country) or self.config.default_params.get("gl"),
            "hl": kwargs.get("hl", language) or self.config.default_params.get("hl"),
            "safe": _convert_safe(kwargs.get("safe", safe_search)) or self.config.default_params.get("safe"),
            "time_period": kwargs.get("time_period", time_frame) or self.config.default_params.get("time_period"),
            raise EngineError(
                f"SerpApi API key is required. Set it via one of these env vars: {', '.join(self.env_api_key_names)}",
    async def search(self, query: str) -> list[SearchResult]:
        params.update({k: v for k, v in self._params.items() if v is not None})
        async with httpx.AsyncClient() as client:
                response = await client.get(
                response.raise_for_status()
                data = response.json()
                serpapi_response = SerpApiResponse(**data)
                        results.append(
                            SearchResult(
                                raw=result.model_dump(),  # Include raw result for debugging
                return self.limit_results(results)
def _convert_safe(safe: bool | str | None) -> str | None:
    if isinstance(safe, bool):
async def google_serpapi(
    config = EngineConfig(
    engine = SerpApiSearchEngine(
    return await engine.search(query)

================
File: src/twat_search/web/engines/tavily.py
================
class TavilySearchResult(BaseModel):
class TavilySearchResponse(BaseModel):
class TavilySearchEngine(SearchEngine):
    def __init__(
        super().__init__(config, **kwargs)
            self.num_results = kwargs.get("num_results") or self.config.default_params.get(
        self.search_depth = search_depth or self.config.default_params.get("search_depth", "basic")
        self.include_domains = include_domains or self.config.default_params.get("include_domains", None)
        self.exclude_domains = exclude_domains or self.config.default_params.get("exclude_domains", None)
        self.include_answer = include_answer or self.config.default_params.get("include_answer", False)
        self.max_tokens = max_tokens or self.config.default_params.get("max_tokens", None)
        self.search_type = search_type or self.config.default_params.get("search_type", "search")
            raise EngineError(
                f"Tavily API key is required. Set it via one of these env vars: {', '.join(self.env_api_key_names)}",
    def _build_payload(self, query: str) -> dict:
    def _convert_result(self, item: dict, rank: int) -> SearchResult | None:
            validated_url = HttpUrl(item.get("url", ""))
            return SearchResult(
                title=item.get("title", ""),
                snippet=textwrap.shorten(
                    item.get("content", "").strip(),
    async def search(self, query: str) -> list[SearchResult]:
        payload = self._build_payload(query)
        async with httpx.AsyncClient() as client:
                response = await client.post(
                response.raise_for_status()
                data = response.json()
                raise EngineError(self.engine_code, f"HTTP error: {e}")
                raise EngineError(self.engine_code, f"Request error: {e}")
                raise EngineError(self.engine_code, f"Error: {e!s}")
            parsed_response = TavilySearchResponse.model_validate(data)
            items = [item.model_dump() for item in parsed_response.results]
            items = data.get("results", [])
        for idx, item in enumerate(items, start=1):
            converted = self._convert_result(item, idx)
                results.append(converted)
async def tavily(
    config = EngineConfig(
    engine = TavilySearchEngine(
    return await engine.search(query)

================
File: src/twat_search/web/engines/you.py
================
logger = logging.getLogger(__name__)
class YouSearchHit(BaseModel):
    snippet: str = Field(alias="description")
class YouSearchResponse(BaseModel):
    search_id: str | None = Field(None, alias="searchId")
class YouNewsArticle(BaseModel):
class YouNewsResponse(BaseModel):
class YouBaseEngine(SearchEngine):
    def __init__(
        super().__init__(config, **kwargs)
        self.country_code = kwargs.get("country") or self.config.default_params.get(
    async def _make_api_call(self, query: str) -> Any:
            params["safesearch"] = str(self.safe_search).lower()
        logger.debug(f"Making You.com API request to {self.base_url} with params: {params}")
            response = await self.make_http_request(
            return response.json()
            raise EngineError(self.engine_code, f"API request failed: {str(e)!s}")
            raise EngineError(self.engine_code, f"Unexpected error: {str(e)!s}")
class YouSearchEngine(YouBaseEngine):
    async def search(self, query: str) -> list[SearchResult]:
            raise EngineError(self.engine_code, "Search query cannot be empty")
        data = await self._make_api_call(query)
            you_response = YouSearchResponse(**data)
                    results.append(
                        SearchResult(
                            raw=hit.model_dump(by_alias=True),
                    if len(results) >= self.max_results:
                    logger.warning(f"Invalid result from You.com: {e}")
            return self.limit_results(results)
            raise EngineError(
class YouNewsSearchEngine(YouBaseEngine):
            you_response = YouNewsResponse(**data)
                            raw=article.model_dump(by_alias=True),
                    logger.warning(f"Invalid news result from You.com: {e}")
async def you(
    config = EngineConfig(
    engine = YouSearchEngine(
    return await engine.search(query)
async def you_news(
    engine = YouNewsSearchEngine(

================
File: src/twat_search/web/__init__.py
================
    __all__.extend(["Config", "EngineConfig", "SearchResult", "search"])
    __all__.extend(["brave", "brave_news"])
    __all__.extend(["pplx"])
    __all__.extend(["serpapi"])
    __all__.extend(["tavily"])
    __all__.extend(["you", "you_news"])
    __all__.extend(["critique"])
    __all__.extend(["duckduckgo"])
    __all__.extend(["bing_scraper"])

================
File: src/twat_search/web/api.py
================
logger = logging.getLogger(__name__)
def get_engine_params(
    std_engine_name = standardize_engine_name(engine_name)
    for k, v in kwargs.items():
        if k.startswith(std_engine_name + "_"):
            engine_specific[k[len(std_engine_name) + 1 :]] = v
        elif k.startswith(engine_name + "_"):  # For backward compatibility
            engine_specific[k[len(engine_name) + 1 :]] = v
    std_engines = [standardize_engine_name(e) for e in engines]
    non_prefixed = {k: v for k, v in kwargs.items() if not any(k.startswith(e + "_") for e in std_engines + engines)}
def init_engine_task(
    engine_config = config.engines.get(std_engine_name)
        engine_config = config.engines.get(engine_name)
            logger.warning(f"Engine '{engine_name}' not configured.")
            engine_config = EngineConfig(enabled=True)
    num_results = kwargs.get("num_results")
        logger.debug(f"Initializing engine '{engine_name}' with num_results={num_results}")
        engine_params = get_engine_params(
            engines=kwargs.get("engines", []),
            common_params=kwargs.get("common_params", {}),
        engine_instance: SearchEngine = get_engine(
        logger.info(f"🔍 Querying engine: {engine_name}")
        return engine_name, engine_instance.search(query)
        logger.warning(
            f"Failed to initialize engine '{engine_name}': {type(e).__name__}",
        logger.error(f"Error initializing engine '{engine_name}': {e}")
async def search(
        config = config or Config()
        explicit_engines_requested = engines is not None and len(engines) > 0
        engines_to_try = engines or list(config.engines.keys())
            logger.error(msg)
            raise SearchError(msg)
        logger.debug(f"Search requested with num_results={num_results}")
        engines_to_try = [standardize_engine_name(e) for e in engines_to_try]
                task = init_engine_task(
                    engine_names.append(task[0])
                    tasks.append(task[1])
                logger.warning(f"Failed to initialize engine '{engine_name}': {e}")
                failed_engines.append(engine_name)
            failed_engines_str = ", ".join(failed_engines)
            logger.warning(f"Failed to initialize engines: {failed_engines_str}")
                msg = f"No search engines could be initialized from requested engines: {', '.join(engines_to_try)}"
            logger.warning("Falling back to any available search engine...")
            for engine_name in get_registered_engines():
                    logger.debug(f"Failed to initialize engine {engine_name}: {e}")
        results = await asyncio.gather(*tasks, return_exceptions=True)
        for engine_name, result in zip(engine_names, results, strict=False):
            if isinstance(result, Exception):
                logger.error(
            elif isinstance(result, list):
                logger.info(
                    f"✅ Engine '{engine_name}' returned {len(result)} results",
                flattened_results.extend(result)
                    f"⚠️ Engine '{engine_name}' returned no results or unexpected type: {type(result)}",
        logger.error(f"Search failed: {e}")

================
File: src/twat_search/web/cli.py
================
class CustomJSONEncoder(json_lib.JSONEncoder):
    def default(self, o: Any) -> Any:
            return str(o)
            return json_lib.JSONEncoder.default(self, o)
console = Console()
class SearchCLI:
    def __init__(self) -> None:
        self.logger = logging.getLogger("twat_search.cli")
        self.log_handler = RichHandler(rich_tracebacks=True)
        self._configure_logging()
        self.console = Console()
        available_engines = get_available_engines()
            self.logger.warning(
                f"{', '.join(missing_engines)}. "
    def _configure_logging(self, verbose: bool = False) -> None:
        logging.basicConfig(
        self.logger.setLevel(level)
        logging.getLogger("twat_search.web.api").setLevel(level)
        logging.getLogger("twat_search.web.engines").setLevel(level)
        logging.getLogger("httpx").setLevel(level)
    def _parse_engines(self, engines_arg: Any) -> list[str] | None:
        if isinstance(engines_arg, str):
            if engines_arg.strip().lower() == "free":
                self.logger.info(f"Using 'free' engines: {', '.join(engines)}")
            elif engines_arg.strip().lower() == "best":
                self.logger.info(f"Using 'best' engines: {', '.join(engines)}")
            elif engines_arg.strip().lower() == "all":
                engines = get_available_engines()
                self.logger.info(
                    f"Using 'all' available engines: {', '.join(engines)}",
            engines = [e.strip() for e in engines_arg.split(",") if e.strip()]
            return [standardize_engine_name(e) for e in engines]
        if isinstance(engines_arg, list | tuple):
            engines = [str(e).strip() for e in engines_arg if str(e).strip()]
            f"Unexpected engines type: {type(engines_arg)}. Using all available engines.",
    async def _run_search(
                standardized_name = standardize_engine_name(engine)
                    available.append(engine)
                    invalid_engines.append(engine)
                        f"Engine '{engine}' is not valid. Valid engines are: {', '.join(ALL_POSSIBLE_ENGINES)}",
                error_msg = f"None of the specified engines are valid: {', '.join(invalid_engines)}"
                self.logger.error(error_msg)
                _display_errors([error_msg])
            self.logger.debug(f"Attempting to search with engines: {engines}")
            results = await search(query=query, engines=engines, strict_mode=True, **kwargs)
            return _process_results(results)
            self.logger.error(f"Search failed: {e}")
            _display_errors([str(e)])
    async def _search_engine(
            engine_func = get_engine_function(engine)
                self.logger.warning(error_msg)
            registered_engines = get_registered_engines()
            engine_class = registered_engines.get(engine)
            friendly = engine_class.friendly_engine_name if engine_class else ENGINE_FRIENDLY_NAMES.get(engine, engine)
            friendly = ENGINE_FRIENDLY_NAMES.get(engine, engine)
            self.console.print(f"[bold]Searching {friendly}[/bold]: {query}")
            results = await engine_func(query=query, **params)
            processed_results = _process_results(results)
                _display_json_results(processed_results)
                _display_results(processed_results, verbose, plain)
            self.logger.error(f"{friendly} search failed: {e}")
    def q(
        self._configure_logging(verbose)
            self.logger.debug(f"Overriding num_results={num_results} with n={kwargs['n']}")
        self.logger.debug(f"Args - n: {kwargs.get('n')}, num_results: {num_results}")
        self.logger.debug(f"Using num_results={num_results}")
            self.logger.debug(
        engine_list = self._parse_engines(engines)
        common_params = {k: v for k, v in common_params.items() if v is not None}
        self.logger.debug(f"Right before executing search, num_results={num_results}")
            results = asyncio.run(
                self._run_search(
            with self.console.status(
            _display_json_results(results)
            _display_results(results, verbose, plain)
    def info(
            config = Config()
                self._display_engines_json(engine, config)
                self._display_engines_plain(engine, config)
                self._list_all_engines(config)
                self._show_engine_details(engine, config)
                self.logger.error(
    def _display_engines_plain(self, engine: str | None, config: Config) -> None:
                self.console.print(engine)
            for engine_name in sorted(config.engines.keys()):
                self.console.print(engine_name)
    def _list_all_engines(self, config: Config) -> None:
        table = Table(title="🔎 Available Search Engines")
        table.add_column("Engine", style="cyan", no_wrap=True)
        table.add_column("Enabled", style="magenta")
        table.add_column("API Key Required", style="yellow")
        sorted_engines = sorted(config.engines.items(), key=lambda x: x[0])
                hasattr(
                if engine_class and hasattr(engine_class, "env_api_key_names"):
                    api_key_required = bool(engine_class.env_api_key_names)
            table.add_row(
        self.console.print(table)
        self.console.print(
    def _show_engine_details(self, engine_name: str, config: Config) -> None:
            self.console.print("\nAvailable engines:")
                self.console.print(f"- {name}")
        api_key_required = hasattr(engine_config, "api_key") and engine_config.api_key is not None
            engine_class = registered_engines.get(engine_name)
            if not api_key_required and engine_class and hasattr(engine_class, "env_api_key_names"):
                if engine_class and hasattr(engine_class, "friendly_name")
                else ENGINE_FRIENDLY_NAMES.get(engine_name, engine_name)
                    value_status = "✅" if os.environ.get(env_name) else "❌"
                    self.console.print(f"  {env_name}: {value_status}")
            self.console.print("\n[bold]Default Parameters:[/bold]")
                for param, value in engine_config.default_params.items():
                    self.console.print(f"  {param}: {value}")
                self.console.print("  No default parameters specified")
                base_engine = engine_name.split("-")[0]
                engine_module = importlib.import_module(module_name)
                function_name = engine_name.replace("-", "_")
                if hasattr(engine_module, function_name):
                    func = getattr(engine_module, function_name)
                    self.console.print("\n[bold]Function Interface:[/bold]")
                        f"  [green]{function_name}()[/green] - {func.__doc__.strip().split('\\n')[0]}",
                    self.console.print("\n[bold]Example Usage:[/bold]")
            self.console.print("\n[bold]Basic Configuration:[/bold]")
    def _display_engines_json(self, engine: str | None, config: Config) -> None:
            result[engine] = _get_engine_info(
            for engine_name, engine_config in sorted(config.engines.items()):
                result[engine_name] = _get_engine_info(
        print(json_lib.dumps(result, indent=2, cls=CustomJSONEncoder))
    async def critique(
            params["source_whitelist"] = [domain.strip() for domain in source_whitelist.split(",")]
            params["source_blacklist"] = [domain.strip() for domain in source_blacklist.split(",")]
        params.update(kwargs)
        return await self._search_engine(
    async def brave(
        params = {k: v for k, v in params.items() if v is not None}
        return await self._search_engine("brave", query, params, json, verbose, plain)
    async def brave_news(
    async def serpapi(
        return await self._search_engine("serpapi", query, params, json, verbose, plain)
    async def tavily(
            params["include_domains"] = [s.strip() for s in include_domains.split(",") if s.strip()]
            params["exclude_domains"] = [s.strip() for s in exclude_domains.split(",") if s.strip()]
        return await self._search_engine("tavily", query, params, json, verbose, plain)
    async def pplx(
        self.logger.debug(f"CLI pplx method called with api_key: {api_key is not None}")
            env_value = os.environ.get(env_var)
            self.logger.debug(f"CLI pplx method - Environment variable {env_var}: {env_value is not None}")
        self.logger.debug(f"CLI pplx method - params: {params}")
        self.logger.debug(f"CLI pplx method - final params: {params}")
        return await self._search_engine("pplx", query, params, json, verbose, plain)
    async def you(
        return await self._search_engine("you", query, params, json, verbose, plain)
    async def you_news(
    async def duckduckgo(
    async def hasdata_google(
    async def hasdata_google_light(
def _check_engine_availability(engine_name: str) -> bool:
    return is_engine_available(engine_name)
def _get_engine_info(
    if hasattr(engine_config, "api_key") and engine_config.api_key is not None:
                {"name": env_name, "set": bool(os.environ.get(env_name))} for env_name in engine_class.env_api_key_names
        if hasattr(
        "enabled": engine_config.enabled if hasattr(engine_config, "enabled") else False,
def _process_results(results: list) -> list[dict[str, Any]]:
        engine_name = getattr(result, "source", None) or "unknown"
        engine_results.setdefault(engine_name, []).append(result)
    for engine, engine_results_list in engine_results.items():
            processed.append(
        for idx, result in enumerate(engine_results_list):
            url = str(result.url)
                    "snippet": result.snippet[:100] + "..." if len(result.snippet) > 100 else result.snippet,
                    "raw_result": getattr(result, "raw", None),
def _display_results(
        console.print("[bold red]No results found![/bold red]")
        urls = set()
                urls.add(result["url"])
        for url in sorted(urls):
            console.print(url)
    table = Table()  # Remove show_lines=True to eliminate row separator lines
        table.add_column("Status", style="magenta")
        table.add_column("Title", style="green")
        table.add_column("URL", style="blue", overflow="fold")
        table.add_column("URL", style="blue", overflow="fold", max_width=70)
            table.add_row(result["engine"], result["url"])
    console.print(table)
                console.print(result)
def _display_errors(error_messages: list[str]) -> None:
    table = Table(title="❌ Search Errors")
    table.add_column("Error", style="red")
        table.add_row(error)
def _display_json_results(processed_results: list[dict[str, Any]]) -> None:
        results_by_engine[engine]["results"].append(
                "snippet": result.get("snippet") if result.get("snippet") != "N/A" else None,
                "raw": result.get("raw_result"),
    print(json_lib.dumps(results_by_engine, indent=2, cls=CustomJSONEncoder))
def main() -> None:
    fire.core.Display = lambda lines, out: console.print(*lines)
    fire.Fire(SearchCLI)
    main()

================
File: src/twat_search/web/config.py
================
load_dotenv()
logger = logging.getLogger(__name__)
class EngineConfig(BaseModel):
    default_params: dict[str, Any] = Field(default_factory=dict)
    def __init__(self, **data: Any) -> None:
        super().__init__(**data)
            logger.debug(f"No API key provided for {self.engine_code}, checking environment variables")
                engine_class = get_registered_engines().get(self.engine_code)
                if engine_class and hasattr(engine_class, "env_api_key_names") and engine_class.env_api_key_names:
                    logger.debug(
                        env_value = os.environ.get(env_var)
                        logger.debug(f"Checking env var '{env_var}' in __init__: {env_value is not None}")
                            logger.debug(f"Using API key from environment variable '{env_var}'")
                logger.debug(f"Couldn't check API key requirements for {self.engine_code}: {e}")
    @field_validator("api_key")
    def validate_api_key(cls, v: str | None, info: Any) -> str | None:
        engine_code = info.data.get("engine_code")
        logger.debug(f"Validating API key for engine '{engine_code}', current value: {v is not None}")
                engine_class = get_registered_engines().get(engine_code)
                logger.debug(f"Found engine class for '{engine_code}': {engine_class is not None}")
                        logger.debug(f"Checking env var '{env_var}': {env_value is not None}")
                    logger.warning(
                        f"Please set one of these environment variables: {', '.join(engine_class.env_api_key_names)}",
                logger.debug(f"Couldn't check API key requirements for {engine_code}: {e}")
class Config(BaseModel):
    engines: dict[str, EngineConfig] = Field(default_factory=dict)
    def get_config_path(cls) -> Path:
            return Path(cls.config_path)
        env_path = os.environ.get("TWAT_SEARCH_CONFIG_PATH")
            return Path(env_path)
        home_config = Path.home() / ".twat" / "search_config.json"
        if home_config.exists():
        xdg_config = Path.home() / ".config" / "twat" / "search_config.json"
        if xdg_config.exists():
        config_data = json.loads(json.dumps(DEFAULT_CONFIG))  # Deep copy
        config_path = self.get_config_path()
        if config_path.exists():
                with open(config_path, encoding="utf-8") as f:
                    file_config = json.load(f)
                self._merge_config(config_data, file_config)
                logger.info(f"Loaded configuration from {config_path}")
                logger.error(f"Error loading configuration from {config_path}: {e}")
        _apply_env_overrides(config_data)
            self._merge_config(config_data, data)
        super().__init__(**config_data)
    def _merge_config(self, target: dict[str, Any], source: dict[str, Any]) -> None:
        for key, value in source.items():
            if key in target and isinstance(target[key], dict) and isinstance(value, dict):
                self._merge_config(target[key], value)
    def add_engine(
            engine_config.default_params.update(default_params)
            self.engines[engine_name] = EngineConfig(
def _apply_env_overrides(config_data: dict[str, Any]) -> None:
    for env_var, config_path in ENV_VAR_MAP.items():
            value = _parse_env_value(env_value)
            if isinstance(config_path, str):
                _set_nested_value(config_data, config_path, value)
                _set_nested_value(config_data, path, value)
def _parse_env_value(env_value: str) -> Any:
    if env_value.lower() in ("true", "yes", "1"):
    elif env_value.lower() in ("false", "no", "0"):
    elif env_value.isdigit():
        return int(env_value)
    elif env_value.replace(".", "", 1).isdigit():
        return float(env_value)
def _set_nested_value(config_data: dict[str, Any], path: list[str], value: Any) -> None:
    for i, path_part in enumerate(path):
        if i == len(path) - 1:
            if path_part not in current or not isinstance(current[path_part], dict):

================
File: src/twat_search/web/engine_constants.py
================
def standardize_engine_name(engine_name: str) -> str:
    return engine_name.replace("-", "_")

================
File: src/twat_search/web/exceptions.py
================
class SearchError(Exception):
    def __init__(self, message: str) -> None:
        super().__init__(message)
class EngineError(SearchError):
    def __init__(self, engine_name: str, message: str) -> None:
        super().__init__(f"Engine '{engine_name}': {message}")

================
File: src/twat_search/web/models.py
================
class SearchResult(BaseModel):
    @field_validator("source")
    def validate_non_empty(cls, v: str) -> str:
        if not v or not v.strip():
            raise ValueError(msg)
        return v.strip()
    @field_validator("title", "snippet")
    def ensure_string(cls, v: str) -> str:
        return v.strip() if v and v.strip() else ""

================
File: src/twat_search/web/utils.py
================
logger = logging.getLogger(__name__)
def load_environment(force_reload: bool = False) -> None:
    if not force_reload and os.environ.get(loaded_flag) == "1":
        logger.debug("Environment variables already loaded")
    load_dotenv()
    if logger.isEnabledFor(logging.DEBUG):  # Only in debug mode
        for key, value in os.environ.items():
                logger.debug(
        logger.info("Environment variables loaded")
class RateLimiter:
    def __init__(self, calls_per_second: float) -> None:
    async def wait(self) -> None:
        current_time = time.time()
            logger.debug(f"Rate limiter sleeping for {delay:.4f} seconds")
            time.sleep(delay)
        self.last_call_time = time.time()
    def wait_if_needed(self) -> None:
        if len(self.call_timestamps) >= self.calls_per_second:
            oldest_timestamp = min(self.call_timestamps)
        self.call_timestamps.append(time.time())
def extract_domain(url: str) -> str:
    parsed = urlparse(url)
        parsed = urlparse(f"https://{url}")
def extract_query_param(url: str, param: str) -> str | None:
    params = parse_qs(parsed.query)
    return params.get(param, [None])[0]

================
File: src/twat_search/__init__.py
================
    __all__.append("__version__")
    __all__.append("web")

================
File: src/twat_search/__main__.py
================
logging.basicConfig(
    handlers=[RichHandler(rich_tracebacks=True)],
logger = logging.getLogger(__name__)
console = Console()
SearchCLIType = TypeVar("SearchCLIType")
class TwatSearchCLI:
    def __init__(self) -> None:
            self.web: Any = web_cli.SearchCLI()
            logger.error(f"Web CLI not available: {e!s}")
            logger.error(
    def _cli_error(*args: Any, **kwargs: Any) -> int:
        console.print(
    def version() -> str:
def main() -> None:
    install(show_locals=True)
    ansi_decoder = AnsiDecoder()
    console = Console(theme=Theme({"prompt": "cyan", "question": "bold cyan"}))
    def display(lines, out):
        console.print(Group(*map(ansi_decoder.decode_line, lines)))
    fire.Fire(TwatSearchCLI, name="twat-search")
    main()

================
File: tests/unit/web/engines/__init__.py
================


================
File: tests/unit/web/engines/test_base.py
================
class TestSearchEngine(SearchEngine):
    async def search(self, query: str) -> list[SearchResult]:
            SearchResult(
                url=HttpUrl("https://example.com/test"),
register_engine(TestSearchEngine)
class DisabledTestSearchEngine(SearchEngine):
        raise NotImplementedError(msg)
register_engine(DisabledTestSearchEngine)
def test_search_engine_is_abstract() -> None:
    assert hasattr(SearchEngine, "__abstractmethods__")
    with pytest.raises(TypeError):
        SearchEngine(EngineConfig())  # type: ignore
def test_search_engine_name_class_var() -> None:
    assert hasattr(SearchEngine, "engine_code")
def test_engine_registration() -> None:
    class NewEngine(SearchEngine):
    returned_class = register_engine(NewEngine)
    engine_instance = get_engine("new_engine", EngineConfig())
    assert isinstance(engine_instance, NewEngine)
def test_get_engine_with_invalid_name() -> None:
    with pytest.raises(SearchError, match="Unknown search engine"):
        get_engine("nonexistent_engine", EngineConfig())
def test_get_engine_with_disabled_engine() -> None:
    config = EngineConfig(enabled=False)
    with pytest.raises(SearchError, match="is disabled"):
        get_engine("disabled_engine", config)
def test_get_engine_with_config() -> None:
    config = EngineConfig(
    engine = get_engine("test_engine", config)
def test_get_engine_with_kwargs() -> None:
    engine = get_engine("test_engine", EngineConfig(), **kwargs)

================
File: tests/unit/web/__init__.py
================


================
File: tests/unit/web/test_api.py
================
logging.basicConfig(level=logging.DEBUG)
T = TypeVar("T")
class MockSearchEngine(SearchEngine):
    def __init__(self, config: EngineConfig, **kwargs: Any) -> None:
        super().__init__(config, **kwargs)
        self.should_fail = kwargs.get("should_fail", False)
    async def search(self, query: str) -> list[SearchResult]:
            raise Exception(msg)
        result_count = self.kwargs.get("result_count", 1)
            SearchResult(
                url=HttpUrl(f"https://example.com/{i + 1}"),
            for i in range(result_count)
register_engine(MockSearchEngine)
def mock_config() -> Config:
    config = Config()
        "mock": EngineConfig(
async def setup_teardown() -> AsyncGenerator[None]:
    tasks = [t for t in asyncio.all_tasks() if t is not asyncio.current_task()]
    with contextlib.suppress(asyncio.CancelledError):
        await asyncio.gather(*tasks)
async def test_search_with_mock_engine(
    results = await search("test query", engines=["mock"], config=mock_config)
    assert len(results) == 2
    assert all(isinstance(result, SearchResult) for result in results)
    assert all(result.source == "mock" for result in results)
async def test_search_with_additional_params(
    results = await search(
    assert len(results) == 3
async def test_search_with_engine_specific_params(
    assert len(results) == 4
async def test_search_with_no_engines(setup_teardown: None) -> None:
    with pytest.raises(SearchError, match="No search engines configured"):
        await search("test query", engines=[])
async def test_search_with_failing_engine(
    assert len(results) == 0
async def test_search_with_nonexistent_engine(
    with pytest.raises(SearchError, match="No search engines could be initialized"):
        await search("test query", engines=["nonexistent"], config=mock_config)
async def test_search_with_disabled_engine(
        await search("test query", engines=["mock"], config=mock_config)

================
File: tests/unit/web/test_config.py
================
def test_engine_config_defaults() -> None:
    config = EngineConfig()
def test_engine_config_values() -> None:
    config = EngineConfig(
def test_config_defaults(isolate_env_vars: None) -> None:
    config = Config()
    assert isinstance(config.engines, dict)
    assert len(config.engines) == 0
def test_config_with_env_vars(monkeypatch: MonkeyPatch, env_vars_for_brave: None) -> None:
def test_config_with_direct_initialization() -> None:
    custom_config = Config(
        engines={"test_engine": EngineConfig(api_key="direct_key", enabled=True, default_params={"count": 5})},
def test_config_env_vars_override_direct_config(monkeypatch: MonkeyPatch) -> None:
    monkeypatch.setenv("BRAVE_API_KEY", "env_key")
        engines={"brave": EngineConfig(api_key="direct_key", enabled=True, default_params={"count": 5})},

================
File: tests/unit/web/test_exceptions.py
================
def test_search_error() -> None:
    exception = SearchError(error_message)
    assert str(exception) == error_message
    assert isinstance(exception, Exception)
def test_engine_error() -> None:
    exception = EngineError(engine_name, error_message)
    assert str(exception) == f"Engine '{engine_name}': {error_message}"
    assert isinstance(exception, SearchError)
def test_engine_error_inheritance() -> None:
        raise EngineError(msg, "Test error")
        if isinstance(e, EngineError):
def test_search_error_as_base_class() -> None:
        raise SearchError(msg)
        exceptions.append(e)
        raise EngineError(msg, "API key missing")
    assert len(exceptions) == 2
    assert isinstance(exceptions[0], SearchError)
    assert isinstance(exceptions[1], EngineError)
    assert "General search error" in str(exceptions[0])
    assert "Engine 'brave': API key missing" in str(exceptions[1])

================
File: tests/unit/web/test_models.py
================
def test_search_result_valid_data() -> None:
    url = HttpUrl("https://example.com")
    result = SearchResult(
    assert str(result.url) == "https://example.com/"
def test_search_result_with_optional_fields() -> None:
def test_search_result_invalid_url() -> None:
    with pytest.raises(ValidationError):
        SearchResult.model_validate(
def test_search_result_empty_fields() -> None:
        SearchResult(
def test_search_result_serialization() -> None:
    result_dict = result.model_dump()
    assert str(result_dict["url"]) == "https://example.com/"
    result_json = result.model_dump_json()
    assert isinstance(result_json, str)
def test_search_result_deserialization() -> None:
    result = SearchResult.model_validate(data)

================
File: tests/unit/web/test_utils.py
================
def rate_limiter() -> RateLimiter:
    return RateLimiter(calls_per_second=5)
def test_rate_limiter_init() -> None:
    limiter = RateLimiter(calls_per_second=10)
def test_rate_limiter_wait_when_not_needed(rate_limiter: RateLimiter) -> None:
    with patch("time.sleep") as mock_sleep:
        rate_limiter.wait_if_needed()
        mock_sleep.assert_not_called()
        for _ in range(3):  # 4 total calls including the one above
def test_rate_limiter_wait_when_needed(rate_limiter: RateLimiter) -> None:
    now = time.time()
    rate_limiter.call_timestamps = [now - 0.01 * i for i in range(rate_limiter.calls_per_second)]
    with patch("time.sleep") as mock_sleep, patch("time.time", return_value=now):
        mock_sleep.assert_called_once()
def test_rate_limiter_cleans_old_timestamps(rate_limiter: RateLimiter) -> None:
    with patch("time.time", return_value=now):
    assert len(rate_limiter.call_timestamps) == len(recent_stamps) + 1  # +1 for the new call
@pytest.mark.parametrize("calls_per_second", [1, 5, 10, 100])
def test_rate_limiter_with_different_rates(calls_per_second: int) -> None:
    limiter = RateLimiter(calls_per_second=calls_per_second)
        for _ in range(calls_per_second):
            limiter.wait_if_needed()
        patch("time.sleep") as mock_sleep,
        patch("time.time", return_value=time.time()),

================
File: tests/unit/__init__.py
================


================
File: tests/unit/mock_engine.py
================
class MockSearchEngine(SearchEngine):
    def __init__(self, config: EngineConfig, **kwargs: Any) -> None:
        super().__init__(config, **kwargs)
        self.should_fail = kwargs.get("should_fail", False)
    async def search(self, query: str) -> list[SearchResult]:
            raise Exception(msg)
        result_count = self.kwargs.get("result_count", 1)
            SearchResult(
                url=HttpUrl(f"https://example.com/{i + 1}"),
            for i in range(result_count)
register_engine(MockSearchEngine)

================
File: tests/web/test_bing_scraper.py
================
class MockSearchResult:
    def __init__(self, title: str, url: str, description: str = "") -> None:
def engine_config() -> EngineConfig:
    return EngineConfig(enabled=True)
def engine(engine_config: EngineConfig) -> BingScraperSearchEngine:
    return BingScraperSearchEngine(config=engine_config, num_results=5)
def mock_results() -> list[MockSearchResult]:
        MockSearchResult(
class TestBingScraperEngine:
    @patch("twat_search.web.engines.bing_scraper.BingScraper")
    def test_init(self, mock_BingScraper: MagicMock, engine: Any) -> None:
        mock_BingScraper.assert_not_called()
    async def test_search_basic(
        mock_instance = MagicMock()
        results = await engine.search("test query")
        assert len(results) == 2
        assert isinstance(results[0], SearchResult)
        assert str(results[0].url) == "https://example.com/1"
        mock_BingScraper.assert_called_once_with(max_retries=3, delay_between_requests=1.0)
        mock_instance.search.assert_called_once_with("test query", num_results=5)
    async def test_custom_parameters(self, mock_BingScraper: MagicMock) -> None:
        engine = BingScraperSearchEngine(
            config=EngineConfig(enabled=True),
        await engine.search("test query")
        mock_BingScraper.assert_called_once_with(max_retries=5, delay_between_requests=2.0)
        mock_instance.search.assert_called_once_with("test query", num_results=10)
    async def test_invalid_url_handling(self, mock_BingScraper: MagicMock, engine: BingScraperSearchEngine) -> None:
        assert len(results) == 1
    @patch("twat_search.web.api.search")
    async def test_bing_scraper_convenience_function(self, mock_search: AsyncMock) -> None:
            SearchResult(
                url=HttpUrl("https://example.com"),
        results = await bing_scraper(
        mock_search.assert_called_once()
    async def test_empty_query(self, mock_BingScraper: MagicMock, engine: BingScraperSearchEngine) -> None:
        with pytest.raises(EngineError) as excinfo:
            await engine.search("")
        assert "Search query cannot be empty" in str(excinfo.value)
    async def test_no_results(self, mock_BingScraper: MagicMock, engine: BingScraperSearchEngine) -> None:
        assert isinstance(results, list)
        assert len(results) == 0
    async def test_network_error(self, mock_BingScraper: MagicMock, engine: BingScraperSearchEngine) -> None:
        mock_instance.search.side_effect = ConnectionError("Network timeout")
        assert "Network error connecting to Bing" in str(excinfo.value)
    async def test_parsing_error(self, mock_BingScraper: MagicMock, engine: BingScraperSearchEngine) -> None:
        mock_instance.search.side_effect = RuntimeError("Failed to parse HTML")
        assert "Error parsing Bing search results" in str(excinfo.value)
    async def test_invalid_result_format(self, mock_BingScraper: MagicMock, engine: BingScraperSearchEngine) -> None:
        class InvalidResult:
            def __init__(self):
        mock_instance.search.return_value = [InvalidResult()]

================
File: tests/conftest.py
================
@pytest.fixture(autouse=True)
def isolate_env_vars(monkeypatch: MonkeyPatch) -> None:
    for env_var in list(os.environ.keys()):
        if any(env_var.endswith(suffix) for suffix in ["_API_KEY", "_ENABLED", "_DEFAULT_PARAMS"]):
            monkeypatch.delenv(env_var, raising=False)
    monkeypatch.setenv("_TEST_ENGINE", "true")
def env_vars_for_brave(monkeypatch: MonkeyPatch) -> None:
        sys.path.insert(0, str(Path(__file__).parent.parent))
        class MockBraveEngine(SearchEngine):
        register_engine(MockBraveEngine)
    monkeypatch.setenv("BRAVE_API_KEY", "test_brave_key")
    monkeypatch.setenv("BRAVE_ENABLED", "true")
    monkeypatch.setenv("BRAVE_DEFAULT_PARAMS", '{"count": 10}')
    monkeypatch.delenv("_TEST_ENGINE", raising=False)

================
File: tests/test_twat_search.py
================
def test_version():

================
File: .gitignore
================
*_autogen/
.DS_Store
__version__.py
__pycache__/
_Chutzpah*
_deps
_NCrunch_*
_pkginfo.txt
_Pvt_Extensions
_ReSharper*/
_TeamCity*
_UpgradeReport_Files/
!?*.[Cc]ache/
!.axoCover/settings.json
!.vscode/extensions.json
!.vscode/launch.json
!.vscode/settings.json
!.vscode/tasks.json
!**/[Pp]ackages/build/
!Directory.Build.rsp
.*crunch*.local.xml
.axoCover/*
.builds
.cr/personal
.fake/
.history/
.ionide/
.localhistory/
.mfractor/
.ntvs_analysis.dat
.paket/paket.exe
.sass-cache/
.vs/
.vscode
.vscode/*
.vshistory/
[Aa][Rr][Mm]/
[Aa][Rr][Mm]64/
[Bb]in/
[Bb]uild[Ll]og.*
[Dd]ebug/
[Dd]ebugPS/
[Dd]ebugPublic/
[Ee]xpress/
[Ll]og/
[Ll]ogs/
[Oo]bj/
[Rr]elease/
[Rr]eleasePS/
[Rr]eleases/
[Tt]est[Rr]esult*/
[Ww][Ii][Nn]32/
*_h.h
*_i.c
*_p.c
*_wpftmp.csproj
*- [Bb]ackup ([0-9]).rdl
*- [Bb]ackup ([0-9][0-9]).rdl
*- [Bb]ackup.rdl
*.[Cc]ache
*.[Pp]ublish.xml
*.[Rr]e[Ss]harper
*.a
*.app
*.appx
*.appxbundle
*.appxupload
*.aps
*.azurePubxml
*.bim_*.settings
*.bim.layout
*.binlog
*.btm.cs
*.btp.cs
*.build.csdef
*.cab
*.cachefile
*.code-workspace
*.coverage
*.coveragexml
*.d
*.dbmdl
*.dbproj.schemaview
*.dll
*.dotCover
*.DotSettings.user
*.dsp
*.dsw
*.dylib
*.e2e
*.exe
*.gch
*.GhostDoc.xml
*.gpState
*.ilk
*.iobj
*.ipdb
*.jfm
*.jmconfig
*.la
*.lai
*.ldf
*.lib
*.lo
*.log
*.mdf
*.meta
*.mm.*
*.mod
*.msi
*.msix
*.msm
*.msp
*.ncb
*.ndf
*.nuget.props
*.nuget.targets
*.nupkg
*.nvuser
*.o
*.obj
*.odx.cs
*.opendb
*.opensdf
*.opt
*.out
*.pch
*.pdb
*.pfx
*.pgc
*.pgd
*.pidb
*.plg
*.psess
*.publishproj
*.publishsettings
*.pubxml
*.pyc
*.rdl.data
*.rptproj.bak
*.rptproj.rsuser
*.rsp
*.rsuser
*.sap
*.sbr
*.scc
*.sdf
*.sln.docstates
*.sln.iml
*.slo
*.smod
*.snupkg
*.so
*.suo
*.svclog
*.tlb
*.tlh
*.tli
*.tlog
*.tmp
*.tmp_proj
*.tss
*.user
*.userosscache
*.userprefs
*.vbp
*.vbw
*.VC.db
*.VC.VC.opendb
*.VisualState.xml
*.vsp
*.vspscc
*.vspx
*.vssscc
*.xsd.cs
**/[Pp]ackages/*
**/*.DesktopClient/GeneratedArtifacts
**/*.DesktopClient/ModelManifest.xml
**/*.HTMLClient/GeneratedArtifacts
**/*.Server/GeneratedArtifacts
**/*.Server/ModelManifest.xml
*~
~$*
$tf/
AppPackages/
artifacts/
ASALocalRun/
AutoTest.Net/
Backup*/
BenchmarkDotNet.Artifacts/
bld/
BundleArtifacts/
ClientBin/
cmake_install.cmake
CMakeCache.txt
CMakeFiles
CMakeLists.txt.user
CMakeScripts
CMakeUserPresets.json
compile_commands.json
coverage*.info
coverage*.json
coverage*.xml
csx/
CTestTestfile.cmake
dlldata.c
DocProject/buildhelp/
DocProject/Help/*.hhc
DocProject/Help/*.hhk
DocProject/Help/*.hhp
DocProject/Help/*.HxC
DocProject/Help/*.HxT
DocProject/Help/html
DocProject/Help/Html2
ecf/
FakesAssemblies/
FodyWeavers.xsd
Generated_Code/
Generated\ Files/
healthchecksdb
install_manifest.txt
ipch/
Makefile
MigrationBackup/
mono_crash.*
nCrunchTemp_*
node_modules/
nunit-*.xml
OpenCover/
orleans.codegen.cs
Package.StoreAssociation.xml
paket-files/
project.fragment.lock.json
project.lock.json
publish/
PublishScripts/
rcf/
ScaffoldingReadMe.txt
ServiceFabricBackup/
StyleCopReport.xml
Testing
TestResult.xml
UpgradeLog*.htm
UpgradeLog*.XML
x64/
x86/
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# Distribution / packaging
!dist/.gitkeep

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/
.ruff_cache/

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# IDE
.idea/
.vscode/
*.swp
*.swo
*~

# OS
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Project specific
__version__.py
_private

================
File: .pre-commit-config.yaml
================
repos:
  - repo: https://github.com/asottile/pyupgrade
    rev: v3.19.1
    hooks:
      - id: pyupgrade
        args: [--py311-plus]
  - repo: https://github.com/pycqa/isort
    rev: 6.0.0
    hooks:
      - id: isort
        args: ['--profile=black', '--line-length=120']
  - repo: https://github.com/MarcoGorelli/absolufy-imports
    rev: v0.3.1
    hooks:
      - id: absolufy-imports
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v5.0.0
    hooks:
      - id: check-yaml
      - id: requirements-txt-fixer
  - repo: https://github.com/asottile/add-trailing-comma
    rev: v3.1.0
    hooks:
      - id: add-trailing-comma
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.9.7
    hooks:
      - id: ruff
        types: [python]
        additional_dependencies: ['typing-extensions']
        args: [
          '--fix',
          '--ignore=ARG001,ARG002,ARG004,ARG005,B904,C901,DTZ005,E501,F401,F811,FBT001,FBT002,FBT003,I001,ISC001,N803,PLR0911,PLR0912,PLR0913,PLR0915,PLR2004,S311,S603,S607,T201,PLR1714'
        ]
      - id: ruff-format
        args: [--respect-gitignore]
        types: [python]

================
File: CHANGELOG.md
================
# Changelog

All notable changes to this project will be documented in this file.

## Changes by date

### 2025-02-26

#### Changed

- Refactored the CLI to import engine functions more safely, handling missing dependencies.
- Updated the CLI to use a dictionary to map the registered engines to their convenience functions, making it easier to call functions.
- Standardized engine names to use underscores internally.
- Modified the `_parse_engines` method in the CLI to handle special strings ("free", "best", "all") for selecting groups of engines.
- Updated the `q` method in the CLI to use unified parameters and to control verbose output.
- Updated the `info` method in the CLI to use unified parameters, to list engines, and to provide detailed information about specific engines.
- Updated `_display_results` in the CLI to handle multiple results per engine and optional plain-text output (no table, just URLs).
- Updated the `_display_json_results` function in the CLI to create JSON output.
- Updated the `config` class to parse environment variables for engines using a regular expression, addressing issues with engine configuration.
- Updated various engine implementations (Brave, Tavily, You, Perplexity, SerpApi) to use unified parameters and improve URL handling.

#### Added

- Implemented `bing_scraper` convenience function.
- Added comprehensive tests to verify the correct behavior of the `bing_scraper` convenience function.
- Implemented the `WebScoutEngine` class, providing basic search functionality.
- Added new search engine integrations:
    - `google_scraper.py`: Implemented a `GoogleScraperEngine` and a `google_scraper` convenience function.
    - `searchit.py`: Implemented several search engines using the `searchit` library: `GoogleSearchitEngine`, `YandexSearchitEngine`, `QwantSearchitEngine`, `BingSearchitEngine`.
    - `anywebsearch.py`: Implemented multiple search engines using the `anywebsearch` library: `GoogleAnyWebSearchEngine`, `BingAnyWebSearchEngine`, `BraveAnyWebSearchEngine`, `QwantAnyWebSearchEngine`, `YandexAnyWebSearchEngine`.
    - `hasdata.py`: Implemented `HasDataGoogleEngine` and `HasDataGoogleLightEngine`.
- Added tests for `bing_scraper`.

### 2025-02-25

#### Added

- Created initial project structure and implemented several search engine integrations (Brave, Google, Tavily, Perplexity, You.com).
- Developed core functionality, including a command-line interface (CLI) and asynchronous search capabilities.
- Implemented configuration management and exception handling.
- Implemented rate limiting utility.
- Added Pydantic models for data validation.
- Added unit tests for the models, configuration, utility functions, exceptions, and the base search engine class.

#### Changed

- Updated the `config.py` file to correctly import BaseSettings from the pydantic-settings package.
- Updated the `pyproject.toml` file to add pydantic-settings as a dependency.
- Updated the example usage in `example.py`.
- Completed the implementation of the web search functionality as specified in TODO.md.
- Planned comprehensive tests for the package.

### 2025-02-25 - Initial Development

- Created initial project structure and files.
- Created a preliminary TODO.md, PROGRESS.md, and research.txt.

---

================
File: cleanup.py
================
LOG_FILE = Path("CLEANUP.txt")
os.chdir(Path(__file__).parent)
def new() -> None:
    if LOG_FILE.exists():
        LOG_FILE.unlink()
def prefix() -> None:
    readme = Path(".cursor/rules/0project.mdc")
    if readme.exists():
        log_message("\n=== PROJECT STATEMENT ===")
        content = readme.read_text()
        log_message(content)
def suffix() -> None:
    todo = Path("TODO.md")
    if todo.exists():
        log_message("\n=== TODO.md ===")
        content = todo.read_text()
def log_message(message: str) -> None:
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with LOG_FILE.open("a") as f:
        f.write(log_line)
def run_command(cmd: list[str], check: bool = True) -> subprocess.CompletedProcess:
        result = subprocess.run(
            log_message(result.stdout)
        log_message(f"Command failed: {' '.join(cmd)}")
        log_message(f"Error: {e.stderr}")
        return subprocess.CompletedProcess(cmd, 1, "", str(e))
def check_command_exists(cmd: str) -> bool:
        subprocess.run(["which", cmd], check=True, capture_output=True)
class Cleanup:
    def __init__(self) -> None:
        self.workspace = Path.cwd()
    def _check_required_files(self) -> bool:
            if not (self.workspace / file).exists():
                log_message(f"Error: {file} is missing")
    def _venv(self) -> None:
        log_message("Setting up virtual environment")
            run_command(["uv", "venv"])
            if venv_path.exists():
                os.environ["VIRTUAL_ENV"] = str(self.workspace / ".venv")
                log_message("Virtual environment created and activated")
                log_message(
            log_message(f"Failed to create virtual environment: {e}")
    def _install(self) -> None:
        log_message("Installing package with all extras")
            self._venv()
            run_command(["uv", "pip", "install", "-e", ".[test,dev]"])
            log_message("Package installed successfully")
            log_message(f"Failed to install package: {e}")
    def status(self) -> None:
        prefix()  # Add README.md content at start
        _print_header("Current Status")
        self._check_required_files()
        _generate_tree()
        result = run_command(["git", "status"], check=False)
        _print_header("Environment Status")
        self._install()
        _run_checks()
        suffix()  # Add TODO.md content at end
    def venv(self) -> None:
        _print_header("Virtual Environment Setup")
    def install(self) -> None:
        _print_header("Package Installation")
    def update(self) -> None:
        self.status()
        if _git_status():
            log_message("Changes detected in repository")
                run_command(
                run_command(["pre-commit", "run", "--all-files"])
                run_command(["git", "add", "-A", "."])
                run_command(["git", "commit", "-m", commit_msg])
                log_message("Changes committed successfully")
                log_message(f"Failed to commit changes: {e}")
            log_message("No changes to commit")
    def push(self) -> None:
        _print_header("Pushing Changes")
            run_command(["git", "push"])
            log_message("Changes pushed successfully")
            log_message(f"Failed to push changes: {e}")
def _generate_tree(self) -> None:
    if not check_command_exists("tree"):
        rules_dir = Path(".cursor/rules")
        rules_dir.mkdir(parents=True, exist_ok=True)
        tree_result = run_command(
        with open(rules_dir / "filetree.mdc", "w") as f:
            f.write("---\ndescription: File tree of the project\nglobs: \n---\n")
            f.write(tree_text)
        log_message("\nProject structure:")
        log_message(tree_text)
        log_message(f"Failed to generate tree: {e}")
def _git_status(self) -> bool:
    result = run_command(["git", "status", "--porcelain"], check=False)
    return bool(result.stdout.strip())
def _run_checks(self) -> None:
    log_message("Running code quality checks")
        log_message(">>> Running code fixes...")
        log_message(">>>Running type checks...")
        run_command(["python", "-m", "mypy", "src", "tests"], check=False)
        log_message(">>> Running tests...")
        run_command(["python", "-m", "pytest", "tests"], check=False)
        log_message("All checks completed")
        log_message(f"Failed during checks: {e}")
def _print_header(self, message: str) -> None:
    log_message(f"\n=== {message} ===")
def repomix(
            cmd.append("--compress")
            cmd.append("--remove-empty-lines")
            cmd.append("-i")
            cmd.append(ignore_patterns)
        cmd.extend(["-o", output_file])
        run_command(cmd)
        log_message(f"Repository content mixed into {output_file}")
        log_message(f"Failed to mix repository: {e}")
def print_usage() -> None:
    log_message("Usage:")
    log_message("  cleanup.py status   # Show current status and run all checks")
    log_message("  cleanup.py venv     # Create virtual environment")
    log_message("  cleanup.py install  # Install package with all extras")
    log_message("  cleanup.py update   # Update and commit changes")
    log_message("  cleanup.py push     # Push changes to remote")
def main() -> None:
    new()  # Clear log file
    if len(sys.argv) < 2:
        print_usage()
        sys.exit(1)
    cleanup = Cleanup()
            cleanup.status()
            cleanup.venv()
            cleanup.install()
            cleanup.update()
            cleanup.push()
        log_message(f"Error: {e}")
    repomix()
    print(LOG_FILE.read_text(encoding="utf-8"))
    main()

================
File: LICENSE
================
MIT License

Copyright (c) 2025 Adam Twardoch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

================
File: NEXTENGINES.md
================
# Next backends and engines to implement

## https://github.com/Sanix-Darker/falla

Implement all engines as `*-falla` from this. 

Token Usage:
GitHub Tokens: 7766
LLM Input Tokens: 0
LLM Output Tokens: 0
Total Tokens: 7766

FileTree:
.gitignore
README.md
app/__init__.py
app/core/Aol.py
app/core/Ask.py
app/core/Bing.py
app/core/DogPile.py
app/core/DuckDuckGo.py
app/core/Falla.py
app/core/Gibiru.py
app/core/Google.py
app/core/Mojeek.py
app/core/Qwant.py
app/core/SearchEncrypt.py
app/core/StartPage.py
app/core/Yahoo.py
app/core/Yandex.py
app/core/__init__.py
app/main.py
app/settings.py
app/utils.py
example.config.txt
requirements.txt
tests/test.py

Analysis:
.gitignore
```.gitignore
config.txt
*__pycache__
.idea
.vscode
.history
*.json
*.log
*.pyc
```
README.md
# FAllA

A search-engine-cli-scraper for more than 15 search engines, including Google. duckduckgo, Bing, Ask, etc...
<br>
**NOTE:** For educationnal purpose, am not responsible of the bad use of this tool !

## Requirements
- Python (3.x)
- Docker-CE (Not required for all search-engine, just few of them)

## How to install

- You need to install all requirements :
```shell-script
pip3 install -r requirements.txt
```
- Install geckodriver :
```shell-script
# For linux users
sudo apt install firefox-geckodriver

# For other OS's users, please check releases on https://github.com/mozilla/geckodriver/releases
```

- Pull and run the splash-scrap module from docker-hub (Some of search engine need this):
```shell-script
docker run -p 8050:8050 scrapinghub/splash
```

- Replace `example.config.txt` by `config.txt` and provide the running IP for the splash-scrap

## How to launch

How to use Falla:
```shell-script
usage: main.py [-h] [-e ENGINE] [-q QUERY]

optional arguments:
  -h, --help            show this help message and exit
  -e ENGINE, --engine ENGINE
                        The search engine
  -q QUERY, --query QUERY
                        The query text
```

- To list all search-engine:
```shell-script
$ python3 -m app.main
# output
[+] Falla [the search-engine-scraper]
[+] Listing search-Engines
[+] > google
[+] > bing
[+] > aol
[+] > dogpile
[+] > falla
[+] > ask
[+] > qwant
[+] > duckduckgo
[+] > mojeek
[+] > gibiru
[+] > yandex
[+] > yahoo
[+] > searchencrypt
[+] > iem
[+] > kallasearch
[+] > wosx
```

- To search something:
```shell-script
$ python3 -m app.main -e google -q "sanix darker"
# output
```
<img src="./images/falla.png">
## Author

- Sanix-darker
app/__init__.py
```.py

```
app/core/Aol.py
```.py
# Falla-Aol
# -*- encoding: utf-8 -*-
# Sanix-darker

from app.core.Falla import Falla


class Aol(Falla):
    def __init__(self):
        self.try_it = 0
        self.max_retry = 3
        self.source = "Aol"
        self.mode = "requests"
        self.results_box = "//div[@id='web']"
        self.each_element = {
            "tag": "li"
        }
        self.href = {
            "tag": "a",
            "type": "attribute",
            "key": "href",
            "child": {}
        }
        self.title = {
            "tag": "a",
            "type": "text"
        }
        self.cite = {
            "tag": "div:compText",
            "child": {
                "tag": "p",
                "type": "text"
            }
        }

    def search(self, search_text, pages=""):
        url = "https://search.aol.com/aol/search?q=" + search_text.replace(" ", "+") + pages
        print("[+] Searching results for '" + url.split("=")[1].replace("+", " ") +
              "' on '" + self.source + "' :\n")

        return self.fetch(url)

# a = Aol()
# print(a.search("un avion"))

```
app/core/Ask.py
```.py
# Falla-Ask
# -*- encoding: utf-8 -*-
# Sanix-darker

from app.core.Falla import Falla


class Ask(Falla):
    def __init__(self):
        self.try_it = 0
        self.max_retry = 3
        self.source = "Ask"
        self.mode = "requests"
        self.results_box = "//div[@class='l-mid-content']"
        self.each_element = {
            "tag": "div",
            "attr": {"class": "PartialSearchResults-item"}
        }
        self.href = {
            "tag": "a:PartialSearchResults-item-title-link",
            "type": "attribute",
            "key": "href",
            "child": {}
        }
        self.title = {
            "tag": "a:PartialSearchResults-item-title-link",
            "type": "text"
        }
        self.cite = {
            "tag": "p:PartialSearchResults-item-abstract",
            "type": "text"
        }

    def search(self, search_text, pages=""):
        url = "https://www.ask.com/web?q=" + search_text.replace(" ", "+") + pages
        print("[+] Searching results for '" + url.split("=")[1].replace("+", " ") +
              "' on '" + self.source + "' :\n")

        return self.fetch(url)

# ak = Ask()
# print(ak.search("un avion"))

```
app/core/Bing.py
```.py
# Falla-Bing
# -*- encoding: utf-8 -*-
# Sanix-darker

from selenium import webdriver
from selenium.webdriver.firefox.options import Options

from app.core.Falla import Falla


class Bing(Falla):
    def __init__(self):
        self.source = "Bing"
        self.mode = "splash_scrap"
        self.try_it = 0
        self.max_retry = 3
        self.results_box = "//ol[@id='b_results']"
        self.each_element = {
            "tag": "li",
            "attr": {"class": "b_algo"}
        }
        self.href = {
            "tag": "a",
            "type": "attribute",
            "key": "href",
            "child": {}
        }
        self.title = {
            "tag": "h2",
            "type": "text",
            "child": {}
        }
        self.cite = {
            "tag": "div:b_caption",
            "child": {
                "tag": "p",
                "type": "text"
            }
        }

    def search(self, search_text, pages=""):
        url = "https://www.bing.com/search?q=" + search_text.replace(" ", "+") + pages
        print("[+] Searching results for '" + url.split("=")[1].replace("+", " ") +
              "' on '" + self.source + "' :\n")

        return self.fetch(url)

# b = Bing()
# print(b.search("un avion"))

```
app/core/DogPile.py
```.py
# Falla-DogPile
# -*- encoding: utf-8 -*-
# Sanix-darker

from app.core.Falla import Falla


class DogPile(Falla):
    def __init__(self):
        self.try_it = 0
        self.max_retry = 3
        self.source = "DogPile"
        self.mode = "requests"
        self.results_box = "//div[@class='mainline-results']"
        self.each_element = {
            "tag": "div",
            "attr": {"class": "web-bing__result"}
        }
        self.href = {
            "tag": "a:web-bing__title",
            "type": "attribute",
            "key": "href",
            "child": {}
        }
        self.title = {
            "tag": "a:web-bing__title",
            "type": "text"
        }
        self.cite = {
            "tag": "span:web-bing__description",
            "type": "text"
        }

    def search(self, search_text, pages=""):
        url = "https://www.dogpile.com/serp?q=" + search_text.replace(" ", "+") + pages
        print("[+] Searching results for '" + url.split("=")[1].replace("+", " ") +
              "' on '" + self.source + "' :\n")

        return self.fetch(url)

# dp = DogPile()
# print(dp.search("un avion"))

```
app/core/DuckDuckGo.py
```.py
# Falla-DuckDuckGo
# -*- encoding: utf-8 -*-
# Sanix-darker

from selenium import webdriver
from selenium.webdriver.firefox.options import Options

from app.core.Falla import Falla


class DuckDuckGo(Falla):
    def __init__(self):
        self.option = Options()
        self.option.headless = True
        self.driver = webdriver.Firefox(options=self.option)

        self.source = "DuckDuckGo"
        self.mode = "selenium"
        self.try_it = 0
        self.max_retry = 3
        self.results_box = "//div[@id='links']"
        self.each_element = {
            "tag": "div",
            "attr": {"class": "result__body"}
        }
        self.href = {
            "tag": "a:result__a",
            "type": "attribute",
            "key": "href",
            "child": {}
        }
        self.title = {
            "tag": "a:result__a",
            "type": "text",
            "child": {}
        }
        self.cite = {
            "tag": "div:result__snippet",
            "type": "text",
            "child": {}
        }

    def search(self, search_text, pages=""):
        url = "https://duckduckgo.com/?q=" + search_text.replace(" ", "+") + pages
        print("[+] Searching results for '" + url.split("=")[1].replace("+", " ") +
              "' on '" + self.source + "' :\n")

        return self.fetch(url)

# d = DuckDuckGo()
# print(d.search("un avion"))

```
app/core/Falla.py
```.py
# Falla
# -*- encoding: utf-8 -*-
# Sanix-darker

import json
import sys
import time
import traceback
import requests
from bs4 import BeautifulSoup

from app.settings import *


class Falla:
    def __init__(self, results_box, each_element, href, title, cite):
        self.driver = None
        self.results_box = results_box
        self.each_element = each_element
        self.href = href
        self.title = title
        self.cite = cite
        self.try_it = 0
        self.max_retry = 4
        self.source = "Falla"
        self.mode = "requests"

        self.option = None
        self.option.headless = None
        self.driver = None

    def get_trace(self):
        """
        This method will just print the Traceback after an error occur
        """
        print("Exception in code:")
        print("-" * 60)
        traceback.print_exc(file=sys.stdout)
        print("-" * 60)

    def get_element_from_type(self, to_return, the_filter=None):
        """
        A parse-method to extract from parse_entry_point an element as attribute or text

        to_return: The BeautifulSoup object
        the_filter: The Json-Filter
        """
        if the_filter["type"] == "text":
            to_return = to_return.getText()
        elif the_filter["type"] == "attribute":
            try:
                to_return = to_return[the_filter["key"]]
            except Exception:
                self.get_trace()
                to_return = ""
        else:
            print("[x] Error, specify a valid type !")

        return to_return

    def get_tag(self, element, tag):
        """
        This method will extract the tag having a attriute or not

        element: The Beautifull-Soup object
        tag: The Json tag (can be a class or other)
        """
        if ":" in tag:
            return element.find(tag.split(":")[0], {"class": tag.split(":")[1]})
        else:
            return element.find(tag)

    def parse_entry_point(self, element, the_filter):
        """
        This method will extract the href, the title and the cite in result

        element: The Beautifull-Soup Object
        the_filter: The JSON object for the Filter
        """
        to_return = self.get_tag(element, the_filter["tag"])

        if "type" in the_filter:
            to_return = self.get_element_from_type(to_return, the_filter)
        else:
            if bool(the_filter["child"]):  # There are some child
                to_return = self.get_tag(to_return, the_filter["child"]["tag"])

                to_return = self.get_element_from_type(to_return, the_filter["child"])
            else:
                print("[x] Malformed filter !")

        return ' '.join(str(to_return).split())

    def get_each_elements(self, soup):
        """
        Get all elements list from Beautifull-Soup object

        soup: The Beautiful-Soup object
        """
        fetchs = []
        if "attr" in self.each_element:
            if self.each_element["attr"] is not None:
                fetchs = soup.findAll(self.each_element["tag"], self.each_element["attr"])
            else:
                fetchs = soup.findAll(self.each_element["tag"])
        else:
            fetchs = soup.findAll(self.each_element["tag"])

        return fetchs

    def scrapy_splash_request(self, to_fetch_url):
        """
        This method is responsible for scrapping a html_content from a url using splash-scrap

        to_fetch_url: The url of the website
        """
        json_data = {
            "response_body": False,
            "lua_source": "function main(splash, args)\r\n  assert(splash:go(args.url))\r\n  assert(splash:wait("
                          "0.5))\r\n  return {\r\n    html = splash:html(),\r\n    png = splash:png(),\r\n    har = "
                          "splash:har(),\r\n  }\r\nend",
            "url": to_fetch_url,
            "html5_media": False,
            "save_args": [],
            "viewport": "1024x768",
            "http_method": "GET",
            "resource_timeout": 0,
            "render_all": False,
            "png": 1,
            "har": 1,
            "timeout": 90,
            "request_body": False,
            "load_args": {},
            "html": 1,
            "images": 1,
            "wait": 0.7
        }

        r = requests.post(SPLASH_SCRAP_URL + "/execute", json=json_data)
        html_string = json.loads(r.content.decode())["html"]

        return html_string

    def get_html_content(self, url):
        """
        This method will check the mode of fetching and proceed

        url: The url of the target
        """
        html_content = ""
        if self.mode == "selenium":
            self.driver.get(url)
            self.driver.implicitly_wait(10)  # in seconds

            element = self.driver.find_element_by_xpath(self.results_box)
            html_content = element.get_attribute('outerHTML')
        elif self.mode == "splash_scrap":
            html_content = self.scrapy_splash_request(url)

        elif self.mode == "requests":
            r = requests.get(url, headers={"Upgrade-Insecure-Requests": "1",
                                           "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, "
                                                         "like Gecko) Chrome/80.0.3987.100 Safari/537.36",
                                           "Sec-Fetch-Dest": "document",
                                           "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,"
                                                     "image/webp,image/apng,*/*;q=0.8,"
                                                     "application/signed-exchange;v=b3;q=0.9 "
                                           })
            html_content = r.content.decode()

        return html_content

    def fetch(self, url):
        """
        The main method for fetching results from url

        url: The url of the target
        """
        html_content = self.get_html_content(url)
        soup = BeautifulSoup(html_content, 'html.parser')
        fetchs = self.get_each_elements(soup)

        results = []
        # print("fetchs: ", fetchs)
        # print("self.parse_entry_point(elt, self.href): ", self.parse_entry_point(fetchs[0], self.href))
        for elt in fetchs:
            try:
                element = {
                    "source": self.source,
                    "href": self.parse_entry_point(elt, self.href),  # elt.find("a")["href"]
                    "title": self.parse_entry_point(elt, self.title),  # str(elt.find("a").find("h3").getText())
                    "cite": self.parse_entry_point(elt, self.cite)  # str(elt.find("a").find("cite").getText())
                }
                results.append(element)
            except Exception:
                self.get_trace()

        if len(results) == 0 and self.try_it < self.max_retry:
            self.try_it += 1
            time.sleep(0.5)
            print("[+] try: ", self.try_it)
            self.fetch(url)

        if self.mode == "selenium":
            self.driver.quit()

        return json.dumps(results, ensure_ascii=False)

```
app/core/Gibiru.py
```.py
# Falla-Gibiru
# -*- encoding: utf-8 -*-
# Sanix-darker

from app.core.Falla import Falla


class Gibiru(Falla):
    def __init__(self):
        self.source = "Gibiru"
        self.mode = "splash_scrap"
        self.try_it = 0
        self.max_retry = 3
        self.results_box = "//div[@class='gsc-resultsRoot']"
        self.each_element = {
            "tag": "div",
            "attr": {"class": "gs-webResult"}
        }
        self.href = {
            "tag": "a:gs-title",
            "type": "attribute",
            "key": "data-ctorig",
            "child": {}
        }
        self.title = {
            "tag": "a:gs-title",
            "type": "text",
            "child": {}
        }
        self.cite = {
            "tag": "div:gs-snippet",
            "type": "text",
            "child": {}
        }

    def search(self, search_text, pages=""):
        url = "https://gibiru.com/results.html?q=" + search_text.replace(" ", "+") + pages
        print("[+] Searching results for '" + url.split("=")[1].replace("+", " ") +
              "' on '" + self.source + "' :\n")

        return self.fetch(url)

# gi = Gibiru()
# print(gi.search("un avion"))

```
app/core/Google.py
```.py
# Falla-Google
# -*- encoding: utf-8 -*-
# Sanix-darker

import json
import time
from selenium import webdriver
from selenium.webdriver.firefox.options import Options

from app.core.Falla import Falla


class Google(Falla):
    def __init__(self):
        self.option = Options()
        self.option.headless = True
        self.driver = webdriver.Firefox(options=self.option)

        self.try_it = 0
        self.max_retry = 3
        self.source = "Google"
        self.mode = "selenium"
        self.results_box = "//div[@id='search']"
        self.each_element = {
            "tag": "div",
            "attr": {"class": "g"}
        }
        self.href = {
            "tag": "a",
            "type": "attribute",
            "key": "href",
            "child": {}
        }
        self.title = {
            "tag": "a",
            "child": {
                "tag": "h3",
                "type": "text"
            }
        }
        self.cite = {
            "tag": "a",
            "child": {
                "tag": "cite",
                "type": "text"
            }
        }

    def search(self, search_text, pages=""):

        base_url = "https://www.google.com/search?q=" + search_text.replace(" ", "+") + pages
        print("[+] Searching results for '" + base_url.split("=")[1].replace("+", " ") +
              "' on '" + self.source + "' :\n")

        # results = json.loads(self.fetch(base_url))
        #
        # if pages > 1:
        #     for nb_page in range(pages):
        #         url = base_url + "&start=" + str(nb_page + 1) + "0"
        #         time.sleep(1)
        #         results = results + self.fetch(url)
        #
        # results = json.dumps(results)

        return self.fetch(base_url)


# g = Google()
# print(g.search("un avion"))

```
app/core/Mojeek.py
```.py
# Falla-Mojeek
# -*- encoding: utf-8 -*-
# Sanix-darker

from app.core.Falla import Falla


class Mojeek(Falla):
    def __init__(self):
        self.try_it = 0
        self.max_retry = 3
        self.source = "Mojeek"
        self.mode = "requests"
        self.results_box = "//ul[@class='results-standard']"
        self.each_element = {
            "tag": "li"
        }
        self.href = {
            "tag": "a:ob",
            "type": "attribute",
            "key": "href",
            "child": {}
        }
        self.title = {
            "tag": "a:ob",
            "type": "text"
        }
        self.cite = {
            "tag": "p:s",
            "type": "text"
        }

    def search(self, search_text, pages=""):
        url = "https://www.mojeek.com/search?q=" + search_text.replace(" ", "+") + pages
        print("[+] Searching results for '" + url.split("=")[1].replace("+", " ") +
              "' on '" + self.source + "' :\n")

        return self.fetch(url)

# mk = Mojeek()
# print(mk.search("un avion"))

```
app/core/Qwant.py
```.py
# Falla-Qwant
# -*- encoding: utf-8 -*-
# Sanix-darker

from selenium import webdriver
from selenium.webdriver.firefox.options import Options

from app.core.Falla import Falla


class Qwant(Falla):
    def __init__(self):
        self.option = Options()
        self.option.headless = True
        self.driver = webdriver.Firefox(options=self.option)

        self.try_it = 0
        self.max_retry = 3
        self.source = "Qwant"
        self.mode = "selenium"
        self.results_box = "//div[@class='result_fragment']"
        self.each_element = {
            "tag": "div",
            "attr": {"class": "result--web"}
        }
        self.href = {
            "tag": "a:result--web--link",
            "type": "attribute",
            "key": "href",
            "child": {}
        }
        self.title = {
            "tag": "span:result--web--title",
            "type": "text",
            "child": {}
        }
        self.cite = {
            "tag": "div:result--web",
            "child": {
                "tag": "p",
                "type": "text"
            }
        }

    def search(self, search_text, pages=""):
        url = "https://www.qwant.com/?q=" + search_text.replace(" ", "+") + pages
        print("[+] Searching results for '" + url.split("=")[1].replace("+", " ") +
              "' on '" + self.source + "' :\n")

        return self.fetch(url)

# qw = Qwant()
# print(qw.search("un avion"))

```
app/core/SearchEncrypt.py
```.py
# Falla-SearchEncrypt
# -*- encoding: utf-8 -*-
# Sanix-darker

from app.core.Falla import Falla


class SearchEncrypt(Falla):
    def __init__(self):
        self.try_it = 0
        self.max_retry = 3
        self.source = "SearchEncrypt"
        self.mode = "requests"
        self.results_box = "//section[@class='serp__results']"
        self.each_element = {
            "tag": "div",
            "attr": {"class": "web-result"}
        }
        self.href = {
            "tag": "a:web-result__link",
            "type": "attribute",
            "key": "href",
            "child": {}
        }
        self.title = {
            "tag": "a:web-result__link",
            "type": "text",
            "child": {}
        }
        self.cite = {
            "tag": "p:web-result__description",
            "child": {
                "tag": "span",
                "type": "text"
            }
        }

    def search(self, search_text, pages=""):
        url = "https://www.searchencrypt.com/search/?q=" + search_text.replace(" ", "+") + pages
        print("[+] Searching results for '" + url.split("=")[1].replace("+", " ") +
              "' on '" + self.source + "' :\n")

        return self.fetch(url)

# se = SearchEncrypt()
# print(se.search("un avion"))

```
app/core/StartPage.py
```.py
# Falla-StartPage
# -*- encoding: utf-8 -*-
# Sanix-darker

from app.core.Falla import Falla


class StartPage(Falla):
    def __init__(self):
        self.try_it = 0
        self.max_retry = 3
        self.source = "StartPage"
        self.mode = "requests"
        self.results_box = "//section[@id='w-gl--default']"
        self.each_element = {
            "tag": "div",
            "attr": {"class": "w-gl__result"}
        }
        self.href = {
            "tag": "a:w-gl__result-title",
            "type": "attribute",
            "key": "href",
            "child": {}
        }
        self.title = {
            "tag": "a:w-gl__result-title",
            "child": {
                "tag": "h3",
                "type": "text"
            }
        }
        self.cite = {
            "tag": "p:w-gl__description",
            "type": "text",
            "child": {}
        }

    def search(self, search_text, pages=""):
        url = "https://www.startpage.com/sp/search?q=" + search_text.replace(" ", "+") + pages
        print("[+] Searching results for '" + url.split("=")[1].replace("+", " ") +
              "' on '" + self.source + "' :\n")

        return self.fetch(url)

# sp = StartPage()
# print(sp.search("un avion"))

```
app/core/Yahoo.py
```.py
# Falla-Yahoo
# -*- encoding: utf-8 -*-
# Sanix-darker

from selenium import webdriver
from selenium.webdriver.firefox.options import Options

from app.core.Falla import Falla


class Yahoo(Falla):
    def __init__(self):
        self.option = Options()
        self.option.headless = True
        self.driver = webdriver.Firefox(options=self.option)

        self.try_it = 0
        self.max_retry = 3
        self.source = "Yahoo"
        self.mode = "selenium"
        self.results_box = "//div[@id='web']"
        self.each_element = {
            "tag": "li"
        }
        self.href = {
            "tag": "a",
            "type": "attribute",
            "key": "href",
            "child": {}
        }
        self.title = {
            "tag": "a",
            "type": "text"
        }
        self.cite = {
            "tag": "div:compText",
            "child": {
                "tag": "p",
                "type": "text"
            }
        }

    def search(self, search_text, pages=""):
        url = "https://search.yahoo.com/search?p=" + search_text.replace(" ", "+") + pages
        print("[+] Searching results for '" + url.split("=")[1].replace("+", " ") +
              "' on '" + self.source + "' :\n")

        return self.fetch(url)

# y = Yahoo()
# print(y.search("un avion"))

```
app/core/Yandex.py
```.py
# Falla-Yandex
# -*- encoding: utf-8 -*-
# Sanix-darker

from selenium import webdriver
from selenium.webdriver.firefox.options import Options

from app.core.Falla import Falla


class Yandex(Falla):
    def __init__(self):
        self.option = Options()
        self.option.headless = True
        self.driver = webdriver.Firefox(options=self.option)

        self.try_it = 0
        self.max_retry = 3
        self.source = "Yandex"
        self.mode = "selenium"
        self.results_box = "//div[@class='content__left']"
        self.each_element = {
            "tag": "li",
            "attr": {"class": "serp-item"}
        }
        self.href = {
            "tag": "a:link",
            "type": "attribute",
            "key": "href",
            "child": {}
        }
        self.title = {
            "tag": "div:organic__url-text",
            "type": "text"
        }
        self.cite = {
            "tag": "div:organic__content-wrapper",
            "child": {
                "tag": "div:text-container",
                "type": "text"
            }
        }

    def search(self, search_text, pages=""):
        url = "https://yandex.com/search/?text=" + search_text.replace(" ", "+") + pages
        print("[+] Searching results for '" + url.split("=")[1].replace("+", " ") +
              "' on '" + self.source + "' :\n")

        return self.fetch(url)

# y = Yandex()
# print(y.search("un avion"))

```
app/core/__init__.py
```.py
from app.core.Aol import Aol
from app.core.Ask import Ask
from app.core.Bing import Bing
from app.core.DogPile import DogPile
from app.core.DuckDuckGo import DuckDuckGo
from app.core.Gibiru import Gibiru
from app.core.Mojeek import Mojeek
from app.core.Google import Google
from app.core.Qwant import Qwant
from app.core.SearchEncrypt import SearchEncrypt
from app.core.StartPage import StartPage
from app.core.Yahoo import Yahoo
from app.core.Yandex import Yandex

```
app/main.py
```.py
# main-script
# -*- encoding: utf-8 -*-
#  _____ _    _     _        _    
# |  ___/ \  | |   | |      / \   
# | |_ / _ \ | |   | |     / _ \  
# |  _/ ___ \| |___| |___ / ___ \ 
# |_|/_/   \_\_____|_____/_/   \_\
#
# By Sanix-darker
__version__ = 0.1
__author__ = "Sanix-darker"

import argparse
from app.utils import *

if __name__ == "__main__":
    # Initialize the arguments
    # python3 -m app.main # Search-Engine list
    # python3 -m app.main -e aol -q "sanix darker"
    #
    # python3 -m app.main -e google -q "sanix darker" -p "&start=10"
    prs = argparse.ArgumentParser()
    prs.add_argument('-e', '--engine', help='The search engine', type=str, default="google")
    prs.add_argument('-q', '--query', help='The query text', type=str)
    prs.add_argument('-p', '--page', help='Number of pages to fetch', type=str, default="")
    prs = prs.parse_args()

    print("[+] Falla [the search-engine-scraper]")
    if prs.engine is not None and prs.query is not None:
        get_results(engine=prs.engine.lower(), query=prs.query.lower(), pages=prs.page)
    else:
        list_engines()

```
app/settings.py
```.py
import configparser as ConfigParser

# Configs parameters configParser.get('your-config', 'path1')
configParser = ConfigParser.RawConfigParser()
configFilePath = r'config.txt'
configParser.read(configFilePath)

# Filling parameters
SPLASH_SCRAP_URL = configParser.get('falla-config', 'SPLASH_SCRAP_URL')

```
app/utils.py
```.py
from os import listdir as ls
from app.core import *
import json


class Bcolors:
    """[summary]
    """
    AUTRE = '\033[96m'  # rose
    HEADER = '\033[95m'  # rose
    OKBLUE = '\033[94m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'  # jaune
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'


def list_engines():
    print(Bcolors().OKBLUE + "[+] Listing search-Engines" + Bcolors().ENDC)
    engines = [elt.replace(".py", "").lower() for elt in ls("./app/core/") if
               ".py" in elt and "__" not in elt and "falla" not in elt]
    for e in engines:
        print("[+] > " + Bcolors().AUTRE + e + Bcolors().ENDC)


def get_results(engine, query, pages):
    if engine == "aol" or engine == "al":
        f = Aol()
    elif engine == "ask" or engine == "ak":
        f = Ask()
    elif engine == "bing" or engine == "b":
        f = Bing()
    elif engine == "dogpile" or engine == "dp":
        f = DogPile()
    elif engine == "duckduckgo" or engine == "dd":
        f = DuckDuckGo()
    elif engine == "gibiru" or engine == "gu":
        f = Gibiru()
    elif engine == "mojeek" or engine == "m":
        f = Mojeek()
    elif engine == "qwant" or engine == "q":
        f = Qwant()
    elif engine == "searchencrypt" or engine == "se":
        f = SearchEncrypt()
    elif engine == "startpage" or engine == "sp":
        f = StartPage()
    elif engine == "yahoo" or engine == "y":
        f = Yahoo()
    elif engine == "google" or engine == "g":
        f = Google()
    else:
        f = Google()

    results = json.loads(f.search(query, pages))
    bcolors = Bcolors()

    for elt in results:
        print("|> " + bcolors.OKBLUE + elt["title"] + bcolors.ENDC)
        print("|- " + bcolors.WARNING + elt["href"] + bcolors.ENDC)
        print("|| " + elt["cite"])
        print("")

```
example.config.txt
```.txt
[falla-config]
SPLASH_SCRAP_URL = http://127.0.0.1:8050

```
requirements.txt
```.txt
requests==2.31.0
pandas==1.0.1
lxml==4.9.1
beautifulsoup4==4.8.2
selenium==3.141.0
configparser

```
tests/test.py
```.py

```

================
File: PROGRESS.md
================
---
this_file: PROGRESS.md
---

Consult @TODO.md for the detailed plan. Work through these items, checking them off as you complete them. Once a large part has been completed, update @TODO.md to reflect the progress.

# Task List

## 1. Fix Critical Engine Failures

- [x] Enhance `base.SearchEngine` class to enforce API key requirements
  - [x] Modify to require subclasses to define `env_api_key_names` as a class variable
  - [x] Add proper validation for engine code and API key requirements
  - [x] Centralize HTTP request handling with retries and error management

- [x] Enhance `config.py` for better API key handling
  - [x] Update `EngineConfig` to validate and require API keys as needed
  - [x] Improve error messages for missing API keys
  - [x] Add field validation for API keys

- [x] Update engine implementations correctly
  - [x] Fix You.com engines (`you` and `you_news`)
  - [x] Fix Perplexity (`pplx`) implementation
  - [ ] Fix SerpAPI integration
  - [ ] Ensure all engines implement proper API key handling

- [ ] Improve `get_engine` function and error handling
  - [ ] Add descriptive error messages when engines are not found or disabled
  - [ ] Handle engine initialization failures gracefully

- [ ] Add comprehensive tests for engine initialization
  - [ ] Test API key requirements
  - [ ] Test engine availability detection
  - [ ] Test disabled engine handling

## 2. Fix Data Integrity and Consistency Issues

- [!] Fix inconsistent `num_results` parameter handling
  - [x] Implement `_get_num_results` method in base class
  - [!] Implement proper result limiting in working engines (currently `-n 1` returns multiple results)
  - [!] Fix broken engines failing with "`object has no attribute 'get_num_results'`" error
    - [!] Fix `brave` engine implementation
    - [!] Fix `brave_news` engine implementation
    - [ ] Fix `critique` engine implementation
  - [ ] Ensure all engines properly handle and respect `num_results`
  - [ ] Add result limiting logic to all engine implementations

- [ ] Fix source attribution in search results
  - [ ] Ensure all engines use `self.engine_code` when creating `SearchResult` objects
  - [ ] Fix typos in result keys (e.g., `"snippetHighlitedWords"`)

- [!] Fix unwanted result combination in Google engines
  - [!] Fix `google_hasdata` engine returning results from other engines
  - [!] Fix `google_hasdata_full` engine returning results from other engines

## 3. Address Engine Reliability Issues (Empty Results)

- [!] Fix `searchit`-based engines
  - [!] Fix `bing_searchit` returning empty results
  - [!] Fix `google_searchit` returning empty results
  - [!] Fix `qwant_searchit` returning empty results
  - [!] Fix `yandex_searchit` returning empty results
  - [ ] Verify proper installation and dependencies
  - [ ] Create test script to isolate issues
  - [ ] Add detailed logging for debugging
  - [ ] Fix implementation issues or remove if necessary

- [!] Fix API key-based engines with initialization failures
  - [!] Fix `pplx` engine initialization
  - [!] Fix `you` engine initialization 
  - [!] Fix `you_news` engine initialization
  - [ ] Debug initialization failures and API key handling

- [ ] Test and fix other engines with empty results
  - [ ] Identify and address common failure patterns

## 4. Codebase Cleanup

- [ ] Remove all `anywebsearch` engines
  - [ ] Delete `src/twat_search/web/engines/anywebsearch.py`
  - [ ] Remove related imports from all files
  - [ ] Update engine constants and registration

- [x] Fix code duplication in `base.SearchEngine`
  - [x] Centralize common functionality
  - [x] Improve error handling consistency

## 5. Improve CLI Interface

- [ ] Update the `q` command in the CLI
  - [ ] Remove engine-specific parameters
  - [ ] Only use common parameters
  - [ ] Test CLI with various parameter combinations

## 6. Enforce Consistent JSON Output Format

- [ ] Standardize JSON output across all engines
  - [ ] Utilize the existing `SearchResult` model consistently
  - [ ] Remove utility functions like `_process_results` and `_display_json_results`
  - [ ] Remove `CustomJSONEncoder` class
  - [ ] Update engine `search` methods to return list of `SearchResult` objects

- [ ] Update API function return types
  - [ ] Change return type to `list[SearchResult]`
  - [ ] Ensure proper handling of results from engines

- [ ] Update CLI display functions
  - [ ] Use `model_dump` for JSON serialization
  - [ ] Implement simplified result display

## 7. Testing and Documentation

- [!] Fix failing tests
  - [ ] Address test failures in `test_api.py`
  - [ ] Address test failures in `test_config.py`
  - [ ] Address test failures in `test_bing_scraper.py`

- [ ] Create comprehensive test suite
  - [ ] Test each engine individually
  - [ ] Test parameter handling
  - [ ] Test error conditions and recovery

- [ ] Enhance debug logging throughout the codebase
  - [ ] Add consistent logging in all engines
  - [ ] Improve error messages for common failure cases

- [ ] Update documentation
  - [ ] Document engine-specific requirements
  - [ ] Add troubleshooting guidelines
  - [ ] Document parameter handling

## 8. Final Verification

- [ ] Run test command on all engines
  - [ ] Verify correct output format
  - [ ] Verify proper parameter handling
  - [ ] Check for consistent error handling

- [x] Run linters and code quality tools
  - [x] Address all Ruff and Mypy issues
  - [ ] Run `cleanup.py status` regularly during development

- [ ] Verify full system functionality
  - [ ] Test with the problem case command
  - [ ] Ensure all engines return proper results

================
File: pyproject.toml
================
[build-system]
requires = [
    'hatchling>=1.27.0',
    'hatch-vcs>=0.4.0',
]
build-backend = 'hatchling.build'
[tool.hatch.build.targets.wheel]
packages = ['src/twat_search']
[tool.hatch.build.hooks.vcs]
version-file = 'src/twat_search/__version__.py'

[tool.hatch.version]
source = 'vcs'

[tool.hatch.version.raw-options]
version_scheme = 'post-release'
[tool.hatch.envs.default]
dependencies = [
    'pytest',
    'pytest-cov',
    'mypy>=1.15.0',
    'ruff>=0.9.6',
    'absolufy-imports>=0.3.1',
    'pre-commit>=4.1.0',
    'pyupgrade>=3',
    'isort>=6.0.0',
]

[tool.hatch.envs.default.scripts]
test = 'pytest {args:tests}'
test-cov = 'pytest --cov-report=term-missing --cov-config=pyproject.toml --cov=src/twat_search --cov=tests {args:tests}'
type-check = 'mypy src/twat_search tests'
lint = [
    'ruff check src/twat_search tests',
    'ruff format src/twat_search tests',
]
[[tool.hatch.envs.all.matrix]]
python = [
    '3.10',
    '3.11',
    '3.12',
]

[tool.hatch.envs.lint]
detached = true
dependencies = [
    'mypy>=1.15.0',
    'ruff>=0.9.6',
    'absolufy-imports>=0.3.1',
    'pre-commit>=4.1.0',
    'pyupgrade>=3',
    'isort>=6.0.0',
]

[tool.hatch.envs.lint.scripts]
typing = 'mypy --install-types --non-interactive {args:src/twat_search tests}'
style = [
    'ruff check {args:.}',
    'ruff format {args:.}',
]
fmt = [
    'ruff format {args:.}',
    'ruff check --fix {args:.}',
]
all = [
    'style',
    'typing',
]

[tool.hatch.envs.test]
dependencies = ['.[test]']

[tool.hatch.envs.test.scripts]
test = 'python -m pytest -n auto {args:tests}'
test-cov = 'python -m pytest -n auto --cov-report=term-missing --cov-config=pyproject.toml --cov=src/twat_search --cov=tests {args:tests}'
bench = 'python -m pytest -v -p no:briefcase tests/test_benchmark.py --benchmark-only'
bench-save = 'python -m pytest -v -p no:briefcase tests/test_benchmark.py --benchmark-only --benchmark-json=benchmark/results.json'

[tool.ruff]
target-version = 'py310'
line-length = 120

[tool.ruff.lint]
select = [
    'A',
    'ARG',
    'B',
    'C',
    'DTZ',
    'E',
    'EM',
    'F',
    'FBT',
    'I',
    'ICN',
    'ISC',
    'N',
    'PLC',
    'PLE',
    'PLR',
    'PLW',
    'Q',
    'RUF',
    'S',
    'T',
    'TID',
    'UP',
    'W',
    'YTT',
]
ignore = [
    'ARG001',
    'ARG002',
    'ARG004',
    'ARG005',
    'B904',
    'C901',
    'DTZ005',
    'E501',
    'F401',
    'F811',
    'FBT001',
    'FBT002',
    'FBT003',
    'I001',
    'ISC001',
    'N803',
    'PLR0911',
    'PLR0912',
    'PLR0913',
    'PLR0915',
    'PLR2004',
    'S311',
    'S603',
    'S607',
    'T201',
]

[tool.ruff.lint.per-file-ignores]
"tests/*" = ['S101']

[tool.mypy]
python_version = '3.10'
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
[tool.coverage.run]
source_pkgs = [
    'twat_search',
    'tests',
]
branch = true
parallel = true
omit = ['src/twat_search/__about__.py']

[tool.coverage.paths]
twat_search = [
    'src/twat_search',
    '*/twat-search/src/twat_search',
]
tests = [
    'tests',
    '*/twat-search/tests',
]

[tool.coverage.report]
exclude_lines = [
    'no cov',
    'if __name__ == .__main__.:',
    'if TYPE_CHECKING:',
]
[tool.pytest.ini_options]
markers = ['''benchmark: marks tests as benchmarks (select with '-m benchmark')''']
addopts = '-v -p no:briefcase'
testpaths = ['tests']
python_files = ['test_*.py']
filterwarnings = [
    'ignore::DeprecationWarning',
    'ignore::UserWarning',
]
asyncio_mode = 'auto'

[tool.pytest-benchmark]
min_rounds = 100
min_time = 0.1
histogram = true
storage = 'file'
save-data = true
compare = [
    'min',
    'max',
    'mean',
    'stddev',
    'median',
    'iqr',
    'ops',
    'rounds',
]

[project]
name = 'twat-search'
dynamic = ['version']
description = 'Advanced search utilities and tools for the twat ecosystem'
readme = 'README.md'
requires-python = '>=3.10'
license = 'MIT'
keywords = [
    'twat',
    'search',
    'utilities',
    'text-search',
    'indexing',
]
classifiers = [
    'Development Status :: 4 - Beta',
    'Programming Language :: Python',
    'Programming Language :: Python :: 3.10',
    'Programming Language :: Python :: 3.11',
    'Programming Language :: Python :: 3.12',
    'Programming Language :: Python :: Implementation :: CPython',
    'Programming Language :: Python :: Implementation :: PyPy',
]
dependencies = [
    'twat>=1.8.1',
    'pydantic>=2.10.6',
    'pydantic-settings>=2.8.0',
    'httpx>=0.28.1',
    'python-dotenv>=1.0.1',
    'fire>=0.5.0',
    'rich>=13.6.0',
    'requests>=2.31.0',
]

[[project.authors]]
name = 'Adam Twardoch'
email = 'adam+github@twardoch.com'

[project.urls]
Documentation = 'https://github.com/twardoch/twat-search#readme'
Issues = 'https://github.com/twardoch/twat-search/issues'
Source = 'https://github.com/twardoch/twat-search'
[project.entry-points."twat.plugins"]
search = 'twat_search'

[project.optional-dependencies]
test = [
    'pytest>=8.3.4',
    'pytest-cov>=6.0.0',
    'pytest-xdist>=3.6.1',
    'pytest-benchmark[histogram]>=5.1.0',
    'pytest-asyncio>=0.25.3',
]
dev = [
    'pre-commit>=4.1.0',
    'ruff>=0.9.6',
    'mypy>=1.15.0',
    'absolufy-imports>=0.3.1',
    'pyupgrade>=3',
    'isort>=6.0.0',
]
brave = []
duckduckgo = ['duckduckgo-search>=7.5.0']
bing_scraper = ['scrape-bing>=0.1.2.1']
tavily = ['tavily-python>=0.5.0']
pplx = []
serpapi = ['serpapi>=0.1.5']
hasdata = []
google_scraper = ['googlesearch-python>=1.3.0']
searchit = ['searchit']
all = [
    'twat',
    'duckduckgo-search>=7.5.0',
    'scrape-bing>=0.1.2.1',
    'tavily-python>=0.5.0',
    'serpapi>=0.1.5',
    'googlesearch-python>=1.3.0',
    'searchit',
    'requests>=2.31.0',
]

[project.scripts]
twat-search = 'twat_search.__main__:main'
twat-search-web = 'twat_search.web.cli:main'

================
File: README.md
================
# Twat Search: multi-engine web search aggregator

## Executive summary

Twat Search is a powerful, asynchronous Python package that provides a unified interface to query multiple search engines simultaneously. It facilitates efficient information retrieval by aggregating, normalizing, and processing results from various search providers through a consistent API. This comprehensive documentation serves as a definitive guide for both CLI and Python usage of the package.

## Key features

- **Multi-Engine Search**: A single query can simultaneously search across multiple providers including Brave, Google (via SerpAPI/HasData), Tavily, Perplexity, You.com, Bing (via web scraping), and more
- **Asynchronous Operation**: Leverages `asyncio` for concurrent searches, maximizing speed and efficiency
- **Rate Limiting**: Built-in mechanisms to prevent exceeding API limits of individual search providers
- **Strong Typing**: Full type annotations and Pydantic validation for improved code reliability and maintainability
- **Robust Error Handling**: Custom exception classes for graceful error management
- **Flexible Configuration**: Configure search engines via environment variables, `.env` files, or directly in code
- **Extensible Architecture**: Designed for easy addition of new search engines
- **Command-Line Interface**: Rich, interactive CLI for searching and exploring engine configurations
- **JSON Output**: Supports JSON output for easy integration with other tools

## Installation options

### Full installation

```bash
uv pip install --system twat-search[all]
```

or 

```bash
uv pip install --system twat-search[all]
```


### Selective installation

Install only specific engine dependencies:

```bash
# Example: install only brave and duckduckgo dependencies
pip install "twat-search[brave,duckduckgo]"

# Example: install duckduckgo and bing scraper
pip install "twat-search[duckduckgo,bing_scraper]"
```

After installation, both `Twat Search` and `Twat Search-web` commands should be available in your PATH. Alternatively, you can run:

```bash
python -m twat_search.__main__
python -m twat_search.web.cli
```

## Quick start guide

### Python API

```python
import asyncio
from twat_search.web import search

async def main():
    # Search across all configured engines
    results = await search("quantum computing applications")

    # Print results
    for result in results:
        print(f"[{result.source}] {result.title}")
        print(f"URL: {result.url}")
        print(f"Snippet: {result.snippet}\n")

# Run the async function
asyncio.run(main())
```

### Command line interface

```bash
# Search using all available engines
Twat Search q "climate change solutions"

# Search with specific engines
Twat Search q "machine learning frameworks" --engines brave,tavily

# Get json output
Twat Search q "renewable energy" --json

# Use engine-specific command
Twat Search brave "web development trends" --count 10
```

## Core architecture

### Module structure

```
twat_search/
└── web/
    ├── engines/            # Individual search engine implementations
    │   ├── __init__.py     # Engine registration and availability checks
    │   ├── base.py         # Base SearchEngine class definition
    │   ├── brave.py        # Brave search implementation
    │   ├── bing_scraper.py # Bing scraper implementation
    │   └── ...             # Other engine implementations
    ├── __init__.py         # Module exports
    ├── api.py              # Main search API
    ├── cli.py              # Command-line interface
    ├── config.py           # Configuration handling
    ├── exceptions.py       # Custom exceptions
    ├── models.py           # Data models
    └── utils.py            # Utility functions
```

## Supported search engines

Twat Search provides a consistent interface to the following search engines:

| Engine | Module | API Key Required | Description | Package Extra |
| --- | --- | --- | --- | --- |
| Brave | `brave` | Yes | Web search via Brave Search API | `brave` |
| Brave News | `brave_news` | Yes | News search via Brave API | `brave` |
| You.com | `you` | Yes | Web search via You.com API | - |
| You.com News | `you_news` | Yes | News search via You.com API | - |
| Tavily | `tavily` | Yes | Research-focused search API | `tavily` |
| Perplexity | `pplx` | Yes | AI-powered search with detailed answers | `pplx` |
| SerpAPI | `serpapi` | Yes | Google search results via SerpAPI | `serpapi` |
| HasData Google | `hasdata-google` | Yes | Google search results via HasData API | `hasdata` |
| HasData Google Light | `hasdata-google-light` | Yes | Light version of HasData API | `hasdata` |
| Critique | `critique` | Yes | Visual and textual search capabilities | - |
| DuckDuckGo | `duckduckgo` | No | Privacy-focused search results | `duckduckgo` |
| Bing Scraper | `bing_scraper` | No | Web scraping of Bing search results | `bing_scraper` |

## Detailed usage guide

### Python API

#### The `search()` function

The core function for performing searches is `twat_search.web.search()` :

```python
from twat_search.web import search, Config

# Basic usage
results = await search("python async programming")

# Advanced usage with specific engines and parameters
results = await search(
    query="python async programming",
    engines=["brave", "tavily", "bing_scraper"],
    num_results=5,
    language="en",
    country="US",
    safe_search=True
)
```

Parameters:

- **`query`**: The search query string (required)
- **`engines`**: A list of engine names to use (e.g., `["brave", "tavily"]`). If `None` or empty, all configured engines will be used
- **`config`**: A `Config` object. If `None`, configuration is loaded from environment variables
- **`**kwargs`\*\*: Additional parameters passed to engines. These can be:
  - General parameters applied to all engines (e.g., `num_results=10`)
  - Engine-specific parameters with prefixes (e.g., `brave_count=20`, `tavily_search_depth="advanced"`)

#### Engine-specific functions

Each engine provides a direct function for individual access:

```python
from twat_search.web.engines.brave import brave
from twat_search.web.engines.bing_scraper import bing_scraper

# Using brave search
brave_results = await brave(
    query="machine learning tutorials",
    count=10,
    country="US",
    safe_search=True
)

# Using bing scraper (no api key required)
bing_results = await bing_scraper(
    query="data science projects",
    num_results=10,
    max_retries=3,
    delay_between_requests=1.0
)
```

#### Working with search results

The `SearchResult` model provides a consistent structure across all engines:

```python
from twat_search.web.models import SearchResult
from pydantic import HttpUrl

# Creating a search result
result = SearchResult(
    title="Example Search Result",
    url=HttpUrl("https://example.com"),
    snippet="This is an example search result snippet...",
    source="brave",
    raw={"original_data": "from_engine"}  # Optional raw data
)

# Accessing properties
print(result.title)    # "Example Search Result"
print(result.url)      # "https://example.com/"
print(result.source)   # "brave"
print(result.snippet)  # "This is an example search result snippet..."
```

### Command line interface

The CLI provides convenient access to all search engines through the `Twat Search` command.

#### General search command

```bash
Twat Search q <query> [options]
```

Common options:

- `--engines <engine1,engine2,...>`: Specify engines to use
- `--num_results <n>`: Number of results to return
- `--country <country_code>`: Country to search in (e.g., "US", "GB")
- `--language <lang_code>`: Language to search in (e.g., "en", "es")
- `--safe_search <true|false>`: Enable or disable safe search
- `--json`: Output results in JSON format
- `--verbose`: Enable verbose logging

Engine-specific parameters can be passed with `--<engine>_<param> <value>` , for example:

```bash
Twat Search q "machine learning" --brave_count 15 --tavily_search_depth advanced
```

#### Engine information command

```bash
Twat Search info [engine_name] [--json]
```

- Shows information about available search engines
- If `engine_name` is provided, shows detailed information about that engine
- The `--json` flag outputs in JSON format

#### Engine-specific commands

Each engine has a dedicated command for direct access:

```bash
# Brave search
Twat Search brave "web development trends" --count 10

# Duckduckgo search
Twat Search duckduckgo "privacy tools" --max_results 5

# Bing scraper
Twat Search bing_scraper "python tutorials" --num_results 10

# Critique with image
Twat Search critique --image-url "https://example.com/image.jpg" "Is this image real?"
```

## Configuration management

### Environment variables

Configure engines using environment variables:

```bash
# Api keys
BRAVE_API_KEY=your_brave_api_key
TAVILY_API_KEY=your_tavily_api_key
PERPLEXITY_API_KEY=your_perplexity_api_key
YOU_API_KEY=your_you_api_key
SERPAPI_API_KEY=your_serpapi_api_key
CRITIQUE_API_KEY=your_critique_api_key
HASDATA_API_KEY=your_hasdata_api_key

# Engine enablement
BRAVE_ENABLED=true
TAVILY_ENABLED=true
PERPLEXITY_ENABLED=true
YOU_ENABLED=true
SERPAPI_ENABLED=true
CRITIQUE_ENABLED=true
DUCKDUCKGO_ENABLED=true
BING_SCRAPER_ENABLED=true
HASDATA_GOOGLE_ENABLED=true

# Default parameters (json format)
BRAVE_DEFAULT_PARAMS={"count": 10, "safesearch": "off"}
TAVILY_DEFAULT_PARAMS={"max_results": 5, "search_depth": "basic"}
PERPLEXITY_DEFAULT_PARAMS={"model": "pplx-7b-online"}
YOU_DEFAULT_PARAMS={"safe_search": true, "count": 8}
SERPAPI_DEFAULT_PARAMS={"num": 10, "gl": "us"}
HASDATA_GOOGLE_DEFAULT_PARAMS={"location": "Austin,Texas,United States", "device_type": "desktop"}
DUCKDUCKGO_DEFAULT_PARAMS={"max_results": 10, "safesearch": "moderate", "time": "d"}
BING_SCRAPER_DEFAULT_PARAMS={"max_retries": 3, "delay_between_requests": 1.0}

# Global default for all engines
NUM_RESULTS=5
```

You can store these in a `.env` file in your project directory, which will be automatically loaded by the library using `python-dotenv` .

### Programmatic configuration

Configure engines programmatically when using the Python API:

```python
from twat_search.web import Config, EngineConfig, search

# Create custom configuration
config = Config(
    engines={
        "brave": EngineConfig(
            api_key="your_brave_api_key",
            enabled=True,
            default_params={"count": 10, "country": "US"}
        ),
        "bing_scraper": EngineConfig(
            enabled=True,
            default_params={"max_retries": 3, "delay_between_requests": 1.0}
        ),
        "tavily": EngineConfig(
            api_key="your_tavily_api_key",
            enabled=True,
            default_params={"search_depth": "advanced"}
        )
    }
)

# Use the configuration
results = await search("quantum computing", config=config)
```

## Engine-specific parameters

Each search engine accepts different parameters. Here's a reference for commonly used ones:

### Brave search

```python
await brave(
    query="search term",
    count=10,              # Number of results (default: 10)
    country="US",          # Country code (ISO 3166-1 alpha-2)
    search_lang="en",      # Search language
    ui_lang="en",          # UI language
    safe_search=True,      # Safe search (True/False)
    freshness="day"        # Time frame (day, week, month)
)
```

### Bing scraper

```python
await bing_scraper(
    query="search term",
    num_results=10,                # Number of results
    max_retries=3,                 # Maximum retry attempts
    delay_between_requests=1.0     # Delay between requests (seconds)
)
```

### Tavily

```python
await tavily(
    query="search term",
    max_results=5,               # Number of results (default: 5)
    search_depth="basic",        # Search depth (basic, advanced)
    include_domains=["example.com"],  # Domains to include
    exclude_domains=["spam.com"],     # Domains to exclude
    include_answer=True,         # Include AI-generated answer
    search_type="search"         # Search type (search, news, etc.)
)
```

### Perplexity (pplx)

```python
await pplx(
    query="search term",
    model="pplx-70b-online"      # Model to use for search
)
```

### You.com

```python
await you(
    query="search term",
    num_results=10,              # Number of results
    country_code="US",           # Country code
    safe_search=True             # Safe search (True/False)
)
```

### Duckduckgo

```python
await duckduckgo(
    query="search term",
    max_results=10,              # Number of results
    region="us-en",              # Region code
    safesearch=True,             # Safe search (True/False)
    timelimit="m",               # Time limit (d=day, w=week, m=month)
    timeout=10                   # Request timeout (seconds)
)
```

### Critique (with image)

```python
await critique(
    query="Is this image real?",
    image_url="https://example.com/image.jpg",  # URL to image
    # OR
    image_base64="base64_encoded_image_data",   # Base64 encoded image
    source_whitelist=["trusted-site.com"],      # Optional domain whitelist
    source_blacklist=["untrusted-site.com"],    # Optional domain blacklist
    output_format="text"                        # Output format
)
```

## Error handling framework

Twat Search provides custom exception classes for proper error handling:

```python
from twat_search.web.exceptions import SearchError, EngineError

try:
    results = await search("quantum computing")
except EngineError as e:
    print(f"Engine-specific error: {e}")
    # e.g., "Engine 'brave': API key is required"
except SearchError as e:
    print(f"General search error: {e}")
    # e.g., "No search engines configured"
```

The exception hierarchy:

- `SearchError`: Base class for all search-related errors
- `EngineError`: Subclass for engine-specific errors, includes the engine name in the message

Typical error scenarios:

- Missing API keys
- Network errors
- Rate limiting
- Invalid responses
- Configuration errors

## Advanced usage techniques

### Concurrent searches

Search across multiple engines concurrently:

```python
import asyncio
from twat_search.web.engines.brave import brave
from twat_search.web.engines.tavily import tavily

async def search_multiple(query):
    brave_task = brave(query)
    tavily_task = tavily(query)

    results = await asyncio.gather(brave_task, tavily_task, return_exceptions=True)

    brave_results, tavily_results = [], []
    if isinstance(results[0], list):
        brave_results = results[0]
    if isinstance(results[1], list):
        tavily_results = results[1]

    return brave_results + tavily_results

# Usage
results = await search_multiple("artificial intelligence")
```

### Custom engine parameters

Specify engine-specific parameters in the unified search function:

```python
from twat_search.web import search

results = await search(
    "machine learning",
    engines=["brave", "tavily", "bing_scraper"],
    # Common parameters
    num_results=10,
    country="US",

    # Engine-specific parameters
    brave_count=15,
    brave_freshness="week",
    tavily_search_depth="advanced",
    bing_scraper_max_retries=5
)
```

### Rate limiting

Use the built-in rate limiter to avoid hitting API limits:

```python
from twat_search.web.utils import RateLimiter

# Create a rate limiter with 5 calls per second
limiter = RateLimiter(calls_per_second=5)

# Use in an async context
async def rate_limited_search():
    for query in ["python", "javascript", "rust", "golang"]:
        limiter.wait_if_needed()  # Wait if necessary
        results = await search(query)
        # Process results...
```

## Development guide

### Running tests

```bash
# Install test dependencies
pip install "twat-search[test]"

# Run tests
pytest

# Run with coverage
pytest --cov=src/twat_search

# Run tests in parallel
pytest -n auto
```

### Adding a new search engine

To add a new search engine:

1. Create a new file in `src/twat_search/web/engines/`
2. Implement a class that inherits from `SearchEngine`
3. Implement the required methods and register the engine

Example:

```python
from pydantic import HttpUrl
from twat_search.web.engines.base import SearchEngine, register_engine
from twat_search.web.models import SearchResult
from twat_search.web.config import EngineConfig


@register_engine
class MyNewSearchEngine(SearchEngine):
    engine_code = "my_new_engine"
    env_api_key_names = ["MY_NEW_ENGINE_API_KEY"]

    def __init__(self, config: EngineConfig, **kwargs) -> None:
        super().__init__(config, **kwargs)
        # Initialize engine-specific parameters

    async def search(self, query: str) -> list[SearchResult]:
        # Implement search logic
        return [
            SearchResult(
                title="My Result",
                url=HttpUrl("https://example.com"),
                snippet="Result snippet",
                source=self.name
            )
        ]


# Convenience function
async def my_new_engine(query: str, **kwargs):
# Implement convenience function
# ...
```

### Development setup

To contribute to `Twat Search` , follow these steps:

1. Clone the repository:

```bash
   git clone https://github.com/twardoch/Twat Search.git
   cd Twat Search
```

2. Set up the virtual environment with `uv`:

```bash
   uv venv
   source .venv/bin/activate
```

3. Install development dependencies:

```bash
   uv pip install -e ".[test,dev]"
```

4. Run tests:

```bash
   uv run pytest
```

5. Run type checking:

```bash
   uv run mypy src tests
```

6. Run linting:

```bash
   uv run ruff check src tests
```

7. Use `cleanup.py` for project maintenance:

```bash
   python cleanup.py status
```

## Troubleshooting guide

### Api key issues

If you're encountering API key errors:

1. Verify the API key is set correctly in environment variables
2. Check the API key format is valid for the specific provider
3. Ensure the API key has the necessary permissions
4. For engines that require API keys, verify the key is set via one of these methods:
   - Environment variable (e.g., `BRAVE_API_KEY` )
   - `.env` file
   - Programmatic configuration

### Rate limiting problems

If you're being rate limited by search providers:

1. Reduce the number of concurrent requests
2. Use the `RateLimiter` utility to space out requests
3. Consider upgrading your API plan with the provider
4. Add delay between requests for engines that support it (e.g., `delay_between_requests` for Bing Scraper)

### No results returned

If you're not getting results:

1. Check that the engine is enabled (`ENGINE_ENABLED=true`)
2. Verify your query is not empty or too restrictive
3. Try with safe search disabled to see if content filtering is the issue
4. Check for engine-specific errors in the logs (use `--verbose` flag with CLI)
5. Ensure you have the required dependencies installed for the engine

### Common error messages

- `"Engine 'X': API key is required"`: The engine requires an API key that hasn't been configured
- `"No search engines configured"`: No engines are enabled or available
- `"Unknown search engine: X"`: The specified engine name is invalid
- `"Engine 'X': is disabled"`: The engine is registered but disabled in configuration

## Development status

Version: 1.8.1

Twat Search is actively developed. See [PROGRESS.md](PROGRESS.md) for completed tasks and [TODO.md](TODO.md) for planned features and improvements.

## Contributing

Contributions are welcome! Please check [TODO.md](TODO.md) for areas that need work. Submit pull requests or open issues on GitHub. Key areas for contribution:

- Adding new search engines
- Improving test coverage
- Enhancing documentation
- Optimizing performance
- Implementing advanced features (e.g., caching, result normalization)

## License

Twat Search is released under the MIT License. See the [LICENSE](LICENSE) file for details.

---

## Appendix: available engines and requirements

| Engine | Package Extra | API Key Required | Environment Variable | Notes |
| --- | --- | --- | --- | --- |
| Brave | `brave` | Yes | `BRAVE_API_KEY` | General web search engine |
| Brave News | `brave` | Yes | `BRAVE_API_KEY` | News-specific search |
| You.com | - | Yes | `YOU_API_KEY` | AI-powered web search |
| You.com News | - | Yes | `YOU_API_KEY` | News-specific search |
| Tavily | `tavily` | Yes | `TAVILY_API_KEY` | Research-focused search |
| Perplexity | `pplx` | Yes | `PPLX_API_KEY` | AI-powered search with detailed answers |
| SerpAPI | `serpapi` | Yes | `SERPAPI_API_KEY` | Google search results API |
| HasData Google | `hasdata` | Yes | `HASDATA_API_KEY` | Google search results API |
| HasData Google Light | `hasdata` | Yes | `HASDATA_API_KEY` | Lightweight Google search API |
| Critique | - | Yes | `CRITIQUE_API_KEY` | Supports image analysis |
| DuckDuckGo | `duckduckgo` | No | - | Privacy-focused search |
| Bing Scraper | `bing_scraper` | No | - | Uses web scraping techniques |

================
File: TODO.md
================
--- 
this_file: TODO.md
--- 

# twat-search 

Tip: Periodically run `./cleanup.py status` to see results of lints and tests.

Edit the detailed plan in this file @TODO.md. Based on the plan, write an itemized list of tasks in @PROGRESS.md and then, as you work, check off your @PROGRESS.md.  

## 1. Progress Update (2025-02-27)

### 1.1. 1.1 Completed Tasks

We have made significant progress in fixing several critical issues:

1. Enhanced the `base.SearchEngine` class:
   - Fixed API key handling with proper validation
   - Improved error handling in HTTP requests with retries and better error messages
   - Implemented type annotations to fix incompatible type errors
   - Added proper documentation

2. Updated engine implementations:
   - Fixed You.com engines (`you` and `you_news`) with proper API key handling
   - Fixed Perplexity (`pplx`) implementation with explicit string conversion in error messages
   - Fixed Ruff code style issues across multiple files

3. Fixed linter issues:
   - Replaced assert statements with conditional checks
   - Fixed string literals in exceptions
   - Added proper type annotations

### 1.2. Urgent TODOs

Based on testing all engines with the command:
```
for engine in $(twat-search web info --plain); do echo; echo; echo; echo ">>> $engine"; twat-search web q -e $engine "Adam Twardoch" -n 1; done;
```

We've identified the following critical issues that need urgent attention:

#### 1.2.1. Result Limiting Not Working
Even with `-n 1` parameter, multiple results are returned for working engines. This needs to be fixed to ensure the `num_results` parameter is respected.

#### 1.2.2. Engine Initialization Failures
Several engines fail to initialize with error about missing `get_num_results` method:
- `brave` - Object has no attribute 'get_num_results'
- `brave_news` - Object has no attribute 'get_num_results'
- `critique` - Initialization failure
- `pplx` - Fails to initialize (likely API key related)
- `you` - Fails to initialize (likely API key related)
- `you_news` - Fails to initialize (likely API key related)

#### 1.2.3. Empty Results from SearchIt-based Engines
These engines return empty results consistently:
- `bing_searchit`
- `google_searchit`
- `qwant_searchit`
- `yandex_searchit`

#### 1.2.4. Unwanted Result Combination
Some engines are incorrectly combining results from multiple sources:
- `google_hasdata` - Returns its own results plus results from other engines
- `google_hasdata_full` - Returns its own results plus results from other engines

#### 1.2.5. Working Engines
The following engines are working, but have the issue with not respecting `num_results`:
- `bing_scraper`
- `duckduckgo`
- `google_scraper`
- `google_serpapi`
- `tavily`

### 1.3. `brave`

twat-search web q "Adam Twardoch" -n 1 --verbose --json -e brave

[02/26/25 20:51:35] DEBUG    Using num_results=5                                                                                                            cli.py:295
                    DEBUG    Using selector: KqueueSelector                                                                                      selector_events.py:64
                    DEBUG    Attempting to search with engines: ['brave']                                                                                   cli.py:171
                    DEBUG    Search requested with num_results=5                                                                                            api.py:148
                    DEBUG    Initializing engine 'brave' with num_results=5                                                                                  api.py:71
                    WARNING  Failed to initialize engine 'brave': EngineError                                                                                api.py:92
                    ERROR    Error initializing engine 'brave': Engine 'brave': Failed to initialize engine 'brave': 'BraveSearchEngine' object has no       api.py:95
                             attribute 'get_num_results'                                                                                                              
                    WARNING  Failed to initialize engine 'brave': Engine 'brave': Failed to initialize engine 'brave': 'BraveSearchEngine' object has no    api.py:184
                             attribute 'get_num_results'                                                                                                              
                    WARNING  Failed to initialize engines: brave                                                                                            api.py:191
                    ERROR    No search engines could be initialized from requested engines: brave                                                           api.py:199
                    ERROR    Search failed: No search engines could be initialized from requested engines: brave                                            api.py:246
                    ERROR    Search failed: No search engines could be initialized from requested engines: brave                                            cli.py:177
                            ❌ Search Errors                            
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Error                                                                ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ No search engines could be initialized from requested engines: brave │
└──────────────────────────────────────────────────────────────────────┘
{}


### 1.4. `brave_news`

```
twat-search web q "Adam Twardoch" -n 1 --verbose --json -e brave_news
```

[02/26/25 20:52:52] DEBUG    Using num_results=5                                                                                                            cli.py:295
                    DEBUG    Using selector: KqueueSelector                                                                                      selector_events.py:64
                    DEBUG    Attempting to search with engines: ['brave_news']                                                                              cli.py:171
                    DEBUG    Search requested with num_results=5                                                                                            api.py:148
                    DEBUG    Initializing engine 'brave_news' with num_results=5                                                                             api.py:71
                    WARNING  Failed to initialize engine 'brave_news': EngineError                                                                           api.py:92
                    ERROR    Error initializing engine 'brave_news': Engine 'brave_news': Failed to initialize engine 'brave_news': 'BraveNewsSearchEngine'  api.py:95
                             object has no attribute 'get_num_results'                                                                                                
                    WARNING  Failed to initialize engine 'brave_news': Engine 'brave_news': Failed to initialize engine 'brave_news':                       api.py:184
                             'BraveNewsSearchEngine' object has no attribute 'get_num_results'                                                                        
                    WARNING  Failed to initialize engines: brave_news                                                                                       api.py:191
                    ERROR    No search engines could be initialized from requested engines: brave_news                                                      api.py:199
                    ERROR    Search failed: No search engines could be initialized from requested engines: brave_news                                       api.py:246
                    ERROR    Search failed: No search engines could be initialized from requested engines: brave_news                                       cli.py:177
                              ❌ Search Errors                               
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Error                                                                     ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ No search engines could be initialized from requested engines: brave_news │
└───────────────────────────────────────────────────────────────────────────┘
{}

### 1.5. 1.8 Further TODOs

Several issues still need immediate attention:

1. Fix failing tests - The test suite has 5 failing tests out of 49:
   - Tests in `test_api.py` related to search functionality
   - Tests in `test_config.py` about configuration expectations
   - A test in `test_bing_scraper.py` for the convenience function

2. Address inconsistent `num_results` parameter handling in all engines

3. Fix `searchit`-based engines that are returning empty results

## 2. Project Status

The basic implementation of the `twat-search` web package is complete. This multi-provider search tool now supports multiple search engines and has a functional CLI interface.

## 3. Problem case

This is the test command that I'm targeting: 

```bash
for engine in $(twat-search web info --plain); do echo; echo; echo; echo ">>> $engine"; twat-search web q -e $engine "Adam Twardoch" -n 1 --json --verbose; done;
```

## 4. Proposed solution

Okay, let's synthesize all the provided information into a single, comprehensive development guide for the `twat-search` project.  This guide will combine the problem statement, the detailed action plan (including code samples), the testing strategy, and the documentation considerations into a cohesive document. It's structured to be a practical guide for someone working on improving this project.

## 5. Twat Search Development Guide

This guide details the current state, problems, and a plan for refactoring and improving the `twat-search` web package.

### 5.1. Project Overview

`twat-search` is an asynchronous Python package designed to query multiple search engines simultaneously, aggregate their results, and present them through a unified API and command-line interface (CLI). It aims for:

*   **Multi-Engine Support:**  Integrate with various search providers (Brave, Google, Tavily, etc.).
*   **Asynchronous Operation:** Utilize `asyncio` for concurrent requests and improved performance.
*   **Configurability:** Allow users to configure engines, API keys, and search parameters via environment variables or configuration files.
*   **Extensibility:**  Make it easy to add new search engine integrations.
*   **Robustness:**  Handle errors gracefully, including API limits and network issues.
*   **Consistency:**  Provide a consistent JSON output format for all engines.

### 5.2. Current Status (as of 2025-02-27)

The basic implementation is complete, with several engines integrated and a functional CLI. Major improvements have been made to the base `SearchEngine` class and error handling. However, there are still issues with test failures and some engines returning empty results that need to be addressed.

### 5.3. Identified Problems

The following problems have been identified through testing and code review:

1.  **Test Failures:** Several tests are failing, particularly in `test_api.py`, `test_config.py`, and `test_bing_scraper.py`. These need to be fixed to ensure the code is working as expected.

2.  **Inconsistent `num_results` Handling:**  The `num_results` parameter, intended to control the number of results returned, is not consistently respected by all engines.

3.  **Empty Results from Some Engines:** The `searchit`-based engines (and potentially others) consistently return empty result sets, indicating a problem with their integration or underlying libraries.

4.  **Inconsistent JSON Output:** Different engines return results with varying key names and structures, making it difficult to process results uniformly. Example: Typos in keys like  `"snippetHighlitedWords"`.

5.  **Code Duplication and Inconsistency in `base.SearchEngine`:**  The base class contains duplicated logic for handling HTTP requests, retries, and parameter validation.  This should be centralized.

6. **Anywebsearch:** Remove all `anywebsearch` engines.

7. **Searchit:** Review the implementation of the `searchit` engines, or remove them.

### 5.4. Action Plan and Implementation Guide

This section provides a detailed, step-by-step plan to address the identified problems.

#### 5.4.1. Fixing Critical Engine Failures

**Goal:** Ensure all engines initialize correctly, handling API key requirements gracefully.

**4.1.1.  Enforce API Key Requirements in `base.SearchEngine`**

*   **File:** `src/twat_search/web/engines/base.py`
*   **Change:**  Modify the `SearchEngine` base class to *require* subclasses to define `env_api_key_names` as an abstract property. Remove the class-level default.

    ```python
    import abc
    import asyncio
    import random  # Import the random module
    import httpx
    from pydantic import BaseModel, HttpUrl, field_validator, ValidationError
    from typing import Any, Optional
    import logging
    from twat_search.web.exceptions import SearchError, EngineError
    from twat_search.web.config import EngineConfig
    from twat_search.web.models import SearchResult
    from twat_search.web.engine_constants import (
        USER_AGENTS,
        ENGINE_FRIENDLY_NAMES,
        standardize_engine_name,
    )

    logger = logging.getLogger(__name__)


    class SearchEngine(abc.ABC):
        engine_code: str  # REQUIRED: Engine code for config and CLI

        # Should not be a class variable, should be an instance
        # and a required property.
        @property
        @abc.abstractmethod
        def env_api_key_names(self) -> list[str]:
            """List of environment variable names that can hold the API key."""
            ...  # raise NotImplementedError("Subclasses must define env_api_key_names")

        @property
        @abc.abstractmethod
        def engine_code(self) -> str:
            """
            The engine code should be a short, unique identifier for the engine,
            using only lowercase letters and underscores.
            """
            ...


        def __init__(self, config: EngineConfig, **kwargs: Any) -> None:
            self.config = config
            # Prioritize kwargs, then config.default_params, then fallbacks.
            self.num_results = self._get_num_results()  # Use the new method.
            self.country = kwargs.get("country") or self.config.default_params.get("country")
            self.language = kwargs.get("language") or self.config.default_params.get("language")
            self.safe_search = kwargs.get("safe_search", self.config.default_params.get("safe_search", True))  # type: ignore
            self.time_frame = kwargs.get("time_frame") or self.config.default_params.get("time_frame")
            self.timeout = kwargs.get("timeout", self.config.default_params.get("timeout", 10))
            self.retries = kwargs.get("retries", self.config.default_params.get("retries", 2))
            self.retry_delay = kwargs.get("retry_delay", self.config.default_params.get("retry_delay", 1.0))
            self.use_random_user_agent = kwargs.get("use_random_user_agent", self.config.default_params.get("use_random_user_agent", True))
            # Store original kwargs for use in subclasses
            self.kwargs = kwargs


            # Check for API key if required
            if self.env_api_key_names:  # Use the property, not the class variable
                api_key = config.api_keys.get(self.engine_code)
                if not api_key:
                  msg = (
                        f"API key is required for {self.engine_code}. "
                        f"Please set it via one of these env vars: {', '.join(self.env_api_key_names)}" # Use property
                    )
                  logger.error(msg)
                  raise EngineError(self.engine_code, msg)  # Use consistent EngineError
                self.api_key = api_key # Assign it to the object if it passes.
            else:
              self.api_key = None



        def _get_num_results(self, param_name: str = "num_results", min_value: int = 1) -> int:
            # Get value from kwargs
            value = self.kwargs.get(param_name)
            if value is not None:
                try:
                    return max(min_value, int(value))
                except (TypeError, ValueError):
                    logger.warning(
                        f"Invalid value for '{param_name}' ({value!r}) in {self.engine_code}, using default."
                    )
            #If not in kwargs get from config
            default = self.config.default_params.get(
                param_name
            ) or self.config.default_params.get("num_results") # num_results as fallback
            if default is not None:
                try:
                    return max(min_value, int(default))
                except (TypeError, ValueError):
                    logger.warning(
                        f"Invalid value for '{param_name}' ({value!r}) in {self.engine_code}, using default."
                    )
            return self.num_results



        async def make_http_request(
            self, method, url, headers=None, **kwargs
        ) -> httpx.Response: # return the full httpx.Response object
            headers = headers or {}
            if self.use_random_user_agent and "user-agent" not in {k.lower() for k in headers}:
                headers["User-Agent"] = random.choice(USER_AGENTS)

            delay = self.retry_delay #initiate delay
            for attempt in range(1, self.retries + 2):  # One extra attempt
                try:
                    async with httpx.AsyncClient(timeout=self.timeout) as client:

                        response = await client.request(
                            method, url, headers=headers, **kwargs
                        )
                        response.raise_for_status()  # Raise for status here
                        return response # return response

                except httpx.HTTPError as e:
                    if attempt > self.retries:  # Use  self.retries
                        # Final retry failed.
                        logger.error(f"Request failed after {self.retries} attempts: {e}")
                        raise EngineError(self.engine_code, f"HTTP request failed after multiple retries: {e}") from e
                    # exponential backoff with jitter
                    jitter = random.uniform(0.5, 1.5)  # Add jitter
                    actual_retry_delay = delay * jitter
                    logger.warning(f"Request failed (attempt {attempt}), retrying in {actual_retry_delay:.2f} seconds: {e}")
                    await asyncio.sleep(actual_retry_delay)
                    delay *= 2  # Exponential backoff

        @abc.abstractmethod
        async def search(self, query: str) -> list[SearchResult]:
            """Perform the search and return a list of SearchResult objects."""
            ...

    _engine_registry = {} # Move to the bottom, after the class definition


    def register_engine(engine_class: type[SearchEngine]) -> type[SearchEngine]:
        try:
          # Check that the engine class has required attributes.
          if not hasattr(engine_class, "engine_code"):
              raise AttributeError("Engine must define an 'engine_code' attribute.")

          # Now using property, and check for its presence.
          if not hasattr(engine_class, "env_api_key_names") or not callable(getattr(engine_class, 'env_api_key_names', None)):
            raise AttributeError("Engine must define a 'env_api_key_names' property.")

          standardized_name = standardize_engine_name(engine_class.engine_code)
          # Check for collisions in engine codes
          if standardized_name in _engine_registry:
                raise ValueError(f"An engine with code '{standardized_name}' is already registered.")
          _engine_registry[standardized_name] = engine_class
          return engine_class
        except Exception as e:
            logger.warning(f"Failed to register engine {engine_class}: {e}")
            return None # Ensure it is not treated as registed.




    def get_engine(engine_name: str, config: EngineConfig, **kwargs: Any) -> SearchEngine:
        engine_class = _engine_registry.get(engine_name)
        if engine_class is None:
            raise EngineError(engine_name, f"Unknown search engine: '{engine_name}'.  Available engines: {', '.join(sorted(_engine_registry.keys()))}") # Improved error message.
        if not config.enabled:
            raise EngineError(engine_name, f"Search engine '{engine_name}' is disabled.")

        return engine_class(config, **kwargs)

    def get_registered_engines() -> dict[str, type[SearchEngine]]:
        return _engine_registry.copy()
    ```
    **Key changes:**

    *   `@abc.abstractmethod` is used for `env_api_key_names` and the new `_get_num_results`. This *forces* subclasses to implement them.
    *   `engine_code` validation added.
    *   A `ValueError` is raised if `engine_code` or the correct `env_api_key_names` are not provided. This prevents silent failures.
    *   `get_engine` now raises a more descriptive `EngineError` if the engine isn't found or is disabled.
    *   `make_http_request` centralizes request logic, including retries, user-agent randomization, and exception handling.  It now returns the full `httpx.Response` object, giving the calling engine more control.
    *  The num_results is validated so that if an invalid value is passed it will take the default value.
    * `_get_num_results` includes a fallback using the general `num_results`.

**4.1.2.  Enhance `config.py`**

*   **File:** `src/twat_search/web/config.py`
*   **Change:** Modify `EngineConfig` to perform validation and require an API key if the engine specifies `env_api_key_names`.

```python
# In src/twat_search/web/config.py
from pydantic import BaseModel, Field, field_validator, ValidationError
import os
import json
import logging
from pathlib import Path

load_dotenv()
logger = logging.getLogger(__name__)

# ... (rest of your config.py) ...

class EngineConfig(BaseModel):
    api_key: str | None = Field(default=None) # Optional in general
    enabled: bool = Field(default=False)
    default_params: dict[str, Any] = Field(default_factory=dict)

    @field_validator("api_key", mode="before")
    def check_api_key(cls, v: str | None, values: ValidationInfo) -> str:
        # print("check_api_key", v, values)
        # if engine has api key names but no key, raise an error
        engine_code = values.data.get('engine_code') # get it from data to avoid errors
        if engine_code is None:
            return v

        try:
            engine_class = get_registered_engines()[standardize_engine_name(engine_code)]
        except KeyError:
            # if engine is not registed yet, we can't check for api_key
            return v

        if hasattr(engine_class, "env_api_key_names"):
            for key_name in engine_class.env_api_key_names:
                if key := os.environ.get(key_name):
                    return key

            # if none is set
            if v is None: # and also no api_key direct value set
                    raise ValueError(
                    f"Engine '{engine_code}' requires an API key. "
                    f"Please set one of these environment variables: {', '.join(engine_class.env_api_key_names)}"
                )

        return v

#keep Config class as is.
class Config(BaseModel):
    engines: dict[str, EngineConfig] = Field(default_factory=dict)
    # ... (rest of your Config class) ...

#Rest of file is ok.
```

**Key changes:**

*   **`@field_validator("api_key", mode="before")`:**  This validator runs *before* the `api_key` field is assigned.  This is crucial because we need to check if the key is required *before* Pydantic's default value logic kicks in.  We use `values.data.get('engine_code')` to safely access the engine code *during* validation, even if it hasn't been fully assigned yet.
*   **`get_registered_engines()`:** We call the `get_registered_engines()` function to access the registry.
*   **`KeyError` Handling:** The code now includes a `try...except KeyError` block. This is important.  During the *initial* loading of the configuration, the `EngineConfig` for an engine might be created *before* the engine class itself is registered.  In this case, looking up the engine class in `_engine_registry` would raise a `KeyError`.  The `try...except` handles this gracefully, skipping the API key check if the engine isn't registered yet.  This prevents a chicken-and-egg problem during startup.
* **Clear Error Message:**  The `ValueError` now provides a very specific message, telling the user *exactly* which environment variable(s) they need to set.
* No longer need for the map with environment variables.

**4.1.3. Update Engine Implementations**

*   **Files:**  `src/twat_search/web/engines/you.py`, `src/twat_search/web/engines/pplx.py`, `src/twat_search/web/engines/serpapi.py` (and any other engine requiring an API key)
*   **Change:**  Define the `env_api_key_names` correctly. Remove any existing API key handling from the `__init__` method (it's now handled by the base class and config).

    ```python
    # Example: src/twat_search/web/engines/you.py
    @register_engine
    class YouSearchEngine(YouBaseEngine):
        engine_code = "you"
        @property
        def env_api_key_names(self) -> list[str]:
          return ["YOU_API_KEY"]
        # ... rest of the engine implementation ...

    # Example: src/twat_search/web/engines/pplx.py
    @register_engine
    class PerplexitySearchEngine(SearchEngine):
      engine_code = "pplx"
      @property
      def env_api_key_names(self) -> list[str]:
        return ["PERPLEXITY_API_KEY"] # Or PERPLEXITYAI_API_KEY if that's correct
    # ...
    #Example: src/twat_search/web/engines/serpapi.py
    @register_engine
    class SerpApiSearchEngine(SearchEngine):
        engine_code = "serpapi"

        @property
        def env_api_key_names(self) -> list[str]:
            return ["SERPAPI_API_KEY"]
    ```
    Also in each class, make sure the `engine_code` is correct, and `_get_num_results` is defined.

**4.1.4. Improve `get_engine`**

Already addressed in 4.1.1.

**4.1.5 Test Engine Initialization**

*   **File:**  `tests/unit/web/engines/test_base.py` (and potentially engine-specific test files)
*   **Change:** Add tests to cover the API key requirement and engine availability:

    ```python
    # Inside test_base.py or a new test file.
    import pytest
    from twat_search.web.engines.base import get_engine, SearchEngine, register_engine, EngineError
    from twat_search.web.config import EngineConfig

    @register_engine
    class MockEngine(SearchEngine):
        engine_code = "mock_engine"
        @property
        def env_api_key_names(self):
          return ["MOCK_API_KEY"]

        async def search(self, query: str) -> list:
            return []

    def test_get_engine_missing_api_key(monkeypatch):
        monkeypatch.delenv("MOCK_API_KEY", raising=False)  # Ensure it's not set
        config = EngineConfig(enabled=True)  # No API key provided
        with pytest.raises(EngineError) as excinfo:
            get_engine("mock_engine", config)
        assert "MOCK_API_KEY" in str(excinfo.value) # Check for expected message

    def test_get_engine_api_key_present(monkeypatch):
        monkeypatch.setenv("MOCK_API_KEY", "test_key")
        config = EngineConfig(enabled=True)
        engine = get_engine("mock_engine", config) # Should not raise error
        assert engine.api_key == "test_key"  # Access api_key (if needed)

    def test_get_engine_disabled(monkeypatch):
      monkeypatch.delenv("MOCK_API_KEY", raising=False)  # Ensure it's not set
      config = EngineConfig(enabled=False)
      with pytest.raises(EngineError):
          get_engine("mock_engine", config)

    ```

**4.2. Data Integrity and Consistency**

**4.2.1. `num_results` Implementation**

*   **`base.SearchEngine`:** Already addressed above (abstract `_get_num_results` method).
*   **Engine-Specific Implementation (Example: `bing_scraper.py`):**

    ```python

    class BingScraperSearchEngine(SearchEngine):
        engine_code = "bing_scraper" # No API key
        @property
        def env_api_key_names(self):
          return []

        def __init__(self, config: EngineConfig, **kwargs: Any) -> None:
          super().__init__(config, **kwargs)
          self.max_results = self._get_num_results()

        #... rest of your BingScraperSearchEngine code ...

    ```
    Make sure `self.max_results` is used to limit the results in the `search` method:
    ```python
    async def search(self, query: str) -> list[SearchResult]:
      # ... your code to fetch results ...

      results = []
      for result in raw_results: # raw_results would be results from scraper.
          search_result = self._convert_result(result)
          if search_result:  # Handle potential None values.
              results.append(search_result)
          if len(results) >= self.max_results:  # Limit results here.
              break

      return results
    ```

*   **Repeat:**  Apply the same `_get_num_results` implementation and result limiting logic to *all* other engine classes.  This is repetitive but crucial.

**4.2.2.  Source Attribution**
*   **`base.SearchEngine`:**  Already addressed with the abstract `engine_code` property.
*   **Engine-Specific Implementation:** In each engine, make *absolutely sure* you're using `self.engine_code` when creating the `SearchResult`:

    ```python
    # Example within an engine's search method:
    def _convert_result(self, raw: dict[str, Any]) -> SearchResult | None:
      try:
        return SearchResult(
            title=raw.get("title", ""),
            url=HttpUrl(raw.get("link", "")), # Using link
            snippet=raw.get("snippet", ""),
            source=self.engine_code,  # ALWAYS use self.engine_code
            raw=raw  # Include the raw data
        )
      except ValidationError as e:
        # warning message
        return None

    ```

**4.3. Engine Reliability (Empty Results)**

*   **`searchit` Engines:**
    1.  **Verify Installation:**  Run `uv pip install searchit` in your activated virtual environment.
    2.  **Minimal Example:** Create a *separate* Python script (`test_searchit.py`):

        ```python
        import asyncio
        from searchit.search import GoogleScraper, ScrapeRequest, SearchitResult

        async def test_searchit():
            scraper = GoogleScraper()
            request = ScrapeRequest(query="test query")
            results = await scraper.scrape(request)
            print(results)

        if __name__ == "__main__":
            asyncio.run(test_searchit())

        ```

        Run this script independently: `python test_searchit.py`.  If this fails, the problem is with the `searchit` library itself, or your installation of it (or potentially network/proxy issues).  If it *works*, the problem is in your integration within `twat-search`.

    3.  **Debugging (if the minimal example works):** Add detailed logging to `src/twat_search/web/engines/searchit.py`:

        ```python
        # Inside the GoogleSearchitEngine, YandexSearchitEngine, etc.
        async def search(self, query: str) -> list[SearchResult]:
            if not query:
                raise EngineError(self.engine_code, "Search query cannot be empty")
            logger.info(f"Searching Google (searchit) with query: '{query}'")

            request = ScrapeRequest(
                query=query,
                domain=self.domain,
                language=self.language,
                geo=self.geo,
                max_results=self.max_results, # Use self.max_results
                sleep_interval=self.sleep_interval,

            )
            logger.debug(f"searchit request: {request!r}")  # Log the request object

            scraper = GoogleScraper( # initiate object here, to pass max_results_per_page
                max_results_per_page=min(100, self.max_results),
            )

            try:
              raw_results = await self._run_scraper(scraper, request) # run scraper
              logger.debug(f"Raw results from searchit: {raw_results!r}")  # Log raw results
              if not raw_results:
                  logger.info("No results returned from Google (searchit)")
                  return []
              logger.debug(
                  f"Received {len(raw_results)} raw results from Google (searchit)",
              )
            except Exception as exc:
                error_msg = f"Error running searchit scraper: {exc}"
                logger.error(error_msg)
                raise EngineError(self.engine_code, error_msg) from exc
        # ...rest of code ...
        ```
*   **Brave News, Perplexity:**  Start by getting the *base* Brave and Perplexity engines working.  Then debug the news-specific versions, looking for differences in API endpoints, parameters, or response formats.  Use extensive logging.

**4.4. Codebase Cleanup (`anywebsearch` Removal)**

This is straightforward:

1.  **Delete File:** Delete `src/twat_search/web/engines/anywebsearch.py`.
2.  **Remove Imports:**  Remove *all* imports related to `anywebsearch` from:
    *   `src/twat_search/web/engines/__init__.py`
    *   `src/twat_search/web/engine_constants.py`
    *   Any test files that might have been using it.
3.  **Run Linters:** Use `ruff check --fix` and `ruff format` to automatically clean up unused imports and formatting.
4.  **Run Tests:**  Run `pytest` to ensure that no other parts of the codebase were relying on `anywebsearch`.

**4.5. Improve the CLI (q command)**
* **Remove Engine Specific Params:** The `q` command in the CLI needs to be updated to only take common params. This should be done by making the parameters not prefixed.
    ```python
     @app.command(
        help="Search across multiple engines simultaneously.",
        context_settings={"allow_extra_args": True, "ignore_unknown_options": True},
    )
    def q(
        self,
        ctx: fire.core.Context,
        query: str = typer.Argument(..., help="The search query."),
        engines: str | None = typer.Option(
            None,
            "--engines",
            "-e",
            help="Comma-separated list of search engines to use (e.g., brave,tavily).",
        ),
        num_results: int = typer.Option(
            None,
            "--num-results",
            "-n",
            help="Number of results to return (overrides engine defaults).",
        ),
        country: str = typer.Option(
            None,
            "--country",
            "-c",
            help="Country code for the search (e.g., US, GB).",
        ),
        language: str = typer.Option(
            None,
            "--language",
            "-l",
            help="Language code for the search (e.g., en, es).",
        ),
        safe_search: bool = typer.Option(
            None,
            "--safe-search",
            "-s",
            help="Enable or disable safe search (True/False).",
        ),
        json_output: bool = typer.Option(
            False, "--json", help="Output results in JSON format."
        ),
        verbose: bool = typer.Option(
            False, "--verbose", "-v", help="Enable verbose logging."
        ),
        plain: bool = typer.Option(
            False, "--plain", "-p", help="Output in plain text, url only."
        ),
    ) -> None:
    ```
* **Remove engine specific parameters:**  from the various engine specific commands, like brave, tavily, pplx, etc.
* **Test:** Run `cleanup.py status`

**4.6 Enforce JSON Output Format**

To standardize the JSON output format across all search engines, you should create a helper function that takes the necessary data and returns a dictionary with a consistent structure. This function will be used by each engine to format its results before returning them.  Since you are already using `pydantic`, use your existing `SearchResult` model.

Here's how you would do this:

1.  **Use `SearchResult` model:**

    In `src/twat_search/web/models.py`, you have the following `SearchResult` model:

    ```python
    class SearchResult(BaseModel):
        url: HttpUrl
        title: str
        snippet: str
        source: str
        raw: dict[str, Any] | None = None  # To store any engine-specific data

        @field_validator("title", "snippet", "source")
        def validate_non_empty(cls, v: str) -> str:
            if not v or not v.strip():
                msg = f"value must not be None or empty string, got='{v}'"
                raise ValueError(msg)
            return v.strip()
    ```

2. **Remove utility function**
Remove the `_process_results` and `_display_json_results`. Remove `CustomJSONEncoder`.

3.  **Modify Engine `search` Methods:**

    Each engine's `search` method should now create and return a list of `SearchResult` objects directly, instead of plain dictionaries.

    Example (using a simplified `DuckDuckGoSearchEngine` for illustration):

    ```python
    # src/twat_search/web/engines/duckduckgo.py
    class DuckDuckGoSearchEngine(SearchEngine):
      # ...

      async def search(self, query: str) -> list[SearchResult]:
          # ... (fetch raw results from DuckDuckGo API) ...
          results: list[SearchResult] = []
          for raw_result in raw_results:
            try:
              search_result = SearchResult(
                  url=raw_result["url"],
                  title=raw_result["title"],
                  snippet=raw_result.get("snippet", ""),  # Provide a default
                  source=self.engine_code, # Set source
                  raw=raw_result,  # Store the raw result
              )
              results.append(search_result)
              if len(results) >= self.max_results:
                break # respect the num_results/max_results.
            except ValidationError as e:
                logger.warning(f"Validation error for result: {e}")
                # Consider logging the raw_result here, for debugging.
                continue  # Skip invalid results
            except Exception as e: # catch unexpected errors
                logger.warning(f"Unexpected error converting result: {e}")
                continue

          return results
    ```
    Key points here:
    * Return type is now `list[SearchResult]`.
    * A SearchResult is instantiated for *every* valid result.
    * `source=self.engine_code` is used to correctly set the source.
    * A `try...except ValidationError` block handles potential Pydantic validation errors, logging them and skipping the invalid result.
    * We have added another exception block to catch other exceptions.
    * Added a limit on the number of results using `self.max_results`.
4. **Update API function**

```python
#src/twat_search/web/api.py

async def search(
    query: str,
    engines: list[str] | None = None,
    config: Config | None = None,
    strict_mode: bool = True, # added to ensure that error is raised if engine does not exist.
    **kwargs: Any,
) -> list[SearchResult]: # Change return type
```

Change the return type to `list[SearchResult]`.

```python
    results = await asyncio.gather(*tasks, return_exceptions=True)

    flattened_results: list[SearchResult] = []
    for engine_name, result in zip(engine_names, results, strict=False):
        if isinstance(result, Exception):
            logger.error(
                f"Engine '{engine_name}' failed: {type(result).__name__} - {result}",
            )
        elif isinstance(result, list):
            # This part now expects a list of SearchResult objects. No conversion needed!
            logger.info(
                f"✅ Engine '{engine_name}' returned {len(result)} results",
            )

            for search_result in result:
                search_result.source = engine_name # override and use the engine name
                flattened_results.append(search_result)
        else:
            logger.warning(
                f"⚠️ Engine '{engine_name}' returned no results or unexpected type: {type(result)}",
            )

    if not flattened_results:
        logger.error(f"Search failed: {e}")
        raise SearchError("No results found from any search engine.")

    return flattened_results

```
The important part here is that `flattened_result` and the return is a list of `SearchResult`.

5. **Update CLI function**
Update the `q` command in `cli.py`

```python
@app.command(
    help="Search across multiple engines simultaneously.",
    context_settings={"allow_extra_args": True, "ignore_unknown_options": True},
)
def q(
    self,
    ctx: fire.core.Context,
    query: str = typer.Argument(..., help="The search query."),
    engines: str | None = typer.Option(
        None,
        "--engines",
        "-e",
        help="Comma-separated list of search engines to use (e.g., brave,tavily).",
    ),
    num_results: int = typer.Option(
        None,
        "--num-results",
        "-n",
        help="Number of results to return (overrides engine defaults).",
    ),
    country: str = typer.Option(
        None,
        "--country",
        "-c",
        help="Country code for the search (e.g., US, GB).",
    ),
    language: str = typer.Option(
        None,
        "--language",
        "-l",
        help="Language code for the search (e.g., en, es).",
    ),
    safe_search: bool = typer.Option(
        None,
        "--safe-search",
        "-s",
        help="Enable or disable safe search (True/False).",
    ),
    json_output: bool = typer.Option(
        False, "--json", help="Output results in JSON format."
    ),
    verbose: bool = typer.Option(
        False, "--verbose", "-v", help="Enable verbose logging."
    ),
    plain: bool = typer.Option(
        False, "--plain", "-p", help="Output in plain text, url only."
    ),
) -> None:

    self._configure_logging(verbose)
    #... parse engines, as you did before ...
    if num_results is not None:
        self.logger.debug(f"Using num_results={num_results}")
        common_params["num_results"] = num_results

    # Run search and handle potential errors.
    try:
        results = asyncio.run(
            self._run_search(
                query,
                engine_list,
                **common_params
            )
        )
        if json_output:
            # Use model_dump to serialize Pydantic models to dict
            print(json.dumps([r.model_dump() for r in results], indent=2)) # Now serializing correctly.
        else:
            _display_results(results, verbose, plain) #Simplified _display_results

    except SearchError as e:
        self.logger.error(f"Search failed: {e}")
        _display_errors([str(e)])
```

*   **Key changes:**
    *   The `q` command no longer takes engine-specific parameters.  It only takes common parameters, which are passed to the `_run_search` function.
    * The JSON output uses pydantic `model_dump` to convert `SearchResult` to dict.
    *   The non-JSON output uses a simplified `_display_results` function (see below).

```python
#Simplified display function
def _display_results(results: list[SearchResult], verbose: bool, plain: bool) -> None:
    if not results:
        console.print("[bold red]No results found![/bold red]")
        return

    if plain:
        urls = set()
        for result in results:
            # Avoid duplicate URLs
            if str(result.url) not in urls:
                urls.add(str(result.url))
        for url in sorted(urls):
            console.print(url)
        return

    table = Table()  # Remove show_lines=True to eliminate row separator lines
    table.add_column("Engine", style="magenta")
    if verbose:
      table.add_column("Title", style="green")
      table.add_column("URL", style="blue", overflow="fold", max_width=70)
    else:
      table.add_column("URL", style="blue", overflow="fold")

    for result in results:
        if verbose:
          table.add_row(result.source, result.title, str(result.url))
        else:
          table.add_row(result.source, str(result.url))
    console.print(table)
```

* **Remove helper functions:**
`_process_results`, `_display_json_results` and `CustomJSONEncoder` class are not needed anymore since `twat_search.web.models.SearchResult` model and pydantic `model_dump` are used.

**6. Testing:**

*   Create test cases for each of the above changes, especially within `tests/unit/web/engines/test_base.py` and in individual engine test files.
*   Test the CLI using `fire.Fire`'s testing utilities, as demonstrated in your existing `tests/test_twat_search.py`.

By systematically applying these steps, the code will become more robust, maintainable, and consistent. The detailed plan and code samples will guide you through the refactoring process. Remember to execute `cleanup.py status` regularly and run `pytest` to check for any errors or regressions introduced during the modifications.  Address *all* Ruff and Mypy issues.

================
File: VERSION.txt
================
v1.8.1



================================================================
End of Codebase
================================================================
