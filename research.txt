# Folder Tree Structure

_private
├── WebLLMs.md
├── grep
│   ├── on-greps1.md
│   ├── on-greps2.md
│   ├── on-greps3.md
│   ├── on-greps4.md
│   └── on-greps5.md
├── mcp
│   ├── mcp2502-p0.md
│   ├── mcp2502-p1.md
│   ├── mcp2502-p2.md
│   ├── mcp2502-p3.md
│   └── mcp2502-p4.md
├── refgrab.py
└── websearch_python_options.txt

3 directories, 13 files



# Folder: _private

## File: WebLLMs.md (Size: 4.25 KB)

```
# WebLLMs

https://github.com/camel-ai/camel/
https://github.com/coleam00/ottomator-agents/
https://github.com/DataBassGit/AgentForge/
https://github.com/dot-agent/nextpy/
https://github.com/open-webui/open-webui

uv pip install --system --upgrade open-webui agentforge 'camel-ai[web_tools]' nextpy






## Critique Labs

```api
CRITIQUE_LABS_API_KEY="SAp7azzQwPKsKyBmg8cvGefNgKzqsRCxPCwndVyDZsg"
```

```python

import requests
import base64

def image_url_to_base64(image_url):
    # Fetch the image from the URL
    response = requests.get(image_url)
    
    # Check if the request was successful
    if response.status_code != 200:
        raise Exception(f"Failed to fetch image. Status code: {response.status_code}")
    
    # Convert the image content to base64
    base64_encoded_image = base64.b64encode(response.content).decode('utf-8')
    
    return f"data:image/jpeg;base64,{base64_encoded_image}"

image_url = "https://cdn.britannica.com/95/94195-050-FCBF777E/Golden-Gate-Bridge-San-Francisco.jpg"
base64_string_encoded_image = image_url_to_base64(image_url)

url = 'https://api.critique-labs.ai/v1/search'
headers = {
    'Content-Type': 'application/json',
    'X-API-Key': 'REPLACE_KEY_VALUE'
}
data = {
    'image': base64_string_encoded_image, ## optional, this is the preferred method of sending an image, though url is also accepted. 
    'prompt': 'how much are flights to this place from Hong Kong right now',
    'source_blacklist': ['expedia.com'], ## optional - specify either source_blacklist or source_whitelist, not both
    # 'source_whitelist': ['kayak.com'], ## optional - specify either source_blacklist or source_whitelist, not both
    'output_format': {'flights': [{"origin": "string", "airline": "string", "destination": "string","price": "number"}], "response": "string"} ## optional. If you want a structured response, you can specify the output format here 

}

# Sending a POST request
response = requests.post(url, headers=headers, json=data)

# Check for any errors
if response.status_code != 200:
    raise Exception(f"Request failed with status code {response.status_code}: {response.text}")

# Print the response body
print(response.json())
```

## Linkup.so

```api
LINKUP_API_KEY="1a5987f9-4a6c-4e49-8177-647dafc7b59b"
```

```python
import requests

url = "https://api.linkup.so/v1/search"

payload = {
    "q": "Can you tell me which women were awarded the Physics Nobel Prize?",
    "depth": "standard",
    "outputType": "sourcedAnswer",
    "structuredOutputSchema": "<string>",
    "includeImages": "false"
}
headers = {
    "Authorization": "Bearer <token>",
    "Content-Type": "application/json"
}

response = requests.request("POST", url, json=payload, headers=headers)

print(response.text)
```
## You.com

### You.com WebLLM

```api
YOUCOM_WEBLLM_API_KEY="a1c92e7f-a735-4ec5-bc60-9ecb99ab1be5<__>1OsYiHETU8N2v5f4Li4ndE0c-Phyfd9ZQsR99AC"
```

```python
import requests

def query_web_llm(query):
    headers = {"X-API-Key": YOUR_API_KEY}
    params = {"query": query}
    return requests.get(
        f"https://api.ydc-index.io/rag?query={query}",
        params=params,
        headers=headers,
    ).json()

results = query_web_llm("who invented the kaleidoscope?")
```

### You.com Web search

```api
YOUCOM_API_KEY="e3557839-e62e-458e-9f1f-c678dd016610<__>1OsYexETU8N2v5f4tzNeVj70"                                                   
```

```python
import requests

def get_ai_snippets_for_query(query):
    headers = {"X-API-Key": YOUR_API_KEY}
    params = {"query": query}
    return requests.get(
        f"https://api.ydc-index.io/search",
        params=params,
        headers=headers,
    ).json()
    
results = get_ai_snippets_for_query("reasons to smile")
```


```python
import requests

def get_search_results(query):
    headers = {"X-API-Key": YOUR_API_KEY}
    params = {"query": query}
    return requests.get(
        "https://chat-api.you.com/search?query={query}",
        params=params,
        headers=headers,
    ).json()

```

## Brave

```
>>> from camel.toolkits.search_toolkit import SearchToolkit
>>> tk = SearchToolkit()
>>> tk.search_brave("FontLab 8")
>>> tk.search_google("FontLab 8")
>>> tk.search_duckduckgo("FontLab 8")
```
```
>>> from agentforge.tools.brave_search import BraveSearch
>>> bs = BraveSearch()
>>> bs.search(query='FontLab 8', count=5)
```
```

## File: mcp/mcp2502-p0.md (Size: 24.70 KB)

```
# AI Assistants Introduction

Okay, here's a Table of Contents (TOC) and a TLDR (Too Long; Didn't Read) summary for the tutorial series, including essential code samples. I've incorporated code examples from the provided files, making sure they highlight the key concepts and progression.  I've tried to strike a balance between being concise (for the TLDR) and informative.

## Table of Contents

*   **Part 1: Model Context Protocol - Unleashing the Power of AI Assistants**
    *   1.  Welcome to the World of AI Agents and MCP
        *   1.1.  The Modern AI Landscape: LLMs and Their Limitations
        *   1.2.  What are AI Agents?
        *   1.3.  Introducing the Model Context Protocol (MCP)
            *   1.3.1.  Why MCP? Solving the "Island of Knowledge" Problem
            *   1.3.2.  Core MCP Concepts: Clients, Servers, Tools, Resources, and Prompts
        *   1.4.  The Power of Standardization: A Shared Language for AI
        *   1.5.  A Glimpse of the Ecosystem: `mcp-cli`, `mcp-agent`, `search-server`, `Upsonic`, and `perplexity-advanced-mcp`
         *  1.6 A word on the `mcp-cli` Github repo.
         *  1.7 Conclusion of Chapter 1
    *   2.  Setting Up Your Development Environment
        *   2.1.  Prerequisites: Python, `pip`, `uv`, and Git
        *   2.2.  Cloning the Repository: `mcp-cli`
        *   2.3.  Installing Dependencies with `uv`
        *   2.4.  (Optional but Recommended) Setting up a Virtual Environment
         *  2.5 Conclusion of Chapter 2
    *   3.  Your First MCP Interaction: Exploring with `mcp-cli`
        *   3.1.  Launching `mcp-cli`: Command-Line Options and Interactive Mode
        *   3.2.  Connecting to an MCP Server: The `--server` argument
        *   3.3.  Exploring Server Capabilities:
            *   3.3.1.  `ping`: Checking Server Responsiveness
            *    4.0.1 `list-tools`: Discovering Available Tools
            *   5.0.1.  `list-resources`: Exploring Available Resources
            *   5.0.2.  `list-prompts`: Examining Pre-defined Prompts
            * 5.0.3 `chat`: Entering Interactive Chat Mode
        *   5.1.  Understanding JSON-RPC: The Language of MCP
            *   5.1.1.  Requests and Responses
            *   5.1.2.  Methods, Parameters, and Results
            *   5.1.3.  Error Handling
        *   5.2.  Calling Tools Directly: The `call-tool` command.
            *   5.2.1.  Provide tool name.
            *   5.2.2.  Provide arguments as JSON.
        *   5.3.  Command-Line Options (Detailed)
         * 5.4  Making sense of the response.
        *   5.5.  Connecting to Multiple Servers Simultaneously: The `--all` option.
        *   7.1  Troubleshooting Common Issues
         * 7.2 Conclusion of Chapter 3.
    *   8.  A Simple Search Agent: Your First MCP Server (`search-server`)
        *   8.1.  Why Search? A Fundamental Capability for AI Agents
        *   8.2.  Understanding the `search-server` Architecture
            *   8.2.1.  The MCP Server (`server.py`)
            *   8.2.2.  Search Engine Proxies (`brave`, `metaso`, `bocha`)
            *   8.2.3.  Configuration (`config.py` within each proxy)
        *   8.3.  Choosing Your Search Engine: Brave, Metaso, or Bocha
            *   8.3.1.  Brave Search: Privacy-Focused, API Key Required
            *   8.3.2.  Metaso: Good for general and academic search, requires `uid` and `sid`
            *   8.3.3.  Bocha: High-quality results, API Key Required, paid service
        *   8.4.  Configuring Your API Keys (or UID/SID)
        *   8.5.  Running the `search-server`
        *   8.6.  Interacting with `search-server` using `mcp-cli`
            *   8.6.1.  Launch `mcp-cli`.
            *   8.6.2.  `ping`
            *   8.6.3.  `list-tools` (seeing `search`, `location_search`, etc.)
            * 9.0.1 Calling `search` and understanding the results.
            * 9.0.2 Experimenting with different search engines and parameters.
        *  9.1 Troubleshooting
        *   9.2.  Conclusion and Next Steps
    *   10. Conclusion of Part 1
        * 10.1  Recap of what we have learnt.
        * 10.2 Preview of next part.

*   **Part 2: Perplexity Advanced MCP**
    *   1.  Understanding MCP and its Benefits: The Key to Unlocking AI's Potential
        *   1.1.  The Limitations of LLMs Out-of-the-Box
        *   1.2.  Bridging the Gap: Enter MCP
        *   1.3.  The MCP Ecosystem: Clients and Servers
        *   1.4.  How MCP Works: A Simplified Analogy
        *   1.5.  Why This Matters: Benefits of MCP
        *   1.6.  Concrete Example: Beyond Basic Search
        *   1.7.  Addressing potential questions from the reader
        * 1.7 Conclusion of Chapter 1
    *   2.  Installation and Setup: Getting Started with `perplexity-advanced-mcp`
        *   2.1.  Prerequisites
        *   2.2.  Installation Options
        *   2.3.  Configuration: Setting Your API Key
        *   2.4.  Verifying the Installation
        *   2.5.  Troubleshooting
        *   2.6.  (Optional) Setting up a Virtual Environment
        * 2.7 Conclusion of Chapter 2
    *   3.  Exploring the Core Features: Putting `ask_perplexity` to Work
        *   3.1.  Anatomy of an `ask_perplexity` Call
        *   3.2.  Simple Queries: Quick and Efficient Information Retrieval
        *   3.3.  Complex Queries: In-Depth Reasoning and Analysis
        *   3.4.  Leveraging File Attachments: Providing Context
        *   3.5.  Interpreting the Results
        *   3.6.  Caveats and Considerations
        * 3.7 Conclusion of Chapter 3
    *   4.  Integration with Claude Desktop: Supercharging Your AI Assistant
        *   4.1.  Why Integrate with Claude Desktop?
        *   4.2.  Prerequisites
        *   4.3.  Step-by-Step Integration Guide
        *   4.4.  Testing the Integration
        *   4.5.  Troubleshooting
        *   4.6.  Best Practices and Tips
        *  4.7 Conclusion of Chapter 4.
    *   5.  Integration with VS Code (Cursor): Bringing Search to Your Code Editor
        *   5.1.  Why Integrate with a Code Editor?
        *   5.2.  Prerequisites
        *   5.3.  Step-by-Step Integration Guide (Cursor)
        *   5.4.  Step-by-Step Integration Guide (VS Code)
        *   5.5.  Using the Tool in Cursor/VS Code
        *   5.6.  Troubleshooting (Cursor/VS Code)
        *   5.7.  Conclusion of Chapter 5
    *   6.  Advanced Usage: Customization, Tips, and Tricks
        *   6.1.  Customizing the Underlying LLM Models (config.py)
        *   6.2.  Understanding and Handling Errors
        *   6.3.  Using MCP Server Arguments
        *   6.4.   Logging and Debugging
        *   6.5.   Rate Limits and Best Practices
        *   6.6.  Contributing and Extending
        *   6.6  Conclusion of Chapter 6
    *   7.  Practical Use Cases and Scenarios
        *   7.1.  Quick Fact Retrieval (Simple Query)
        *   7.2.  In-Depth Research (Complex Query)
        *   7.3.  Code Explanation with File Attachment
        *   7.4.  Using Perplexity vs. OpenRouter
        *   7.5.  Combining with Other Tools (Advanced)
        *   7.6.  Limitations
        *   7.7.  Future Directions
        *   7.8.  Conclusion

*   **Part 3: Advanced Agents, Reliability and the Model Context Protocol (MCP)**
    *   1.  Introduction to `mcp-agent`
        *   1.1.  What is `mcp-agent`? A Framework for Clients, Servers, and Agents
        *   1.2.  Key Concepts: `MCPApp`, `Agent`, `AugmentedLLM`, `Executor`
        *   1.3.  `mcp-agent` vs. `mcp-cli` vs. Upsonic: Understanding the Roles
        *   1.4.  Installation: `pip install mcp-agent`
        *   1.5.  Configuration: `mcp_agent.config.yaml` and `mcp_agent.secrets.yaml`
    *   2.  Building a Basic MCP Client with `mcp-agent`
        *   2.1.  Using `mcp_agent.mcp.gen_client` to Connect to Servers
        *   2.2.  Example: Connecting to `perplexity-advanced-mcp`
        *   2.3.  Listing Tools, Calling Tools
        *   2.4.  Persistent Connections with `MCPConnectionManager`
        *   2.5.  Using `mcp_agent.app.MCPApp` in Client Mode
    *   3.  Building an MCP Server with `mcp-agent`
        *   3.1.  The `mcp_agent.mcp.mcp_agent_server` Module
        *   3.2.  Defining Tools with `@server.list_tools()` and `@server.call_tool()`
        *   3.3.  Example: Building a Simple "Echo" Server
        *   3.4.  Running the Server with `uvx`
        * 3.5 Testing with `mcp-cli`.
    *   4.  Building Agents with `mcp_agent.agents.Agent`
        *   4.1.  The `Agent` Class: Connecting to MCP Servers
        *   4.2.  Agent Instructions and Capabilities
        *   4.3.  Example: A Simple File-Reading Agent (using `filesystem` server)
        *   4.4.  The `decorator_app.py` and the `@agent` decorator.
        *   4.5.  Attaching an LLM with `attach_llm`
            *   4.5.1.  Using `OpenAIAugmentedLLM`
            *   4.5.2.  Using `AnthropicAugmentedLLM`
            *   4.5.3.  Custom LLM Integrations
    *   5.  Advanced Agent Patterns with `mcp-agent.workflows`
        *   5.1.  The `AugmentedLLM` Base Class
        *   5.2.  Parallel Execution with `ParallelLLM`
        *   5.3.  Routing with `LLMRouter` (OpenAI and Anthropic versions)
        *   5.4.  Workflow Orchestration with `Orchestrator`
        *   5.5.  Defining Tasks and Dependencies
        *   5.6.  Dynamic Plan Generation
        *   5.7.  Evaluator-Optimizer Pattern with `EvaluatorOptimizerLLM`
        *   5.8.  The Swarm Pattern with `SwarmAgent` and `AnthropicSwarm`/`OpenAISwarm`
    *   6.  `mcp-agent`'s Executor System
        *   6.1.  The `Executor` Base Class
        *   6.2.  `AsyncioExecutor`: The Default Asynchronous Executor
        *   6.3.  `TemporalExecutor`: Distributed Execution with Temporal (Optional)
        *   6.4.  Task Registry and Decorators: `@app.workflow`, `@app.workflow_run`, `@app.workflow_task`
    *   7.  Logging and Tracing
        *   7.1.  `mcp_agent.logging`: Structured Logging
        *   7.2.  Customizing Log Levels and Transports
        *   7.3.  OpenTelemetry Integration for Distributed Tracing
    *    8. The MCP Server Registry
        * 8.1 Managing multiple MCP servers
        * 8.2. Using the MCPAggregator (server-of-servers).
    *   9.  Conclusion and Next Steps

*   **Part 4: AI Assistants - Production-Ready Agents with Upsonic and MCP**
    *   1.  Introduction to Upsonic
        *   1.1.  What is Upsonic? A Production-Focused Agent Framework.
            *   1.1.1.  Addressing Real-World Agent Challenges: Reliability, Scalability, and Modularity
            *   1.1.2.  Upsonic's Core Philosophy: Task-Centric, Composable, and Production-Ready
        *   1.2.  Core Concepts: Tasks, Agents, and the Reliability Layer
            *   1.2.1.  Tasks: Defining Units of Work for AI Agents
            *   1.2.2.  Agents: Orchestrating Tasks and Leveraging Tools
            *   1.2.3.  The Reliability Layer: Ensuring Robust and Trustworthy AI Outputs
        *   1.3.  Why Upsonic? Addressing Real-World Agent Challenges
            *   1.3.1.  Limitations of Basic LLM Interactions
            *   1.3.2.  The Need for Reliability in Production AI Agents
            *   1.3.3.  Upsonic as a Solution for Robust Agent Development
        *   1.4.  Installing Upsonic: `pip install upsonic`
         * 1.5 Setting up API keys.
            * 1.5.1 OpenAI, Anthropic, and other providers.
            * 1.5.2 Environment Variables vs. Configuration Files
        * 1.6 Conclusion of Chapter 1
    *   2.  Your First Upsonic Agent: Simple Task Execution
        *   2.1.  Creating a Basic `Task`
            *   2.1.1.  Defining Task Descriptions and Objectives
            *   2.1.2.  Exploring Task Parameters and Context
        *   2.2.  Defining an `Agent`
            *   2.2.1.  Agent Initialization and Configuration
            *   2.2.2.  Attaching an LLM to an Agent
        *   2.3.  Using `agent.print_do()` to Execute and Display Results
            *   2.3.1.  Understanding the `do()` Method and Task Execution Flow
            *   2.3.2.  Displaying Agent Responses with `print_do()`
        *   2.4.  Exploring `ObjectResponse` and other response types.
            * 2.4.1 Understanding the `ObjectResponse`.
                * 2.4.1.1 Defining Structured Output with `ObjectResponse`
                * 2.4.1.2 Accessing Structured Data from Agent Responses
            * 2.4.2  Exploring `StrResponse`, `IntResponse`, `FloatResponse`, `BoolResponse`, and `StrInListResponse`.
                * 2.4.2.1 Using Basic Response Types for Simple Tasks
                * 2.4.2.2 Choosing the Right Response Type for Your Task
            * 2.4.3 Creating a `CustomTaskResponse`.
                * 2.4.3.1  Building Flexible Output Structures with `CustomTaskResponse`
                * 2.4.3.2  Implementing Custom Output Handling with the `output()` Method
        *   2.5.  Example: A "Hello, World!" Agent
            *   2.5.1.  Step-by-Step Construction of a Simple Upsonic Agent
            *   2.5.2.  Running the "Hello, World!" Agent and Observing the Output
         *  2.6 Conclusion of Chapter 2.
    *   3.  Leveraging MCP Servers with Upsonic
        *   3.1.  The Power of Tools: Connecting to the MCP Ecosystem
            *   3.1.1.  Expanding Agent Capabilities with External Tools
            *   3.1.2.  MCP Servers as Tool Providers for Upsonic Agents
        *   3.2.  Specifying Tools in a `Task`: The `tools` Parameter
            *   3.2.1.  Integrating MCP Tools into Upsonic Tasks
            *   3.2.2.  Understanding Tool Parameters and Usage within Tasks
        *   3.3.  Running an MCP Server (using `search-server` as an example).
            *   3.3.1.  Launching `search-server`
            *   3.3.2.  Configuring `search-server` API Keys
        *   3.4.  Example: A Web Search Agent using `search-server`.
             * 3.4.1 Create and Run the `Task`.
                 * 3.4.1.1 Defining a Task that utilizes the `search` tool
                 * 3.4.1.2 Connecting the Task to the `search-server`
             * 3.4.2 See the results.
                * 3.4.2.1 Examining the Output of the Web Search Agent
                * 3.4.2.2 Understanding the Structure of Search Results
        *   3.5.  Understanding Tool Registration: Connecting to the `tools_server`.
            * 3.5.1 Installing tools.
                * 3.5.1.1 Using `tools_server` to Manage and Register Tools
                * 3.5.1.2 Installing Pre-built and Custom Tools
            * 3.5.2 uninstalling tools.
                * 3.5.2.1 Removing Tools from `tools_server`
                * 3.5.2.2 Managing Tool Dependencies
            * 3.5.3 Tool naming conventions.
                * 3.5.3.1 Best Practices for Naming and Organizing Tools
                * 3.5.3.2 Ensuring Clarity and Maintainability in Tool Management
        * 3.6 Using the `ComputerUse`, `BrowserUse` and `Search` tools.
           * 3.6.1 Exploring `ComputerUse` Tools for Local Interactions
           * 3.6.2 Utilizing `BrowserUse` Tools for Web Automation
           * 3.6.3  Advanced Web Searching with `Search` Tools
        * 3.7 Conclusion of Chapter 3.
    *   4.  Building Reliable Agents: The Reliability Layer
        * 4.1 The importance of reliability in AI.
            * 4.1.1 The Challenge of Hallucinations and Inaccuracies in LLMs
            * 4.1.2 Reliability as a Key Requirement for Production AI Agents
        * 4.2 Introducing the reliability layer.
            * 4.2.1 Upsonic's Approach to Enhancing Agent Reliability
            * 4.2.2 The Role of Verification and Validation in Reliable AI
        * 4.3 Exploring the reliability options.
            * 4.3.1 `prevent_hallucination`
                * 4.3.1.1 Implementing Hallucination Prevention Techniques
                * 4.3.1.2 Configuring and Activating `prevent_hallucination`
            * 4.3.2  Understanding the `ValidationResult`
                * 4.3.2.1  Structure and Components of `ValidationResult`
                * 4.3.2.2  Interpreting Validation Feedback and Metrics
            * 4.3.3 `url_validation`, `number_validation`, `information_validation` and `code_validation`.
                * 4.3.3.1  Leveraging Pre-built Validation Types for Common Scenarios
                * 4.3.3.2  Customizing Validation Logic for Specific Needs
         * 4.4 Conclusion of Chapter 4.
    *   5.  Advanced Upsonic: Multi-Agent Workflows
        *   5.1.  The Need for Multi-Agent Systems
            *   5.1.1.  Addressing Complex Tasks with Collaborative AI
            *   5.1.2.  Benefits of Multi-Agent Workflows: Scalability, Robustness, and Specialization
        *   5.2.  Introducing `MultiAgent` in Upsonic.
            *   5.2.1.  The `MultiAgent` Class: Orchestrating Multiple Agents for Complex Tasks
            *   5.2.2.  Defining Agent Teams and Task Distribution Strategies
        *   5.3.  Example: A Content Creation and Review Workflow
            *   5.3.1.  Defining Multiple Agents
                * 5.3.1.1 Creating Specialized Agents for Content Creation and Review
                * 5.3.1.2 Configuring Agent Instructions and Capabilities
            *   5.3.2.  Creating Interdependent Tasks
                * 5.3.2.1 Designing a Task Workflow for Content Creation and Review
                * 5.3.2.2 Defining Task Dependencies and Execution Order
            *   5.3.3.  Executing with `MultiAgent.do()`
                * 5.3.3.1 Running the Multi-Agent Workflow and Observing the Results
                * 5.3.3.2 Analyzing Task Execution Flow in Multi-Agent Systems
        *   5.4.  Understanding Task Dependencies and Execution Order
            *   5.4.1.  Managing Task Flow and Data Dependencies in Multi-Agent Workflows
            *   5.4.2.  Ensuring Coordinated Execution in Complex Agent Systems
         * 5.5 Conclusion of Chapter 5
    *   6. Direct LLM and Upsonic
        * 6.1 Direct LLM call.
            * 6.1.1 Making Simple LLM Calls with `Direct.do()`
            * 6.1.2 When to Use Direct LLM Calls vs. Agent-Based Tasks
        * 6.2 Using `Direct.do()` for quick calls.
            * 6.2.1  Streamlining Simple Tasks with Direct LLM Interactions
            * 6.2.2  Example: Quick Fact Retrieval with `Direct.do()`
         * 6.3 Conclusion of Chapter 6
    *   7.  Conclusion and Next Steps
        *   8.1.  Review of the Tutorial Series.
            *   8.1.1.  Recap of Part 1: Foundational MCP Concepts and `mcp-cli`
            *   8.1.2.  Recap of Part 2: Perplexity Advanced MCP Server
            *   8.1.3.  Recap of Part 3: Advanced Agents with `mcp-agent` and `Upsonic`
        *   8.2.  The Future of AI Agents and MCP
            *   8.2.1.  Evolving Trends in AI Agent Development
            *   8.2.2.  The Growing MCP Ecosystem and its Potential
        *   8.3.  Further Exploration:
            *   8.3.1.  Contributing to the projects
                *  8.3.1.1 How to Contribute to `mcp-cli`, `mcp-agent`, and `perplexity-advanced-mcp`
                *  8.3.1.2 Community and Contribution Guidelines
            *   8.3.2.  Building your own MCP servers
                * 8.3.2.1 Creating Custom MCP Servers for Specialized Tools
                * 8.3.2.2 Publishing and Sharing Your MCP Servers
            *   8.3.3.  Exploring advanced agent architectures
                * 8.3.3.1 Research Directions in Agent Architectures
                * 8.3.3.2 Experimenting with Advanced Upsonic Workflows
            *   8.3.4.  Addressing the challenges of production deployment.
                * 8.3.4.1 Scalability and Performance Considerations for Production Agents
                * 8.3.4.2 Security and Monitoring in Production AI Deployments

## TLDR - Essential Concepts and Code

This tutorial series teaches you how to build powerful AI agents that can interact with the real world using the **Model Context Protocol (MCP)**. It covers everything from basic setup to advanced concepts.

**Here's the super-condensed version:**

1.  **The Problem:** LLMs (like GPT-4) are smart but have limited knowledge (they're trained on data up to a certain point) and can't *do* things in the real world (like search the web or access your files).

2.  **The Solution: MCP + Agents:** MCP lets LLMs connect to external "tools" (like a search engine, a file system, or a database).  An AI *agent* is an LLM plus the ability to use these tools via MCP.

3.  **The Tools:**
    *   **`mcp-cli`:** A command-line tool to *explore* MCP servers.  Great for learning and testing.
    *   **`search-server`:** A *simple* MCP server you build that lets an agent search the web (using Brave, Metaso, or Bocha).
    *   **`perplexity-advanced-mcp`:** A *more powerful* search server using Perplexity or OpenRouter.
    *   **`mcp-agent`:** A Python framework for building *complex*, *production-ready* agents.  Provides building blocks for workflows, error handling, and more.
    * **Upsonic:** Builds on top of `mcp-agent`. Simplifies and strengthens agent development, providing a higher-level, production-focused framework.

**Essential Code Samples:**

*   **Exploring with `mcp-cli` (Part 1):**

    ```bash
    # Install mcp-cli
    pip install mcp-cli

    # Clone the mcp-cli repository
    git clone https://github.com/chrishayuk/mcp-cli
    cd mcp-cli

    # Install dependencies with uv
    uv sync --reinstall

    # Connect to a built-in example server (sqlite):
    uv run mcp-cli --server sqlite

    # Inside the mcp-cli interactive prompt:
    > ping
    > list-tools
    > list-resources
    > chat
    > call-tool # Then enter tool name, and {} as JSON arguments
    ```

*   **Running the `search-server` (Part 1):**
    ```bash
    # --- In one terminal ---
    # Make sure to have a .env file with the appropriate keys
    # Run the server (using Brave Search, for example)
    uv run search --engine brave
    ```
    ```bash
    # --- In another terminal, using mcp-cli ---
    uv run mcp-cli --server search # Assuming search-server is set up as shown in the tutorial

    > chat
    /tool search.search {"query": "What is the capital of France?"}
    ```

*   **Basic MCP Client with `mcp-agent` (Part 3):**

    ```python
    # client.py
    import asyncio
    from mcp_agent.app import MCPApp
    from mcp_agent.mcp.gen_client import gen_client
    from mcp_agent.logging.logger import get_logger

    logger = get_logger("client_example")

    async def main():
        app = MCPApp(name="basic_client")
        async with app.run() as agent_app:
            context = agent_app.context
            # Configure server
            context.config.mcp.servers["search"].args.extend(["--engine", "brave"])

            # Use 'gen_client' for connection, automatically closed
            async with gen_client("search", server_registry=context.server_registry) as client:
                # The client is now connected
                tools = await client.list_tools()
                logger.info("Available tools:", data=tools)

                result = await client.call_tool(
                    "search", {"query": "latest news about AI"}
                )
                logger.info("Search result:", data=result)

    if __name__ == "__main__":
        asyncio.run(main())
    ```

    ```yaml
    # mcp_agent.config.yaml (minimal example)
    execution_engine: asyncio

    mcp:
      servers:
        search:
          command: "uvx"
          args: ["mcp-server-search"]  # No --engine here
    ```

*   **A Simple Upsonic Agent (Part 4):**

    ```python
    # hello_world.py
    from upsonic import Agent, Task

    agent = Agent("Greeter")
    task = Task(description="Say hello to the world!")

    result = agent.print_do(task) # Executes task, prints formatted output
    print(result)
    ```

* **An Upsonic agent with tools.**
```python
import os
from upsonic import Agent, Task, StrResponse

# Ensure your BRAVE_API_KEY (or other search engine key) is set
os.environ["BRAVE_API_KEY"] = "your_brave_api_key"


agent = Agent("SearchAgent")

task = Task(
    description="What is the population of Tokyo?",
    tools=["search"], # This references the 'search' server we set up.
    response_format=StrResponse
)

result = agent.print_do(task)
print(result)
```
*   **Enabling Reliability Layer in Upsonic (Part 4):**

    ```python
    from upsonic import Agent, ReliabilityLayer

    reliability_layer = ReliabilityLayer(prevent_hallucination=10)  # Enable with a strength level

    agent = Agent("MyAgent", reliability_layer=reliability_layer)
    ```

**Key Takeaways:**

*   MCP is about *connecting* LLMs to the outside world.
*   `mcp-cli` is for exploration. `mcp-agent` is for building real applications. Upsonic builds on top of `mcp-agent` to provide more advanced features.
*   Agents have instructions, can connect to MCP servers, and use an LLM.
*   `mcp-agent` provides building blocks ("workflows") like `ParallelLLM`, `LLMRouter`, and `Orchestrator` to create sophisticated agent behaviors.
*  Upsonic focuses on `Task`, `Agent`, and `Reliability`
*  You can create your own tools and expose them to other clients and servers via the Model Context Protocol (MCP).

This TLDR and TOC should give anyone a very good overview of the tutorial series and the key capabilities it covers.  The code examples provide a taste of how to use each major component.
```

## File: mcp/mcp2502-p1.md (Size: 35.69 KB)

```
# AI Assistants Part 1: Model Context Protocol - Unleashing the Power of AI Assistants

(Focus on foundational concepts and `mcp-cli`)

## 1. Welcome to the World of AI Agents and MCP

The field of Artificial Intelligence has made incredible strides in recent years, largely thanks to the rise of **Large Language Models (LLMs)** like OpenAI's GPT-4, Anthropic's Claude, and others. These models can generate human-quality text, translate languages, write different kinds of creative content, and answer your questions in an informative way. However, even the most advanced LLMs have a fundamental limitation: their knowledge is *static*.

### 1.1. The Modern AI Landscape: LLMs and Their Limitations

Think of an LLM as a vast, incredibly detailed snapshot of information taken at a specific point in time (when it was trained). It can access and process this internal knowledge with amazing speed and fluency. However, it can't *directly* interact with the real world, access up-to-date information, or use external tools. This creates a few key problems:

*   **Outdated Knowledge:** An LLM's information is frozen at its training time. It knows nothing about events that happened after that.
*   **No Real-World Interaction:** An LLM can't book a flight, send an email, control your smart home devices, or access your private files. It's confined to its internal data.
*   **Hallucinations:** When asked about something outside its training data, an LLM can sometimes confidently generate incorrect or nonsensical answers. This is often called "hallucinating."

### 1.2. What are AI Agents?

An **AI agent** is a system that goes beyond the limitations of a standalone LLM. It's an AI that can:

*   **Perceive** its environment (e.g., read files, access web pages)
*   **Reason** and make decisions about how to act
*   **Act** on its environment (e.g., send an email, execute a command)
*   **Interact** with other systems and tools

In essence, an AI agent is an LLM *plus* the ability to interact with the world.

**Example:** Let's imagine a simple AI agent whose job is to find the population of Tokyo and write it to a file named `population.txt`. A standalone LLM *might* know the population of Tokyo (if it was in its training data), but it *can't* access the latest data, and it *definitely* can't write to a file. An agent, using MCP, *can*.

### 1.3. Introducing the Model Context Protocol (MCP)

The **Model Context Protocol (MCP)** is a standardized communication protocol that allows AI agents (and other applications) to interact with external systems and tools. Think of it as a *universal adapter* for AI. It's a defined set of rules and message formats, based on **JSON-RPC**, that let different systems "talk" to each other, even if they were built by different people, using different technologies. You can find the official specification here: [https://modelcontextprotocol.io/](https://modelcontextprotocol.io/).

#### 1.3.1. Why MCP? Solving the "Island of Knowledge" Problem

Without MCP, each LLM and each application that uses an LLM is essentially an "island of knowledge," isolated from the rest of the digital world. To connect them, developers have to write custom integrations for *every* tool and *every* LLM. This is time-consuming, complex, and doesn't scale.

MCP solves this by providing a *standardized interface*. Imagine you have a power outlet in your home. You can plug in a lamp, a TV, or a computer, all using the same outlet, because they all follow the same electrical standard. MCP does the same for AI. It allows different AI agents to connect to different tools (search engines, databases, file systems, etc.) without requiring custom code for each connection.

#### 1.3.2. Core MCP Concepts: Clients, Servers, Tools, Resources, and Prompts

The MCP ecosystem has a few key components:

*   **Clients:** These are the systems that *use* MCP. They are typically AI agents, applications, or IDE integrations (like Cursor or Claude Desktop) that want to leverage external capabilities.
*   **Servers:** These are the systems that *provide* the capabilities. They expose *tools*, *resources*, and *prompts* that clients can use. A server might provide access to a search engine, a database, a file system, or any other functionality.
*   **Tools:** A tool is a specific *function* that a server can perform. For example, a search server might offer a "search" tool. A file system server might offer "read_file" and "write_file" tools.
*   **Resources:** A resource is a piece of *data* that a server manages, and that clients can potentially access or modify. This could be a file, a database record, or any other structured information. Clients can often *subscribe* to resources to be notified of changes.
*   **Prompts:** A prompt is a pre-defined way of interacting with an LLM. Servers can offer prompts that are optimized for specific tasks or models.

**Continuing our example:** In our "population of Tokyo" agent, the agent itself would be the MCP *client*. It would connect to two MCP *servers*: a search server (to find the population) and a file system server (to write the file). The search server would expose a "search" *tool*, and the file system server would expose "read_file" and "write_file" *tools*. The file `population.txt` would be a *resource* managed by the file system server.

### 1.4. The Power of Standardization: A Shared Language for AI

The key to MCP is *standardization*. Everyone agrees on the rules of communication. This brings several significant benefits:

*   **Interoperability:** Any MCP client can interact with any MCP server, regardless of who built them or what technologies they use.
*   **Composability:** You can easily combine different tools and resources from different servers to create powerful, complex AI agents.
*   **Extensibility:** Anyone can create and publish new MCP servers, expanding the capabilities available to the entire ecosystem.
*   **Reusability:** You can reuse existing MCP servers and tools in different projects, saving time and effort.

### 1.5. A Glimpse of the Ecosystem: `mcp-cli`, `mcp-agent`, `search-server`, `Upsonic`, and `perplexity-advanced-mcp`

This tutorial series will introduce you to several key parts of the MCP ecosystem:

*   **`mcp-cli` (This tutorial):** A command-line tool for interacting with MCP servers. It's a great way to explore the protocol and test your server connections.
*   **`mcp-agent` (Part 3):** A framework for building production-ready AI agents using MCP. It provides advanced features like workflow orchestration and reliability checks.
*   **`search-server` (Part 1 & 3):** A simple MCP server that provides web search capabilities, using various search engine backends (Brave, Metaso, Bocha).
*   **`Upsonic` (Part 3 & 4):** A production-focused agent framework that leverages MCP for reliable and scalable agent development.
* **`perplexity-advanced-mcp` (Part 2):** An MCP server that provides access to the Perplexity and OpenRouter APIs for powerful web search.

### 1.6. A word on the mcp-cli Github repo

The `mcp-cli` tool is open source and available on GitHub: [https://github.com/chrishayuk/mcp-cli](https://github.com/chrishayuk/mcp-cli). You can view the source code, report issues, and contribute to its development.

### 1.7. Conclusion of Chapter 1

You've now been introduced to the fundamental concepts of AI agents and the Model Context Protocol. You understand the problem MCP solves, the core components of the ecosystem, and the benefits of standardization. In the next chapter, we'll get our hands dirty and set up our development environment.

## 2. Setting Up Your Development Environment

This chapter will guide you through setting up your environment to use `mcp-cli`. We'll keep it as simple as possible, but also provide enough detail to cover common scenarios.

### 2.1. Prerequisites: Python, `pip`, `uv`, and Git

Before we begin, you'll need the following installed:

*   **Python 3.10 or higher:** `mcp-cli` requires a relatively modern version of Python. You can check your Python version by opening a terminal (or command prompt on Windows) and typing:

    ```bash
    python --version
    ```

    If you see a version number like `3.10.x`, `3.11.x`, `3.12.x`, or higher, you're good to go. If you have an older version (or no Python installed), you'll need to install a newer version. You can download Python from the official website ([https://www.python.org/downloads/](https://www.python.org/downloads/)) or use a package manager like `pyenv` (recommended for managing multiple Python versions) or `conda`.

*   **`pip` and `uv`:** `pip` is the standard Python package installer.  `uv` is a faster alternative that we'll be using in this tutorial.  `pip` is usually included with Python installations. You can check if you have `pip` installed by running:

    ```bash
    pip --version
    ```

    If you do not have `uv` installed, you may install it using `pip`.

    ```bash
    pip install uv
    ```
    If you are getting permission errors on MacOS or Linux, try:
    ```
    pip install --user uv
    ```
    If you are still getting errors, refer to the uv installation guide:
    [https://github.com/astral-sh/uv](https://github.com/astral-sh/uv)

*   **Git:** `git` is a version control system that we'll use to download the `mcp-cli` code. You can check if you have `git` installed by running:

    ```bash
    git --version
    ```

    If you don't have `git` installed, you can download it from the official website ([https://git-scm.com/downloads](https://git-scm.com/downloads/)) or use your operating system's package manager.

### 2.2. Cloning the Repository: `mcp-cli`

Now that you have the prerequisites, let's download the `mcp-cli` code. Open a terminal and run the following command:

```bash
git clone https://github.com/chrishayuk/mcp-cli
cd mcp-cli
```

This command does two things:

1.  `git clone https://github.com/chrishayuk/mcp-cli`: This downloads the `mcp-cli` code from GitHub to your computer. It creates a new directory named `mcp-cli` containing the code.
2.  `cd mcp-cli`: This changes your current directory to the newly created `mcp-cli` directory.  All subsequent commands should be run from within this directory.

### 2.3. Installing Dependencies with `uv`

`mcp-cli` has a few external dependencies (other Python packages that it relies on). We'll use `uv` to install these. From within the `mcp-cli` directory (make sure you `cd` into it as shown above), run:

```bash
uv sync --reinstall
```

This command reads the `pyproject.toml` file (which lists the dependencies) and installs them. The `--reinstall` flag ensures you get a clean installation, removing any previously installed versions of these packages.

### 2.4. (Optional but Recommended) Setting up a Virtual Environment

It's good practice to use a *virtual environment* for Python projects. A virtual environment is an isolated space for your project's dependencies. This prevents conflicts between different projects that might require different versions of the same package.

While not strictly required for this tutorial, we *highly* recommend using a virtual environment. Here's how to do it with `uv`:

1.  **Create a virtual environment:**

    ```bash
    uv venv .venv
    ```

    This creates a directory named `.venv` inside the `mcp-cli` directory. This directory will contain the isolated Python environment.

2.  **Activate the environment:**

    The activation command depends on your operating system and shell.

    *   **Linux/macOS (bash/zsh):**

        ```bash
        source .venv/bin/activate
        ```

    *   **Windows (cmd.exe):**

        ```bash
        .venv\Scripts\activate.bat
        ```

    *   **Windows (PowerShell):**

        ```powershell
        .venv\Scripts\Activate.ps1
        ```

    After activation, your terminal prompt should change to show the name of the virtual environment (e.g., `(.venv) $`). This indicates that you're now working inside the isolated environment.
3. **Install `uv`:**
    ```bash
        pip install uv
    ```

4.  **Resynchronize dependencies:**

    ```bash
    uv sync --reinstall
    ```
    Any packages you install (or remove) while the environment is active will be isolated within this environment.

5.  **Deactivate:** When you're finished working on the project, you can deactivate the environment:

    ```bash
    deactivate
    ```

    This returns you to your system's default Python environment.

### 2.5. Conclusion of Chapter 2 

You've now successfully set up your development environment! You have Python, `pip`, `uv`, and `git` installed, and you've downloaded the `mcp-cli` code. You're ready to start interacting with MCP servers. In the next chapter, we'll use `mcp-cli` to explore a simple server and learn the basics of the protocol.

## 3. Your First MCP Interaction: Exploring with `mcp-cli`

Now that your environment is set up, let's start interacting with an MCP server using `mcp-cli`. We'll begin by launching `mcp-cli` in interactive mode and exploring some basic commands.

### 3.1. Launching `mcp-cli`: Command-Line Options and Interactive Mode

Open a terminal (or command prompt) and navigate to the `mcp-cli` directory (where you cloned the repository). The simplest way to launch `mcp-cli` is in interactive mode, connected to a built-in example server:

```bash
uv run mcp-cli --server sqlite
```

This command starts `mcp-cli` and tells it to connect to a built-in, minimal MCP server (`sqlite`). You should see a welcome message like this:

```
# Welcome to the Interactive MCP Command-Line Tool (Multi-Server Mode)

Type 'help' for available commands or 'quit' to exit.
```

You're now in interactive mode! You can type commands, and `mcp-cli` will send them to the connected server(s).

**Important:** Type `help` and press Enter. You'll see a list of available commands. This is your built-in help system, and it's very useful for exploring `mcp-cli`.

### 3.2. Connecting to an MCP Server: The `--server` argument

The `--server` argument tells `mcp-cli` which server to connect to. In the example above, we used `--server sqlite`.

### 3.3. Exploring Server Capabilities:

Let's explore some of the basic commands you can use to interact with the connected server.

#### 3.3.1. `ping`: Checking Server Responsiveness

The simplest command is `ping`. It checks if the server is alive and responding. Type `ping` and press Enter:

```
> ping
```

You should see a response similar to this (the exact output may vary):
```
Pinging Servers...
## 4. Server 1 Ping Result

✅ **Server is up and running**
```

This tells you that the server is running and responding to requests. Try exploring! What happens if you type something that *isn't* a command?


#### 4.0.1. `list-tools`: Discovering Available Tools

The `list-tools` command shows you the tools that the connected server(s) provide. Type `list-tools` and press Enter:

```
> list-tools
```
With the `sqlite` server, you'll see:
```
Fetching Tools List from all servers...
## 5. Server 1 Tools List

No tools available.
```
This means the `sqlite` server, which is very basic, doesn't provide any tools. We will use a more fully-featured server soon.


#### 5.0.1. `list-resources`: Exploring Available Resources

The `list-resources` command shows you the resources that the server makes available. Type `list-resources` and press Enter:

``>
> list-resources
```

Again, the `sqlite` server doesn't provide any resources, so you'll see:

```
Fetching Resources List from all servers...

No resources available.
```

#### 5.0.2. `list-prompts`: Examining Pre-defined Prompts

Servers can also provide pre-defined prompts. Try `list-prompts`:

```
> list-prompts
```
The output will show you any prompts the server makes available. The built in `sqlite` server has no available prompts.

#### 5.0.3. `chat`: Entering Interactive Chat Mode
The `chat` command opens up an interactive chat session. You can use `/tool servername.toolname` to call a tool:

```
> chat
```

### 5.1. Understanding JSON-RPC: The Language of MCP

Before we go further, it's important to understand that MCP uses a protocol called **JSON-RPC** for communication. JSON-RPC is a simple way for computers to send messages to each other over a network or other communication channel (like inter-process communication on the same machine).

#### 5.1.1. Requests and Responses

All communication in JSON-RPC consists of *requests* and *responses*. A client (like `mcp-cli`) sends a request to a server, and the server sends back a response.

Here's a simplified example of a JSON-RPC request to the `ping` method:

```json
{
  "jsonrpc": "2.0",
  "method": "ping",
  "id": "1"
}
```

And here's a possible response:

```json
{
  "jsonrpc": "2.0",
  "result": {},
  "id": "1"
}
```

#### 5.1.2. Methods, Parameters, and Results

*   **`jsonrpc`:** This field specifies the version of the JSON-RPC protocol (always "2.0").
*   **`method`:** Every request specifies a *method* to call. This is like calling a function on the server. The `ping`, `list-tools`, `list-resources`, and `list-prompts` commands you used above are all examples of methods.
*   **`params`:** Requests can include *parameters*, which are like arguments to a function. For example, the `call-tool` command (which we'll see later) takes the tool name and its arguments as parameters. Parameters are always provided as a JSON object.
*   **`result`:** If a request is successful, the server will return a *result*. The result is usually a JSON object containing the data returned by the method.
*   **`id`:** The `id` field is used to match requests with responses. `mcp-cli` automatically generates unique IDs for each request.

#### 5.1.3. Error Handling

If a request fails (for example, if you call a tool that doesn't exist, or provide invalid parameters), the server will return an *error* response. The error response will include an error code and a message describing the problem. Here's an example:

```json
{
  "jsonrpc": "2.0",
  "error": {
    "code": -32601,
    "message": "Method not found"
  },
  "id": "5"
}
```

### 5.2. Calling Tools Directly: The `call-tool` command.

The `call-tool` command allows you to directly invoke a tool provided by the server. Let's try it with the `sqlite` server, even though we know it doesn't provide any tools. This will demonstrate the error handling.

#### 5.2.1. Provide tool name.

First you need to type `call-tool`

```
> call-tool
```

#### 5.2.2. Provide arguments as JSON.

`mcp-cli` will then ask for the tool name:

```
Enter tool name:
```

Type a tool name (e.g., `nonexistent_tool`) and press Enter.

It will also ask for the arguments as JSON:

```
Enter tool arguments as JSON (e.g., {'key': 'value'}):
```

Type `{}` (an empty JSON object, meaning no arguments) and press Enter.

You'll see an error message like this:

```
Calling tool 'nonexistent_tool' with arguments:

{
}

Error executing command: 'JSONRPCMessage' object has no attribute 'get'
```
The error message indicates that the tool was not found, as the `sqlite` server does not expose any tools.

### 5.3. Command-Line Options (Detailed)

While we've been using `mcp-cli` in interactive mode, it also has several command-line options that control its behavior. You can see a list of these options by running:

```bash
uv run mcp-cli --help
```

Here's a breakdown of the key options:

*   **`--server <server_name>`:** Specifies the server configuration to use. This is the most important option. It tells `mcp-cli` which server to connect to. We've already seen this in action (`--server sqlite`).
*   **`--config-file <path>`:** (Optional) Path to the JSON configuration file. Defaults to `server_config.json`. We'll cover configuration files in more detail later.
*   **`--all`:** (Optional) Use all the servers provided in the config file specified by `--config-file`. We'll use this option later.
* **`--provider`:** (Optional) Specifies the provider to use (`openai` or `anthropic` or `ollama`). Defaults to `openai`.
* **`--model`: (Optional) Specifies the model to use. Defaults depend on the provider:
  - `gpt-4o-mini` for OpenAI.
  - `claude-3-5-haiku-latest` for Anthropic.
  - `qwen2.5-coder` for Ollama.
*   **`command`:** (Optional) You can provide a command directly on the command line (e.g., `uv run mcp-cli --server sqlite ping`). If you do this, `mcp-cli` will execute the command and then exit, instead of entering interactive mode.

**Examples:**

*   Run the client with the default OpenAI provider and model:

    ```bash
    uv run mcp-cli --server sqlite
    ```

*   Run the client with a specific configuration and Ollama provider:

    ```bash
    uv run mcp-cli --server sqlite --provider ollama --model llama3.2
    ```

*  Run a single command and exit:
    ```
    uv run mcp-cli --server sqlite ping
    ```
### 5.4.  Making sense of the response.

When you call a tool, the response will be in JSON format. It may include fields that are useful to the application or LLM. For example, the response from the `search-server` includes the tool's name, description and input schema. It also includes the output of the tool's execution.

### 5.5. Connecting to Multiple Servers Simultaneously: The `--all` option.

`mcp-cli` can connect to multiple MCP servers at the same time. To do so, you'll need a configuration file. By default, `mcp-cli` looks for a file named `server_config.json` in the current directory. Here's an example `server_config.json` file:

```json
{
  "mcpServers": {
    "sqlite": {
      "command": "uvx",
      "args": ["mcp-server-sqlite", "--db-path", "test.db"]
    },
      "ping_only": {
          "command": "python",
          "args": ["src/search/proxy/ping_only_server.py"]
      }
  }
}
```
**src/search/proxy/ping_only_server.py**
```python
from mcp.server.fastmcp import FastMCP

# Create the MCP server
server = FastMCP("ping-only")

@server.ping
async def ping():
    # Respond to a ping
    return


if __name__ == "__main__":
    server.run()
```

This example file has two defined MCP servers. One `sqlite`, and one minimal server called `ping-only` that just responds to the ping command. To connect to *all* servers defined in the configuration file, use the `--all` option:

```bash
uv run mcp-cli --all
```

Now, when you use commands like `list-tools`, you'll see the tools from *all* connected servers. This enables you to create more complex workflows that combine functionality from multiple sources. You can also connect to specific servers:

```bash
uv run mcp-cli --config-file server_config.json --server sqlite --server ping_only
```
You can also test that the server is working correctly by calling a specific command with the `--all` option:
```bash
uv run mcp-cli --all ping
```
You should see the output:
```
Pinging Servers...
## 6. Server 1 Ping Result

✅ **Server is up and running**
## 7. Server 2 Ping Result

✅ **Server is up and running**
```

### 7.1. Troubleshooting Common Issues

Here are some common problems you might encounter when using `mcp-cli`, and how to fix them:

*   **`mcp-cli` command not found:** Make sure you've activated your virtual environment (if you're using one) and that `mcp-cli` is installed correctly. Try running `pip install .` (or `uv install .`) again from the `mcp-cli` directory.

*   **`Connection refused` error:** This usually means the MCP server you're trying to connect to isn't running, or is running on a different port/address. Double-check the server's documentation and your `mcp-cli` command. If using a custom server, make sure that server is running first.

*   **`Invalid JSON` errors:** JSON is very strict about syntax. Make sure your arguments to `call-tool` are valid JSON. Common mistakes include:
    *   Missing quotes around keys and string values.
    *   Using single quotes instead of double quotes.
    *   Extra commas after the last element in an object or array.
    *   Unescaped special characters.

* **Server not appearing in Claude Desktop/Cursor:** Ensure that the command you are using to call the server is correct. In particular check the following:
    * Make sure that the correct python interpreter is being used.
    * Make sure that the dependencies are installed, by using `uv sync`.
    * Make sure that you are using the correct provider (OpenRouter or Perplexity).

### 7.2. Conclusion of Chapter 3

You've now taken your first steps with `mcp-cli`! You've learned how to:

*   Launch `mcp-cli` in interactive mode.
*   Connect to an MCP server.
*   Explore server capabilities using `ping`, `list-tools`, `list-resources`, and `list-prompts`.
* Use the chat mode.
*   Call a tool directly with `call-tool`.
*   Connect to multiple servers using `--all` and a configuration file.
*   Understand the basics of JSON-RPC.
*   Troubleshoot common issues.

In the next chapter, we'll set up a simple search server (`search-server`) and use `mcp-cli` to interact with it. This will give you a more concrete understanding of how MCP clients and servers work together.

## 8. A Simple Search Agent: Your First MCP Server (`search-server`)

In the previous chapters, you learned about the Model Context Protocol (MCP) and how to use the `mcp-cli` tool to interact with MCP servers. Now, we'll build our own simple MCP server: a search agent called `search-server`.

### 8.1. Why Search? A Fundamental Capability for AI Agents

Search is a cornerstone capability for many AI agents. As we discussed earlier, LLMs have static knowledge. To answer questions about current events, access up-to-date information, or retrieve specific data from the web, an AI agent *needs* to be able to search. Providing a search tool via MCP makes this capability available to any MCP client.

### 8.2. Understanding the `search-server` Architecture

The `search-server` is designed to be modular and extensible. It consists of three main parts:

#### 8.2.1. The MCP Server (`server.py`)

This is the core of the `search-server`. It's a Python script that implements the MCP server logic. It listens for incoming requests from MCP clients (like `mcp-cli`), using the JSON-RPC protocol, handles those requests, and sends back responses. The server itself doesn't directly perform searches; instead, it delegates the search functionality to *search engine proxies*.

#### 8.2.2. Search Engine Proxies (`brave`, `metaso`, `bocha`)

These are separate modules, each responsible for interacting with a specific search engine's API (Brave Search, Metaso, or Bocha). Each proxy "translates" the standardized MCP requests into the specific format required by the underlying search engine, and then translates the search engine's response back into the MCP format. This modular design makes it easy to add support for new search engines in the future.

#### 8.2.3. Configuration (`config.py` within each proxy)

Each search engine proxy has its own `config.py` file. This file contains settings specific to that search engine, such as API keys, rate limits, and default search parameters. You'll need to modify these configuration files to use your own API keys (or other credentials).

### 8.3. Choosing Your Search Engine: Brave, Metaso, or Bocha

The `search-server` supports three different search engine backends:

#### 8.3.1. Brave Search: Privacy-Focused, API Key Required

Brave Search is a privacy-focused search engine that offers a free API (with limits) and paid plans. To use it, you'll need to obtain an API key from their website ([https://search.brave.com/](https://search.brave.com/)). Brave Search provides both general web search and location search.

#### 8.3.2. Metaso: Good for general and academic search, requires `uid` and `sid`

Metaso is another search engine option. It's particularly good for general and academic searches. Instead of an API key, Metaso requires a `uid` and `sid` (user ID and session ID), which you can obtain from your browser's cookies after logging into the Metaso website ([https://metaso.cn/](https://metaso.cn/)).

#### 8.3.3. Bocha: High-quality results, API Key Required, paid service

Bocha ([https://open.bochaai.com/](https://open.bochaai.com/)) is a paid search API service that provides high-quality results. You'll need to obtain an API key from their website.

**Note:** You can only use *one* search engine at a time with `search-server`. You'll choose which one to use when you run the server.

### 8.4. Configuring Your API Keys (or UID/SID)

Before you can run the `search-server`, you need to configure your API key (or UID/SID for Metaso). The recommended way to do this is by setting environment variables. A convenient way to manage environment variables is to create a `.env` file in the project's root directory
(the `search-server` directory). The `python-dotenv` package (which is included in the project's dependencies) will automatically load these variables when you run the server.

Create a file named `.env` (if it doesn't exist) and add the appropriate variables:

*   **Brave Search:**

    ```
    BRAVE_API_KEY=your_brave_api_key
    ```

*   **Metaso:**

    ```
    METASO_UID=your_metaso_uid
    METASO_SID=your_metaso_sid
    ```

*   **Bocha:**

    ```
    BOCHA_API_KEY=your_bocha_api_key
    ```

You can also set these environment variables directly in your terminal:

```bash
export BRAVE_API_KEY="your_brave_api_key"  # Linux/macOS
setx BRAVE_API_KEY "your_brave_api_key"  # Windows
```

### 8.5. Running the `search-server`

To run the `search-server`, use the following command from the project root (i.e., the `search-server` directory):

```bash
uv run search --engine <engine_name>
```

Replace `<engine_name>` with the search engine you want to use: `brave`, `metaso`, or `bocha`. For example:

```bash
uv run search --engine brave
```
You can also specify it in the environment, by setting the `SEARCH_ENGINE` variable.

If the server starts successfully, you'll see output similar to this:

```
INFO:     Started server process [12345]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
```

The server is now running and listening for connections on port 8000. You can stop the server by pressing `Ctrl+C`.

Note that `uv run search` is just a shorthand. Other commands such as `python src/search/server.py` will also work.

### 8.6. Interacting with `search-server` using `mcp-cli`

Now, let's use `mcp-cli` to interact with our running `search-server`.

#### 8.6.1. Launch `mcp-cli`.

Open a *new* terminal window (keeping the `search-server` running in the first one) and navigate to the `mcp-cli` directory. Activate your virtual environment if using one.

#### 8.6.2. `ping`

First, let's check if `mcp-cli` can connect to the server:

```bash
uv run mcp-cli --server search ping
```
You can also enter interactive mode:
```bash
uv run mcp-cli --server search
> ping
```
You should see a successful ping response, indicating that `mcp-cli` is connected to the `search-server`.

#### 8.6.3. `list-tools` (seeing `search`, `location_search`, etc.)

Now, let's see what tools the `search-server` provides:
```
> list-tools
```

You should see a list of tools, including `search` (for web search) and potentially `location_search` (depending on the search engine):

```
Fetching Tools List from all servers...
## 9. Server 1 Tools List

- **search**: Perform a web search using the selected search provider
- **location_search**: Search location using brave
```

#### 9.0.1. Calling `search` and understanding the results

Let's perform a search. We will use chat mode for this, as that is what you will be using if integrating with Cursor or Claude Desktop.

```
> chat
```
Then type the following to the chat prompt:

```
/tool search.search {"query": "latest news about AI"}
```

`mcp-cli` will send the request to the `search-server`, which will use its configured search engine to perform the search. The results will be displayed in your terminal. The output will be structured JSON, but `mcp-cli` presents it in a more readable format. You'll see a list of search results, including titles, URLs, and snippets.

#### 9.0.2. Experimenting with different search engines and parameters.

Try the following:

*   Run `search-server` with different `--engine` options (`brave`, `metaso`, `bocha`). You'll need to stop the server (`Ctrl+C`) and restart it with the new engine.
*   If you're using Brave Search, try the `location_search` tool.

### 9.1. Troubleshooting

*   **`Connection refused`:** Make sure the `search-server` is running in a separate terminal. Check that you're using the correct port (default is 8000). If using a custom server, make sure that server is running first.
*   **`API Key Error`:** Double-check that you've set the correct environment variable for your chosen search engine (e.g., `BRAVE_API_KEY`, `METASO_UID`, `BOCHA_API_KEY`).
*   **No results:** Some search queries might not return any results. Try a different query.

### 9.2. Conclusion and Next Steps

You've now built and interacted with your first MCP server! You've learned about the architecture of `search-server`, how to configure it, and how to use `mcp-cli` to perform searches.

**Challenge:** Try to combine `search-server` with the `mcp-cli`'s `--all` option, along with the `ping_only_server.py` example from the previous chapter. Can you see the tools from both servers?

In the next part of this tutorial series, we’ll go deeper into the Perplexity Advanced MCP, a more complex and capable server.

## 10. Conclusion of Part 1

### 10.1. Recap of what we have learnt.

In this first part of our tutorial series on AI assistants and the Model Context Protocol (MCP), we've covered a lot of ground:

1.  **Introduction to AI Agents and MCP:** We began by understanding the limitations of standalone Large Language Models (LLMs) and how AI agents, powered by MCP, overcome these limitations. We learned that MCP acts as a standard interface, enabling LLMs to interact with external tools and data, making them significantly more powerful and versatile.

2.  **Setting Up the Development Environment:** We walked through the process of setting up a Python development environment, including installing necessary tools like `uv` and cloning the `mcp-cli` repository. We also highlighted the importance of virtual environments for managing project dependencies.

3.  **First MCP Interaction with `mcp-cli`:** We used `mcp-cli` to connect to a basic, built-in MCP server (`sqlite`) and explored fundamental commands like `ping`, `list-tools`, `list-resources`, and `list-prompts`. We learned about the JSON-RPC protocol that underlies MCP communication. We also saw how to call tools directly using `call-tool` and how to connect to multiple servers simultaneously.

4.  **Building a Simple Search Agent (`search-server`):** We built our own MCP server, `search-server`, capable of performing web searches using different search engine backends (Brave, Metaso, Bocha). We learned about the server's architecture, how to configure API keys, and how to interact with it using `mcp-cli`.

Through these chapters, you've gained a solid foundational understanding of MCP and have hands-on experience with both a client (`mcp-cli`) and a server (`search-server`). You've seen how easy it is to extend the capabilities of an AI agent by connecting it to external tools.

### 10.2. Preview of next part.

In Part 2 of this tutorial series, we'll dive deeper into the MCP ecosystem by exploring `perplexity-advanced-mcp`. This server provides a more sophisticated search capability, leveraging the Perplexity and OpenRouter APIs. We'll cover:

*    **Installation and Setup:** Installing and configuring.
*    **Key Features:** Understanding the `ask_perplexity` tool and its capabilities.
*  **Query Types:** Using simple and complex queries.
*  **File Attachments**: Providing additional context for searches.
*   **Integration with Clients:** Connecting `perplexity-advanced-mcp` to popular AI assistants like Claude Desktop and IDE integrations like Cursor.
*    **Advanced Configuration:** Customizing models and settings.

By the end of Part 2, you'll have a powerful search tool at your disposal and a deeper understanding of how to build more complex and capable MCP servers. You'll be well-equipped to start building your own AI agents that can leverage the vast resources of the web and other external systems.
```

## File: mcp/mcp2502-p2.md (Size: 52.07 KB)

```

# AI Assistants part 2: Perplexity Advanced MC

## 1. Understanding MCP and its Benefits: The Key to Unlocking AI's Potential

Before we dive into the specifics of `perplexity-advanced-mcp`, it's crucial to understand the foundation upon which it's built: the **Model Context Protocol (MCP)**.  Think of MCP as a bridge, a translator, or a universal adapter that connects the isolated world of Large Language Models (LLMs) to the dynamic, ever-changing realm of external data and tools.

### 1.1. The Limitations of LLMs Out-of-the-Box

LLMs like GPT-4, Claude, and others are incredibly powerful. They can write coherent text, translate languages, summarize information, and even generate code.  However, their knowledge is fundamentally *static*. It's frozen in time, based on the data they were trained on.  This presents several limitations:

*   **Outdated Information:**  An LLM trained in 2023 knows nothing about events that happened in 2024. Ask it about the current president of a country, and it might give you the correct answer, or it might be completely wrong.
*   **Lack of Real-World Interaction:**  LLMs can't directly interact with the world. They can't book flights, send emails, access your private files, or control devices.
*   **"Hallucinations":**  When faced with a question outside their training data, LLMs can sometimes "hallucinate" – confidently generate incorrect or nonsensical answers.
*   **Limited Context:**  An LLM's context is typically limited to the conversation history.  It can't easily incorporate information from a long document, a database, or a live data feed.

### 1.2. Bridging the Gap: Enter MCP

MCP addresses these limitations by providing a *standardized interface* for LLMs to interact with external systems.  It's like a universal remote control for AI.  Instead of being confined to their internal knowledge, LLMs can use MCP to:

*   **Query Databases:** Retrieve specific data from SQL databases, NoSQL databases, or even spreadsheets.
*   **Access the Web:**  Fetch the content of web pages, search the internet, and extract information.
*   **Interact with APIs:**  Send requests to any API that exposes an MCP interface (e.g., weather services, project management tools, internal company systems).
*   **Execute Code:** Run code snippets in secure environments (e.g., Python, JavaScript).
*   **Manipulate Files:** Read, write, and modify files on a local or remote filesystem.

### 1.3. The MCP Ecosystem: Clients and Servers

The MCP ecosystem consists of two main components:

*   **MCP Clients:** These are the AI assistants, IDE integrations, or applications that *use* MCP.  Examples include:
    *   Claude Desktop
    *   Cursor (VS Code extension)
    *   Custom-built AI agents

*   **MCP Servers:**  These are the "tool providers."  They expose specific functionalities (like web search, file access, or database queries) through a standardized MCP interface.  `perplexity-advanced-mcp` is an example of an MCP server.

### 1.4. How MCP Works: A Simplified Analogy

Imagine you have a very smart robot (the LLM) that can understand and generate language, but it's stuck in a room with no windows or doors.  MCP is like installing a set of intercoms and robotic arms in the room:

1.  **You (the user) give the robot a task:**  "Find me the latest news about the James Webb Space Telescope."
2.  **The robot (via an MCP client) realizes it needs outside information:**  It knows it needs to search the web, but it can't do that directly.
3.  **The client sends a request to an MCP server:**  The client formats the request according to the MCP specification (JSON-RPC), asking for the "search" tool to be used with the query "James Webb Space Telescope".
4.  **The MCP server (like `perplexity-advanced-mcp`) executes the request:**  It uses its underlying search API (Brave, Perplexity, etc.) to fetch the information.
5.  **The server returns the results (again, via MCP):**  The results are formatted according to the MCP specification (structured JSON).
6.  **The client gives the results to the robot:**  The LLM now has the information it needs.
7.  **The robot completes the task:**  It uses its language understanding and generation capabilities to summarize the news and provide you with an answer.

### 1.5. Why This Matters: Benefits of MCP

*   **Modularity:**  You can easily swap out tools and providers without rewriting your entire agent.  Need a different search engine?  Just connect to a different MCP server.
*   **Extensibility:**  Anyone can create and publish MCP servers, expanding the range of capabilities available to AI agents.
*   **Security:** MCP servers can implement access controls, ensuring that LLMs only access authorized data and tools.
*   **Reproducibility:**  Because interactions are standardized, it's easier to track and reproduce the steps an AI agent took to arrive at a result.
*   **Standardization:**  MCP promotes a common language for AI interactions, fostering collaboration and interoperability.

### 1.6. Concrete Example: Beyond Basic Search

Let's say you're a software developer working on a project that uses a library called "Pandas".  You're using Cursor (with MCP integration) and you ask:

"What's the best way to filter a Pandas DataFrame based on multiple conditions?"

Without MCP, the LLM might give you a generic answer based on its training data, which might be outdated.

With `perplexity-advanced-mcp` (or another MCP server providing web search):

1.  Cursor recognizes the need for up-to-date information and sends a request to `perplexity-advanced-mcp`'s `ask_perplexity` tool.
2.  `perplexity-advanced-mcp` uses its configured search provider (e.g., Perplexity) to search the web for relevant information.
3.  The server returns the results, which might include:
    *   Links to the latest Pandas documentation.
    *   Code snippets from Stack Overflow showing best practices.
    *   Blog posts discussing recent updates or changes to Pandas.
4.  Cursor presents this information to the LLM, which can now provide a *much more accurate and relevant* answer, potentially even citing the sources it used.

**1.7 Addressing potential questions from the reader:**

*   **"Isn't this just like using a search engine directly?"**  No.  With MCP, the *AI agent* is in control.  It decides *when* to search, *what* to search for, and *how* to use the results.  It's not just about getting links; it's about integrating external knowledge into the AI's reasoning process.

*   **"Is this secure?"** MCP itself is a protocol, and security is handled at the server level. `perplexity-advanced-mcp` uses API keys for authentication, and you should always be mindful of the data you're exposing to external services.  The best practice is to run your MCP servers in a controlled environment (e.g., locally or within a trusted network).

*   **"What if the search results are irrelevant?"**  The LLM is responsible for filtering and interpreting the results.  Just like a human researcher, it may need to refine its search queries or try different sources to find the best information.  The `complex` query type in `perplexity-advanced-mcp` is specifically designed for more nuanced information gathering.

*  **"Do I need to be an AI expert to use this?"** Absolutely not.  The beauty of MCP is that it simplifies the process of building AI agents.  You can leverage existing tools (like `perplexity-advanced-mcp`) without needing to build everything from scratch.  If you can write basic Python, you can use this.

### 1.7. Conclusion of Chapter 1

MCP represents a significant step forward in building practical, real-world AI applications.  By providing a standardized way for LLMs to access external tools and data, it unlocks a whole new level of capabilities.  `perplexity-advanced-mcp` is a concrete example of how this power can be harnessed, specifically for web search.  In the following chapters, we'll explore how to use this tool in practice and how to integrate it with your favorite AI development environments.





Okay, let's move on to Chapter 2, focusing on "Installation and Setup." This chapter needs to be practical, step-by-step, and ensure the reader can get the system running. We'll cover different installation options and configuration.

---

## 2. Installation and Setup: Getting Started with `perplexity-advanced-mcp`

Now that you understand the power of MCP and how `perplexity-advanced-mcp` can enhance your AI agent's capabilities, let's get it installed and configured.  This chapter will guide you through the process.

### 2.1. Prerequisites

Before you begin, ensure you have the following:

*   **Python 3.10 or higher:** `perplexity-advanced-mcp` requires a relatively recent version of Python.  You can check your Python version by running:

    ```bash
    python --version
    ```

    If you need to install or update Python, we recommend using a package manager like `pyenv` or `conda` to manage multiple Python versions.

*   **`pip` and `uv`:**  We'll be using `pip` (or its faster alternative `uv`) to install the package.  It's usually included with Python installations. If you do not have `uv` installed, you may install it using `pip`.

    ```bash
    pip install uv
    ```

* **An API Key (OpenRouter or Perplexity):**
    You must choose whether to use OpenRouter, or Perplexity, and have an API key.
    - **OpenRouter:** Sign up for an account at [https://openrouter.ai/](https://openrouter.ai/) and obtain an API key from your dashboard.
    - **Perplexity:** Obtain an API key from [https://www.perplexity.ai/](https://www.perplexity.ai/).

### 2.2. Installation Options

There are two main ways to install `perplexity-advanced-mcp`:

*   **From PyPI (Recommended):** This is the easiest way to install the latest stable release.

    ```bash
    pip install perplexity-advanced-mcp
    ```
    or with `uv`:
    ```bash
    uv pip install perplexity-advanced-mcp
    ```

*   **From Source (For Development):** If you want to contribute to the project or use the very latest (potentially unstable) code, you can clone the repository and install it from source.

    ```bash
    git clone https://github.com/code-yeongyu/perplexity-advanced-mcp.git
    cd perplexity-advanced-mcp
    pip install .
    ```
    or with `uv`:
    ```bash
    uv pip install .
    ```

### 2.3. Configuration: Setting Your API Key

`perplexity-advanced-mcp` needs your API key to access either the OpenRouter or Perplexity service.  There are two primary ways to provide this key:

*   **Environment Variables (Recommended):** This is the most secure and flexible method.  Set *either* `OPENROUTER_API_KEY` *or* `PERPLEXITY_API_KEY` in your environment.

    *   **Linux/macOS:**
        ```bash
        export OPENROUTER_API_KEY="your_openrouter_key_here"
        # OR
        export PERPLEXITY_API_KEY="your_perplexity_key_here"
        ```
        You can add these lines to your `.bashrc`, `.zshrc`, or other shell configuration file to make them permanent.

    *   **Windows:**
        ```bash
        setx OPENROUTER_API_KEY "your_openrouter_key_here"
        # OR
        setx PERPLEXITY_API_KEY "your_perplexity_key_here"
        ```
        You may need to restart your terminal or even your computer for these changes to take effect.

* **Command-line Arguments (Less Secure):** You can pass the API key directly when starting the server.  This is less secure because the key might appear in your command history.

    ```bash
    uvx perplexity-advanced-mcp -o your_openrouter_key_here  # For OpenRouter
    # OR
    uvx perplexity-advanced-mcp -p your_perplexity_key_here  # For Perplexity
    ```
    If you're on Windows and using powershell:

    ```bash
    uvx perplexity-advanced-mcp -o "your_openrouter_key_here"
    # OR
    uvx perplexity-advanced-mcp -p "your_perplexity_key_here"
    ```

**Important:** You should only provide *one* API key (either OpenRouter *or* Perplexity). Providing both will result in an error.

### 2.4. Verifying the Installation

Once installed and configured, you can verify that everything is working by starting the MCP server:

```bash
uvx perplexity-advanced-mcp
```

If the installation is successful, you should see output similar to this (the exact output may vary):

```
INFO:     Started server process [12345]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
```

This indicates that the server is running and ready to accept connections from MCP clients.  You can stop the server with `Ctrl+C`.

### 2.5. Troubleshooting

*   **`No module named 'uv'`:**  Make sure you've installed `uv` as described in the prerequisites.
*   **`API Key Error`:**  Double-check that you've set the correct environment variable (`OPENROUTER_API_KEY` or `PERPLEXITY_API_KEY`) and that the key is valid.
*   **`Connection Refused`:**  Ensure that no other process is already using port 8000.  You might need to stop any conflicting services.

### 2.6.  (Optional) Setting up a Virtual Environment

It's highly recommended to use a virtual environment to isolate your project's dependencies.  This prevents conflicts between different Python projects.  Here's how to do it with `uv`:

1.  **Create a virtual environment:**
    ```bash
    uv venv .venv
    ```
    This creates a directory named `.venv` in your project folder.

2.  **Activate the environment:**
    *   **Linux/macOS:**
        ```bash
        source .venv/bin/activate
        ```
    *   **Windows (CMD):**
        ```bash
        .venv\Scripts\activate.bat
        ```
    *   **Windows (PowerShell):**
        ```powershell
        .venv\Scripts\Activate.ps1
        ```

3. **Install `uv`:**
    ```bash
        pip install uv
        uv sync
    ```

    Your terminal prompt should now change to indicate that you're inside the virtual environment (e.g., `(.venv) $`).

4.  **Install `perplexity-advanced-mcp` (inside the activated environment):**
    ```bash
    uv pip install perplexity-advanced-mcp
    ```

Now, any packages you install will be isolated within this environment.  When you're finished, you can deactivate the environment with:

```bash
deactivate
```

### 2.7. Conclusion of Chapter 2

You should now have `perplexity-advanced-mcp` installed and configured, ready to be used as an MCP server. You have a basic understanding of the various configuration options and how to troubleshoot common installation issues. In the next chapter, we will start exploring the API and building our first application.


---


## 3. Exploring the Core Features: Putting `ask_perplexity` to Work

With `perplexity-advanced-mcp` successfully installed and configured, we can now explore its core functionality: the `ask_perplexity` tool. This tool acts as your gateway to leveraging the power of Perplexity or OpenRouter for web searches within your AI agents.

### 3.1. Anatomy of an `ask_perplexity` Call

As we saw in Chapter 1, the `ask_perplexity` tool accepts the following inputs, described by its JSON Schema:

```json
{
    "name": "ask_perplexity",
    "description": "Perplexity is fundamentally an LLM that can search the internet...",
    "inputSchema": {
        "type": "object",
        "properties": {
            "query": {
                "type": "string",
                "description": "The query to search for"
            },
            "query_type": {
                "type": "string",
                "description": "Type of query to determine model selection",
                "enum": [
                    "simple",
                    "complex"
                ]
            },
            "attachment_paths": {
                "type": "array",
                "items": {
                    "type": "string"
                },
                "description": "An optional list of absolute file paths to attach as context"
            }
        },
        "required": [
            "query",
            "query_type"
        ]
    }
}
```

*   **`query` (string, required):** The actual search query you want to perform.  This is the same kind of query you'd type into a regular search engine.
*   **`query_type` (string, required):**  Either `"simple"` or `"complex"`. This determines which underlying LLM model is used.
*   **`attachment_paths` (list[str], optional):**  A list of *absolute* file paths.  The contents of these files will be included as context for the query.

### 3.2. Simple Queries: Quick and Efficient Information Retrieval

Simple queries are ideal for straightforward, fact-based questions. They utilize a smaller, faster, and more cost-effective LLM.

Let's see a conceptual example (remember, this is how an MCP *client* would interact, not a direct Python call):

```python
# Conceptual example - how an MCP client would call the tool

async def simulate_mcp_call(tool_name: str, arguments: dict) -> str:
    """Simulates calling an MCP tool.  This is a placeholder."""
    print(f"---- SIMULATED MCP CALL: {tool_name} ----")
    print(f"Arguments: {arguments}")
    # In a real MCP client, this would send a request to the server.
    return "<simulated_response>Paris is the capital of France.</simulated_response>"

async def simple_query_example():
    result = await simulate_mcp_call(
        "ask_perplexity",
        {"query": "What is the capital of France?", "query_type": "simple"},
    )
    print(f"Simple Query Result: {result}")

# asyncio.run(simple_query_example()) # Commented out for Markdown
```

**Output (Simulated):**

```
---- SIMULATED MCP CALL: ask_perplexity ----
Arguments: {'query': 'What is the capital of France?', 'query_type': 'simple'}
Simple Query Result: <simulated_response>Paris is the capital of France.</simulated_response>
```

In this case, the client sends a simple query. `perplexity-advanced-mcp` uses the configured `simple` model (e.g., `perplexity/sonar` for OpenRouter or `sonar-pro` for Perplexity) to quickly retrieve the answer.

### 3.3. Complex Queries: In-Depth Reasoning and Analysis

Complex queries are designed for tasks that require more in-depth analysis, synthesis of information from multiple sources, or multi-step reasoning.  They utilize a larger, more powerful LLM.

```python
# Conceptual example - how an MCP client would call the tool

async def complex_query_example():
    result = await simulate_mcp_call(
        "ask_perplexity",
        {
            "query": "Compare and contrast the economic policies of France and Germany in 2024.",
            "query_type": "complex",
        },
    )
    print(f"Complex Query Result: {result}")

# asyncio.run(complex_query_example()) # Commented out for Markdown
```

**Output (Simulated):**

```
---- SIMULATED MCP CALL: ask_perplexity ----
Arguments: {'query': 'Compare and contrast the economic policies of France and Germany in 2024.', 'query_type': 'complex'}
Complex Query Result: <simulated_response>Here's a comparison of French and German economic policies in 2024... (detailed analysis would follow).</simulated_response>
```

Here, the query requires more than just a simple lookup.  The LLM needs to gather information from multiple sources, understand the economic policies of both countries, and then perform a comparison.  This is where the `complex` query type shines.

### 3.4. Leveraging File Attachments: Providing Context

One of the most powerful features of `perplexity-advanced-mcp` is the ability to attach local files as context for your queries.  This allows the LLM to draw on information that isn't publicly available on the web.

**Important:** File paths *must* be absolute.

Let's create a simple text file named `company_data.txt` (in the same directory as your script) with the following content:

```
Company: Acme Corp
Industry: Widget Manufacturing
Revenue (2023): $120 million
Employees: 500
Headquarters: Anytown, USA
```

Now, let's see a conceptual example of using this file as context:

```python
# Conceptual example - how an MCP client would call the tool
import os

async def file_attachment_example():
    # Get the absolute path to the file
    current_dir = os.path.dirname(os.path.abspath(__file__))
    file_path = os.path.join(current_dir, "company_data.txt")

    #Ensure file exists at the correct path.

    result = await simulate_mcp_call(
        "ask_perplexity",
        {
            "query": "What was the revenue of Acme Corp in 2023?",
            "query_type": "simple",  # Even simple queries can use attachments
            "attachment_paths": [file_path],
        },
    )
    print(f"File Attachment Result: {result}")


# asyncio.run(file_attachment_example())  # Commented out for Markdown

```

**Output (Simulated):**

```
---- SIMULATED MCP CALL: ask_perplexity ----
Arguments: {'query': 'What was the revenue of Acme Corp in 2023?', 'query_type': 'simple', 'attachment_paths': ['/absolute/path/to/company_data.txt']}
File Attachment Result: <simulated_response>Acme Corp's revenue in 2023 was $120 million.</simulated_response>
```

The LLM, guided by the MCP server, has access to the *content* of `company_data.txt`, even though that file isn't publicly available online.  The server reads the file and includes its contents within the request sent to the underlying search API.  The LLM then uses this context to answer the question.

### 3.5. Interpreting the Results

The `ask_perplexity` tool returns a string, which is the LLM's generated response.  This response will be based on the information found by the search API, combined with the LLM's own knowledge and reasoning abilities.

### 3.6. Caveats and Considerations

*   **File Size Limits:** While `perplexity-advanced-mcp` handles file attachments, very large files might lead to performance issues or exceed API limits.  Consider pre-processing or summarizing large files before attaching them.  The MCP server *does not* automatically truncate or summarize files.  That responsibility lies with the client.
*   **Relevance Filtering:** The LLM is responsible for determining the relevance of the search results and using them to answer your query.  Sometimes, the search results might not be directly relevant, and the LLM might need to make inferences.
*   **API Limitations:** The underlying search APIs (OpenRouter and Perplexity) have their own limitations and quotas.  Be aware of these when designing your applications.
*   **Error Handling:**  In a real application, you should implement proper error handling to gracefully manage API errors, timeouts, and other potential issues.

### 3.7. Conclusion of Chapter 3

This chapter provided a practical overview of the `ask_perplexity` tool, demonstrating its versatility in handling different query types and leveraging file attachments. You've learned how to formulate requests and interpret the results. In the next chapter, we'll look at integrating this tool into larger workflows, specifically focusing on using it within Claude Desktop and VSCode (Cursor).

---

## 4. Integration with Claude Desktop: Supercharging Your AI Assistant

Claude Desktop is a powerful AI assistant that can be significantly enhanced by connecting it to external tools via the Model Context Protocol (MCP).  In this chapter, we'll walk through the process of integrating `perplexity-advanced-mcp` with Claude Desktop, allowing Claude to leverage web search capabilities for more informed and up-to-date responses.

### 4.1. Why Integrate with Claude Desktop?

Claude Desktop provides a user-friendly interface for interacting with Anthropic's Claude LLMs. By integrating `perplexity-advanced-mcp`, you give Claude the ability to:

*   **Access Current Information:** Overcome the knowledge cutoff limitations of the base LLM.
*   **Retrieve Specific Data:** Fetch information from websites, articles, and other online resources.
*   **Improve Accuracy:** Reduce hallucinations by grounding responses in real-world data.
*   **Expand Capabilities:**  Enable Claude to handle a wider range of tasks that require external information.

### 4.2. Prerequisites

Before you begin, make sure you have:

*   **Claude Desktop Installed:** Download and install the latest version from [https://claude.ai/download](https://claude.ai/download).
*   **`perplexity-advanced-mcp` Installed and Configured:** Follow the instructions in Chapter 2 to install the package and set up your API key (either OpenRouter or Perplexity).
*   **A Running MCP Server:** You need to have the `perplexity-advanced-mcp` server running.  You can start it with:
    ```bash
    uvx perplexity-advanced-mcp -o <your_openrouter_key>  # Or -p for Perplexity
    ```
    or using environment variables:
        ```bash
        OPENROUTER_API_KEY="your_key_here" uvx perplexity-advanced-mcp
        # OR
        PERPLEXITY_API_KEY="your_key_here" uvx perplexity-advanced-mcp
        ```
        Make sure the key matches your chosen provider.
* Basic text editor

### 4.3. Step-by-Step Integration Guide

1.  **Locate the Claude Desktop Configuration File:**

    *   **macOS:**  The configuration file is usually located at:
        `~/Library/Application Support/Claude/claude_desktop_config.json`

        You can open this in Finder by pressing `Command + Shift + G` and pasting the path.

    *   **Windows:**  The configuration file is typically found at:
        `C:\Users\<Your Username>\AppData\Roaming\Claude\claude_desktop_config.json`

        Replace `<Your Username>` with your actual Windows username.  You can access this folder by typing `%APPDATA%\Claude` in the Windows Explorer address bar.

    * **Linux:** The path can vary depending on your distribution. It's generally in a hidden directory in your home folder, for instance, `~/.config/Claude/claude_desktop_config.json`.

    If you cannot find this file, launch Claude Desktop and navigate to Settings, then Developer, then Edit Config. This will take you to the file to modify it.

2.  **Edit the Configuration File:**

    Open `claude_desktop_config.json` in a text editor (like VS Code, Sublime Text, Notepad, etc.).  You'll see a JSON structure.  We need to add a new entry to the `mcpServers` array.

    **Important:** JSON is very strict about syntax.  Make sure you add commas correctly, and don't introduce any extra characters.

    Here's what a *complete* `claude_desktop_config.json` file might look like after adding the configuration.  The important part is the `mcpServers` section.  Your file might have other settings, but *do not* modify those unless you know what you're doing.

    ```json
    {
      "windowState": {
        "bounds": {
          "x": 253,
          "y": 181,
          "width": 1152,
          "height": 748
        },
        "isMaximized": false,
        "isFullScreen": false
      },
      "mcpServers": {
        "perplexity": {
          "command": "uvx",
          "args": [
            "perplexity-advanced-mcp",
            "-o",
            "YOUR_OPENROUTER_API_KEY"
          ],
            "env": {}
        }
      },
      "serviceWorkers": []
    }
    ```

    *   **`"perplexity"`:**  This is a *name* you choose for this server.  You'll use this name when invoking the tool from within Claude.  You can use a different name if you prefer.
    *   **`"command": "uvx"`:** This specifies the command to run the server.  We're using `uvx` because it's the recommended way to run `perplexity-advanced-mcp`.
    *   **`"args": [...]`:**  These are the arguments passed to the `uvx` command.
        *   `"perplexity-advanced-mcp"`:  This is the name of the package we installed.
        *   `-o YOUR_OPENROUTER_API_KEY`:  Replace `YOUR_OPENROUTER_API_KEY` with your *actual* OpenRouter API key.  **This is crucial.**  If you're using Perplexity, use `-p YOUR_PERPLEXITY_API_KEY` instead, and replace `YOUR_PERPLEXITY_API_KEY` with your Perplexity API key.
        *  `"env": {}`: This is an empty object. You can optionally include environment variables here.
    * **Alternative Configuration (Using Environment Variables)**

        If you set the `OPENROUTER_API_KEY` or `PERPLEXITY_API_KEY` environment variable (as described in Chapter 2), you can simplify the configuration:
        ```json
        {
          "mcpServers": {
            "perplexity": {
              "command": "uvx",
              "args": ["perplexity-advanced-mcp"]
            }
          }
        }
        ```

3.  **Restart Claude Desktop:**  After saving the `claude_desktop_config.json` file, you *must* completely restart Claude Desktop.  Just closing the window isn't enough; you need to quit the application entirely.  On macOS, you can usually do this by right-clicking the Claude icon in the dock and choosing "Quit."  On Windows, look for it in the system tray.

### 4.4. Testing the Integration

Once Claude Desktop has restarted, you can test the integration:

1.  **Start a New Chat:** Open a new chat window in Claude Desktop.
2.  **Invoke the Tool:**  Type a message that uses the `ask_perplexity` tool.  Remember to use the server name you configured (e.g., `"perplexity"`).
3.  **Example:** `What's the population of Tokyo? /tool perplexity.ask_perplexity {"query": "population of Tokyo", "query_type": "simple"}`
4.  **Observe the Output:**  Claude should send the request to your running `perplexity-advanced-mcp` server.  You'll see activity in the terminal where you started the server.  Claude will then incorporate the search results into its response.

### 4.5. Troubleshooting

*   **Claude doesn't recognize the tool:**  Double-check the server name in your `claude_desktop_config.json` file.  Make sure you're using that name in your Claude prompts.  Also, ensure you restarted Claude *completely* after editing the config.
*   **Errors in the server terminal:**  If you see errors in the terminal where `perplexity-advanced-mcp` is running, carefully examine the error messages.  They often provide clues about what went wrong (e.g., invalid API key, network issues).
*  **`No module named ...`:** Check that the dependencies are met, by running `uv sync` inside the project folder.
*   **"Tool calls are not allowed by this message":**  Make sure you're using a model that supports tool use.  For example, `claude-3-5-sonnet` *does* support tool use; older Claude models might not. Also you have to call the tool by using `/tool servername.toolname`.

### 4.6. Best Practices and Tips

*   **Be Specific in Your Queries:** The more specific your query, the better the results.
*   **Use `complex` for Detailed Analysis:**  If you need Claude to synthesize information from multiple sources or perform in-depth reasoning, use the `complex` query type.
*   **Experiment with Attachments:**  Try providing context files to see how they improve the quality of Claude's responses.
*   **Monitor Costs:**  Keep track of your API usage, especially with the `complex` query type, as it uses more powerful (and more expensive) models.

### 4.7. Conclusion of Chapter 4

You've now successfully integrated `perplexity-advanced-mcp` with Claude Desktop, giving your AI assistant the power of web search.  This opens up a wide range of possibilities for more informed and accurate responses. In the next chapter, we'll cover integration with VS Code and Cursor, which offers a similar workflow but within a code editor environment.

---

Okay, let's create Chapter 5: "Integration with VS Code (Cursor)." This chapter will be very similar in structure to Chapter 4, but tailored to the specifics of Cursor and VS Code.

---


## 5. Integration with VS Code (Cursor): Bringing Search to Your Code Editor

Cursor is an AI-first code editor built on top of VS Code, designed to make coding more efficient and intuitive. It has built-in support for the Model Context Protocol (MCP), which means integrating `perplexity-advanced-mcp` is straightforward. This chapter will show you how to connect Cursor to your running `perplexity-advanced-mcp` server and leverage its search capabilities directly within your coding workflow. We will also discuss using it with VS Code.

### 5.1. Why Integrate with a Code Editor?

Integrating web search directly into your code editor provides several advantages:

*   **Contextual Documentation:** Quickly find documentation for libraries, APIs, or code snippets without leaving your editor.
*   **Code Examples:** Search for relevant code examples and best practices while you're writing.
*   **Error Resolution:** Investigate error messages and find solutions directly within your workflow.
*   **Fact-Checking:** Verify information or assumptions while writing code or documentation.
*   **Staying Up-to-Date:** Access the latest information about libraries, frameworks, and best practices.

### 5.2. Prerequisites

Before you begin, ensure you have:

*   **Cursor or VS Code Installed:** Download and install Cursor from [https://cursor.com/](https://cursor.com/). If you want to use VS Code, install it from: [https://code.visualstudio.com/](https://code.visualstudio.com/).
*   **`perplexity-advanced-mcp` Installed and Configured:** Follow the instructions in Chapter 2.
*   **A Running MCP Server:** Start the server using:

    ```bash
    uvx perplexity-advanced-mcp -o <your_openrouter_key>  # Or -p for Perplexity
    ```
    or using environment variables:
        ```bash
        OPENROUTER_API_KEY="your_key_here" uvx perplexity-advanced-mcp
        # OR
        PERPLEXITY_API_KEY="your_key_here" uvx perplexity-advanced-mcp
        ```

### 5.3. Step-by-Step Integration Guide (Cursor)

1.  **Open Cursor Settings:** Go to File > Settings > Settings (or use the shortcut `Ctrl+,` or `Cmd+,`).
2.  **Navigate to MCP Servers:** In the Settings search bar, type "MCP Servers".  This should bring up the relevant configuration section.
3.  **Add a New Server:** Click the "Add Item" button to create a new MCP server entry.
4.  **Configure the Server:** Fill in the following fields:

    *   **Name:**  `perplexity` (or any name you prefer)
    *   **Type:** `cmd`
    *   **Command:**  `uvx perplexity-advanced-mcp`
    * **Arguments**: `-o YOUR_OPENROUTER_API_KEY` (replace with your API key, or `-p` for Perplexity)
    * **Environment variables**: (Optional) add your API Key here:
        ```json
        {
            "OPENROUTER_API_KEY": "your_key_here"
        }
        ```
    *   **Description:** (Optional) A short description, e.g., "Perplexity Search"

    The configuration should look something like this (with your actual API key):

    ![Cursor MCP Server Configuration](./images/cursor_config.png)

    *Alternatively, if using environment variables:*

    ![Cursor MCP Server Configuration - Env Vars](./images/cursor_config_env.png)

5.  **Save Changes:**  Cursor saves settings automatically. You do *not* need to restart Cursor for the changes to take effect.

### 5.4. Step-by-Step Integration Guide (VS Code)

To use Perplexity Advanced MCP as a tool provider in VS Code, you need to have an extension that supports the Model Context Protocol. One such extension is [Continue](https://continue.dev/). The following steps will guide you on how to install and configure Continue to work with your MCP server.

1.  **Install Continue:**
    -   Open VS Code.
    -   Go to the Extensions view by clicking on the Extensions icon in the Activity Bar on the side of the window or by pressing `Ctrl+Shift+X` (or `Cmd+Shift+X` on macOS).
    -   Search for "Continue" and click Install.

2.  **Configure Continue:**
    -   After installation, Continue will add a new icon to the Activity Bar. Click on it to open the Continue sidebar.
    -   Continue works with a configuration file that specifies the available models and tools. By default, Continue looks for a file named `config.py` or `config.json` in a `.continue` directory within your workspace or your home directory. Let's configure it to use our MCP server.
    -   Create a file named `config.json` in a `.continue` directory at the root of your workspace (or in your home directory if you prefer global settings).
    -   Add the following configuration to `config.json`:

        ```json
        {
            "models": [
                {
                    "title": "Perplexity Advanced MCP",
                    "provider": "mcp",
                    "model": "perplexity",
                    "api_key": "YOUR_OPENROUTER_OR_PERPLEXITY_API_KEY",
                    "completion_options": {
                        "model": "perplexity"
                    }
                }
            ],
            "system_message": "You are a helpful assistant that can access the internet through Perplexity Advanced MCP."
        }
        ```
    - **Important:**
      *   Set `"model": "perplexity"` which refers to the MCP server.
      *   Replace `YOUR_OPENROUTER_OR_PERPLEXITY_API_KEY` with a placeholder.  You'll set this as an environment variable, which is the recommended way to handle API keys.

3.  **Set API Key as an Environment Variable:**
    -   Follow the same steps outlined in section 2.3 of this guide, either setting the `OPENROUTER_API_KEY` or the `PERPLEXITY_API_KEY` environment variable.

4. **Start the MCP Server:**
   - In a separate terminal, ensure your MCP server is running. Use the same command as for the other setups:
    ```bash
    uvx perplexity-advanced-mcp -o <your_openrouter_key>  # Or -p for Perplexity
    ```
    or using environment variables:
        ```bash
        OPENROUTER_API_KEY="your_key_here" uvx perplexity-advanced-mcp
        # OR
        PERPLEXITY_API_KEY="your_key_here" uvx perplexity-advanced-mcp
        ```
    -   You should see output indicating that the server has started.

5. **Use Continue in VS Code:**

    - Open a file in VS Code.
    - Select some text, right-click, and choose "Continue: Ask a Question".
    - You can also use the Continue sidebar to start a chat.
    - Since we've configured "Perplexity Advanced MCP" as a model, Continue will automatically use it.  You don't need to explicitly invoke it with `/tool`.

### 5.5. Using the Tool in Cursor/VS Code

Cursor and other clients provide the `@` syntax to access MCPs.

   ```python
   # Example usage within Cursor/VS Code
   @perplexity
   ```

Then, you can ask questions like:
```
What is the current version of python?
```
or
```
What is the content of the file /Users/myuser/file.txt?
```

### 5.6. Troubleshooting (Cursor/VS Code)

The troubleshooting steps are largely the same as with Claude Desktop:

*   **Check Server Name:** Ensure the name you're using in your prompts (e.g., `@perplexity`) matches the name you configured.
*   **Server Running:**  Verify that the `perplexity-advanced-mcp` server is running in a separate terminal.
*   **API Key:**  Double-check your API key.
*   **Cursor/VS Code Restart:** Sometimes, restarting Cursor/VS Code can resolve configuration issues.
*   **Error Messages:** Pay close attention to any error messages in the Cursor/VS Code output or in the terminal where the server is running.
* **Check Client Config:** Make sure you have the correct configuration in Cursor settings or in VS Code Continue configuration.

### 5.7. Conclusion of Chapter 5

This chapter has shown you how to integrate `perplexity-advanced-mcp` with both Cursor and VS Code using the Continue extension, bringing the power of web search directly into your code editor. This integration can dramatically improve your development workflow by providing quick access to information and code examples.  The next chapter will cover more advanced topics.

---

Okay, let's draft Chapter 6: "Advanced Usage: Customization, Tips, and Tricks". This chapter will delve into more advanced features, configuration options, and best practices for getting the most out of `perplexity-advanced-mcp`.

---

## 6. Advanced Usage: Customization, Tips, and Tricks

You've now mastered the basics of `perplexity-advanced-mcp` and integrated it with your favorite AI assistant or IDE. This chapter explores more advanced features, configuration options, and best practices to help you tailor the tool to your specific needs and maximize its effectiveness.

### 6.1. Customizing the Underlying LLM Models (config.py)

The `src/perplexity_advanced_mcp/config.py` file is the central location for configuring the behavior of the MCP server.  One of the most important settings you can customize is the choice of underlying LLM models.

```python
# src/perplexity_advanced_mcp/config.py

PROVIDER_CONFIG: dict[ProviderType, ModelConfig] = {
    "openrouter": {
        "models": {
            QueryType.SIMPLE: "perplexity/sonar",
            QueryType.COMPLEX: "perplexity/sonar-reasoning",
        },
        "key": None,  # API key will be set at runtime
    },
    "perplexity": {
        "models": {
            QueryType.SIMPLE: "sonar-pro",
            QueryType.COMPLEX: "sonar-reasoning-pro",
        },
        "key": None,  # API key will be set at runtime
    },
}
```

*   **`PROVIDER_CONFIG`:** This dictionary defines the settings for each supported provider (currently `openrouter` and `perplexity`).
*   **`models`:**  This nested dictionary maps `QueryType` (either `SIMPLE` or `COMPLEX`) to the specific model ID to use.
*   **`key`:** This field is intentionally left as `None`.  The API key is provided via environment variables or command-line arguments, *not* directly in the configuration file for security reasons.

**Changing Models:**

To use a different model, simply modify the `models` dictionary.  For example, to use `mistralai/mistral-medium` for complex queries via OpenRouter, you would change the configuration to:

```python
    "openrouter": {
        "models": {
            QueryType.SIMPLE: "perplexity/sonar",
            QueryType.COMPLEX: "mistralai/mistral-medium",  # Changed model
        },
        "key": None,
    },
```

**Important Considerations:**

*   **Model Availability:**  Make sure the model ID you specify is actually available from the provider you're using.  Refer to the OpenRouter or Perplexity documentation for a list of supported models.
*   **Model Capabilities:** Different models have different strengths and weaknesses.  Some are better at specific tasks (e.g., coding, creative writing) than others.  Experiment to find the best model for your needs.
*   **Cost:**  More powerful models are typically more expensive to use.  Be mindful of the cost implications of your model choices.

### 6.2. Understanding and Handling Errors

`perplexity-advanced-mcp` is designed to be robust, but errors can still occur (e.g., network issues, invalid API keys, exceeding rate limits).  The `api_client.py` module defines several custom exceptions:

*   **`APIKeyError`:** Raised when the required API key is missing or invalid.
*   **`APIRequestError`:** Raised when the API request itself fails (e.g., due to a network error or an invalid request).  This often wraps the underlying `httpx` exception.

When using `perplexity-advanced-mcp` programmatically (rather than through an MCP client like Cursor), you should wrap your calls in `try...except` blocks to handle these exceptions gracefully.  MCP clients like Cursor will generally handle errors from the server and present them to the user.

### 6.3. Using MCP Server Arguments

The MCP server itself (`perplexity_advanced_mcp/cli.py`) takes two arguments:

```
uvx perplexity-advanced-mcp -o <your_openrouter_key>
```

or

```
uvx perplexity-advanced-mcp -p <your_perplexity_api_key>
```

*   `-o` or `--openrouter-api-key`: Specifies the OpenRouter API key.
*   `-p` or `--perplexity-api-key`: Specifies the Perplexity API key.

As mentioned before, you must use exactly *one* of these options.

### 6.4.  Logging and Debugging

`perplexity-advanced-mcp` uses Python's built-in `logging` module.  By default, it logs at the `INFO` level.  You can adjust the logging level by modifying the `logger` in `cli.py`, using `--verbose` flag, or via code:
    
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

For more detailed debugging, you can increase the log level to `DEBUG`.  This will provide more verbose output, including the raw requests and responses being sent to the search APIs.  This can be helpful for diagnosing issues.

When the MCP server is run with verbosity, it will print a lot more information to your command prompt.

### 6.5.  Rate Limits and Best Practices

Both OpenRouter and Perplexity have rate limits (requests per second/minute/month).  `perplexity-advanced-mcp` includes basic rate limit handling.

*   It will automatically retry requests that fail due to rate limiting (using an exponential backoff strategy).

However, you should still be mindful of rate limits, especially when using the `complex` query type or making many requests in a short period.

**Best Practices:**

*   **Use `simple` queries when possible:** They are faster and cheaper.
*   **Cache results:** If you're making the same query repeatedly, consider caching the results locally to avoid unnecessary API calls.
*   **Batch requests:**  If you have a large number of queries to make, consider batching them (this is not currently implemented in `perplexity-advanced-mcp` but is a potential future enhancement).
*   **Monitor your usage:** Keep an eye on your API usage (via the OpenRouter or Perplexity dashboards) to avoid exceeding your limits.

**6.6 Contributing and Extending**

`perplexity-advanced-mcp` is open source, and contributions are welcome!  Here are some ways you can contribute:

*   **Report Issues:**  If you find a bug or have a suggestion, please open an issue on the GitHub repository.
*   **Submit Pull Requests:**  If you've made an improvement to the code, submit a pull request.
*   **Add Support for New Providers:**  You can extend `perplexity-advanced-mcp` to support other search APIs by following the pattern used for OpenRouter and Perplexity.  This typically involves:
    *   Creating a new client module (e.g., `src/perplexity_advanced_mcp/proxy/newprovider/client.py`).
    *   Updating the `PROVIDER_CONFIG` in `config.py`.
    *   Adding the new provider to the CLI.
*   **Improve Documentation:** Help make the documentation clearer, more complete, or more accurate.

### 6.6. Conclusion of Chapter 6

This chapter covered advanced topics, including customizing the underlying LLM models, understanding and handling errors, configuring logging, and being mindful of rate limits.  You now have the knowledge to fine-tune `perplexity-advanced-mcp` to your specific needs and contribute to its development. This also gives you a sense of the code structure.

---

Okay, let's create Chapter 7. Since we've covered the core functionality, installation, usage with clients, and advanced configuration, this final chapter will focus on more specific, practical use cases and scenarios. We'll tie together the concepts from previous chapters and show how `perplexity-advanced-mcp` can be used in real-world applications. We will focus on using the command line, and we'll include example outputs. We'll also discuss limitations and future directions.

---

## 7. Practical Use Cases and Scenarios

This chapter presents practical examples of how `perplexity-advanced-mcp` can be used in various scenarios, demonstrating its versatility and power as an MCP server. We'll focus on using the command-line interface (`uvx`) for these examples, as it's the most direct way to interact with the server. Remember to have your `OPENROUTER_API_KEY` or `PERPLEXITY_API_KEY` environment variable set, or provide it using the `-o` or `-p` flags, respectively.

### 7.1. Quick Fact Retrieval (Simple Query)

One of the most common uses for an AI assistant is to quickly retrieve factual information. `perplexity-advanced-mcp` excels at this using its `simple` query type.

**Example:**

```bash
uvx perplexity-advanced-mcp -o "YOUR_OPENROUTER_API_KEY" ask-perplexity --query "What is the highest mountain in the world?" --query-type simple
```
*Replace "YOUR_OPENROUTER_API_KEY" with your key.*

**Expected Output (may vary slightly):**

```
<simulated_response>The highest mountain in the world is Mount Everest.</simulated_response>
```

This demonstrates a straightforward question-answer interaction.  The `simple` query type uses a smaller, faster model (like `perplexity/sonar`), making it ideal for this kind of task.

### 7.2. In-Depth Research (Complex Query)

For more complex inquiries requiring synthesis of information from multiple sources, the `complex` query type is more appropriate.

**Example:**

```bash
uvx perplexity-advanced-mcp -o "YOUR_OPENROUTER_API_KEY" ask-perplexity --query "What are the main causes and potential solutions for climate change?" --query-type complex
```

**Expected Output (abridged):**

```
<simulated_response>
The main causes of climate change are... (detailed explanation with multiple paragraphs).

Potential solutions include... (detailed explanation, possibly citing multiple sources).
</simulated_response>
```

This example demonstrates a more complex query that requires the LLM to synthesize information from multiple sources and provide a more comprehensive answer.

### 7.3. Code Explanation with File Attachment

One of the most powerful features of `perplexity-advanced-mcp` is the ability to attach local files as context. Let's say you have a Python file (`example.py`) and you want to understand a specific function:

**example.py:**

```python
def factorial(n):
    """
    Calculates the factorial of a non-negative integer.
    """
    if n == 0:
        return 1
    else:
        return n * factorial(n-1)
```

**Example:**

```bash
# First, get the *absolute* path to your file.  On Linux/macOS:
# realpath example.py  # Use this command to get the absolute path

uvx perplexity-advanced-mcp -o "YOUR_OPENROUTER_API_KEY" ask-perplexity --query "Explain the factorial function in example.py" --query-type complex --attachment-paths "/absolute/path/to/example.py"
```

**Expected Output (abridged):**

```
<simulated_response>
The `factorial` function in `example.py` calculates the factorial of a non-negative integer using recursion... (detailed explanation of the code).
</simulated_response>
```

The LLM can now access the *content* of `example.py` and provide a relevant explanation. This is a huge advantage for code-related tasks.  Remember to *always use absolute paths* for attachments.

### 7.4. Using Perplexity vs. OpenRouter

The examples above use OpenRouter (`-o`). To use Perplexity instead, you would replace the `-o` flag with `-p` and provide your Perplexity API key:

```bash
uvx perplexity-advanced-mcp -p "YOUR_PERPLEXITY_API_KEY" ask-perplexity --query "..." --query-type simple
```

The choice between OpenRouter and Perplexity depends on your specific needs, budget, and preferred models.  Experiment with both to see which works best for you.

### 7.5. Combining with Other Tools (Advanced)

`perplexity-advanced-mcp` is designed to be *composable*.  You can combine it with other MCP servers to create even more powerful workflows.  For example, you could use it in conjunction with:

*   **A filesystem server:** To read and write files, allowing the LLM to interact with your local file system.
*   **A code execution server:** To allow the LLM to run code snippets (with appropriate security precautions!).
*   **A custom server:** You could build your own MCP server to expose any functionality you need.

By combining these tools, you can build sophisticated AI agents that can perform complex, multi-step tasks. This is a concept that is out of the scope for a basic usage, but it is an option to consider for more complex scenarios.

### 7.6. Limitations

*   **File Size:** While file attachments are supported, extremely large files may cause issues.  Consider pre-processing or summarizing large files before attaching them.
*   **API Dependencies:**  The tool relies on external APIs (OpenRouter or Perplexity).  Availability and performance depend on these services.
*   **Cost:**  Using LLMs and search APIs can incur costs.  Be mindful of your usage and API pricing.
* **Rate Limiting:**  The search APIs are rate-limited, so be careful about making too many requests in a short period of time.
* **Security**: Use a .env file or similar to store API Keys, and do not push them to git.

### 7.7. Future Directions

`perplexity-advanced-mcp` is an evolving project.  Some potential future enhancements include:

*   **Support for More Search Providers:**  Adding support for other search APIs (e.g., Google Search, Bing).
*   **Improved Error Handling:**  Providing more informative error messages and handling edge cases more gracefully.
*   **Streaming Support:**  Enabling streaming responses for long-running queries.
*   **Caching:**  Implementing caching to reduce API calls and improve performance.

### 7.8. Conclusion

`perplexity-advanced-mcp` provides a powerful and flexible way to integrate web search into your AI workflows. By understanding its core features, configuration options, and limitations, you can leverage its capabilities to build more intelligent and informed AI agents.  The ability to combine it with other MCP servers opens up a vast range of possibilities for creating sophisticated AI applications.
```

## File: mcp/mcp2502-p3.md (Size: 64.34 KB)

```

# AI Assistants part 3: Advanced Agents, Reliability and the Model Context Protocol (MCP)

(Focus on `mcp-agent`)

## 1. Introduction to `mcp-agent`

### 1.1. What is `mcp-agent`? A Framework for Clients, Servers, and Agents

In the previous parts of this tutorial series, you learned how to use `mcp-cli` to interact with MCP servers and how to build a simple search server (`search-server`).  While `mcp-cli` is excellent for exploration and testing, it's not designed for building complex, production-ready AI agents. That's where `mcp-agent` comes in.

`mcp-agent` is a Python framework for building sophisticated AI agents that leverage the Model Context Protocol (MCP). It's designed for:

*   **Composability:**  `mcp-agent` provides building blocks (called "workflows") that can be easily combined to create complex agent behaviors. You can chain together different LLMs, tools, and routing strategies.
*   **Reliability:**  `mcp-agent` supports robust error handling, retries, and (optionally) distributed execution using Temporal (more on that later).
*   **Extensibility:**  It's easy to add support for new MCP servers, LLMs, and workflow patterns.
*   **Production Readiness:** `mcp-agent` is built with production deployments in mind, providing features like structured logging and tracing.

`mcp-agent` can be used to build:

*   **MCP Clients:** Applications that *use* the capabilities exposed by MCP servers.
*   **MCP Servers:** Applications that *provide* capabilities to other MCP clients.  `mcp-agent` includes tools to simplify server creation.
*   **AI Agents:**  Systems that combine LLMs, tools, and workflows to achieve complex goals.  An agent can act as both a client *and* a server.

### 1.2. Key Concepts: `MCPApp`, `Agent`, `AugmentedLLM`, `Executor`

Let's break down the core components of `mcp-agent`:

*   **`MCPApp`:** This is the central class for your `mcp-agent` application.  It handles:
    *   Loading configuration (from `mcp_agent.config.yaml` and `mcp_agent.secrets.yaml`).
    *   Initializing the `Executor` (more on that below).
    *   Providing access to the `ServerRegistry` (for managing MCP server connections).
    *   Managing the application's lifecycle (initialization and cleanup).
    *   Providing access to the shared `Context` object

    You'll typically create a single `MCPApp` instance for your entire application.

*   **`Agent`:** An `Agent` represents a specific AI agent with a defined purpose and capabilities.  Key features:
    *   **`name`:**  A unique identifier for the agent.
    *   **`instruction`:** A natural language description of the agent's role and capabilities. This is used as a system prompt for the LLM. The instruction can be a string or a callable.
    *   **`server_names`:**  A list of MCP server names that this agent can connect to.  This gives the agent access to the tools exposed by those servers.
    *   **`functions`**: Local functions that the agent can use.
    *   **`attach_llm()`:**  A method to associate an `AugmentedLLM` instance with the agent.
    *  **`human_input_callback`:** Optionally, a function that gets called to handle request for human input.

*   **`AugmentedLLM`:** This is an abstract base class for Large Language Models (LLMs) that have been *augmented* with the ability to use MCP tools.  `mcp-agent` provides concrete implementations:
    *   **`OpenAIAugmentedLLM`:**  Uses OpenAI's models (e.g., GPT-4).
    *   **`AnthropicAugmentedLLM`:**  Uses Anthropic's models (e.g., Claude).

    An `AugmentedLLM` can:
    *   Generate text (like a regular LLM).
    *   Call tools provided by MCP servers.
    *   Maintain a conversation history.

    The `AugmentedLLM` class provides a standardized interface, so you can easily switch between different LLM providers.

*   **`Executor`:** The `Executor` is responsible for actually *running* the tasks that make up your agent's workflows.  `mcp-agent` provides two executor implementations:
    *   **`AsyncioExecutor`:**  The default executor.  It uses Python's built-in `asyncio` library for concurrency.  This is suitable for many applications, especially those that are I/O-bound (e.g., making network requests).
    *   **`TemporalExecutor`:**  A more advanced executor that uses [Temporal](https://temporal.io/) for distributed, reliable execution.  This is recommended for production deployments where durability and fault tolerance are critical.  (Using the `TemporalExecutor` requires setting up a Temporal server.)

### 1.3.  `mcp-agent` vs. `mcp-cli` vs. Upsonic: Understanding the Roles

It's important to understand how `mcp-agent` relates to `mcp-cli` and the Upsonic framework:

*   **`mcp-cli`:**  A command-line tool for *exploring* and *testing* MCP servers.  It's great for:
    *   Manually sending requests to servers.
    *   Discovering available tools and resources.
    *   Debugging server connections.

    However, `mcp-cli` is *not* intended for building production applications.  It's a low-level tool.

*   **`mcp-agent`:**  A Python framework for building *production-ready* AI agents that use MCP.  It provides:
    *   Abstractions for managing server connections.
    *   Classes for representing agents and LLMs.
    *   Building blocks for creating complex workflows.
    *   Support for reliable, distributed execution (with Temporal).

    `mcp-agent` is designed for building *applications*, not just for interactive exploration.

* **Upsonic**: Upsonic provides a higher-level abstraction, leveraging the capabilities of `mcp-agent` to offer features focused on reliability, such as evaluator-optimizer patterns, and robust error handling.  If you're building a production-grade agent that requires high levels of reliability, Upsonic builds upon `mcp-agent`.

**Analogy:**

*   `mcp-cli` is like a multimeter – a versatile tool for testing electrical circuits, but not something you'd use to *build* a complex electronic device.
*   `mcp-agent` is like a set of circuit components (resistors, capacitors, transistors) and a breadboard – you can use them to build sophisticated circuits, but you need to understand the principles of electronics.
*  Upsonic is like a ready made electronics kit that includes `mcp-agent` components and provides high-level abstractions for reliability.

### 1.4. Installation: `pip install mcp-agent`

To install `mcp-agent`, use `pip` (or `uv`, as recommended in the previous parts of the tutorial):

```bash
pip install mcp-agent
# or
uv pip install mcp-agent
```

If you're working with the cloned `mcp-agent` repository, you can install it in editable mode:

```bash
cd mcp-agent
pip install -e .
# or
uv pip install -e .
```

### 1.5. Configuration: `mcp_agent.config.yaml` and `mcp_agent.secrets.yaml`

`mcp-agent` uses two YAML files for configuration:

*   **`mcp_agent.config.yaml`:**  This file contains general settings, including:
    *   **`execution_engine`:**  Which executor to use (`asyncio` or `temporal`).
    *   **`logger`:**  Logging configuration (type, level, path, etc.).
    *   **`mcp`:**  Configuration for MCP servers.  This is similar to the `server_config.json` file used by `mcp-cli`.  You define the servers you want to connect to, including their command, arguments, and environment variables.
        *   **`servers`:** A dictionary of server configurations, keyed by server name.
    * **`openai` / `anthropic`:** (Optional) API keys and default models for OpenAI and Anthropic.
    * **`otel`:** (Optional) OpenTelemetry configuration

    Example `mcp_agent.config.yaml`:

    ```yaml
    execution_engine: asyncio  # Use the default asyncio executor

    logger:
      type: file  # Log to a file
      level: debug  # Set log level to debug
      path: ./mcp-agent.log

    mcp:
      servers:
        search: # using brave
          command: "uvx"
          args: ["mcp-server-search", "--engine", "brave"]
        filesystem:
          command: "npx"
          args: ["-y", "@modelcontextprotocol/server-filesystem", "."]  # Allow access to current directory

    openai:
      default_model: gpt-4o  # Use GPT-4o by default

    ```

*   **`mcp_agent.secrets.yaml`:**  This file is used to store *sensitive* information, like API keys.  This file should *not* be checked into version control. It has the same structure as `mcp_agent.config.yaml`, but is intended for values that should be kept secret.

    Example `mcp_agent.secrets.yaml`:

    ```yaml
    openai:
      api_key: "your_openai_api_key"
    anthropic:
        api_key: "your_anthropic_api_key"
    ```

The values in `mcp_agent.secrets.yaml` will *override* any values with the same key in `mcp_agent.config.yaml`. This allows you to have a default configuration in `mcp_agent.config.yaml` and override specific values (like API keys) in the secrets file.  It is also possible to define these secrets using environment variables.  Environment variables take the highest precedence.

---

This covers the introduction and setup for `mcp-agent`. The next section will show how to build a basic MCP client.



Okay, let's continue with the tutorial, building a basic MCP client with `mcp-agent`.

---

## 2. Building a Basic MCP Client with `mcp-agent`

Now that you have a good understanding of the core concepts of `mcp-agent`, let's build a simple client application that connects to an MCP server and interacts with it. We'll build on the concepts from Part 1 and use the `search-server` we set up there.

### 2.1. Using `mcp_agent.mcp.gen_client` to Connect to Servers

The `mcp_agent.mcp.gen_client` function provides a convenient way to connect to an MCP server. It handles the details of starting the server process (if necessary) and establishing a connection. It's designed to be used with an `async with` statement, ensuring that the connection is properly closed when you're finished.

**Example: Connecting to `search-server` (Brave Engine)**

First, make sure you have the `search-server` set up as described in Part 1, and that you have your `BRAVE_API_KEY` set in your environment (or in a `.env` file).  We'll use the Brave search engine for this example.

Create a new Python file (e.g., `client.py`) and add the following code:

```python
import asyncio
import os

from mcp_agent.app import MCPApp
from mcp_agent.mcp.gen_client import gen_client
from mcp_agent.logging.logger import get_logger

logger = get_logger("client_example")

async def main():
    # Initialize the MCPApp.  This loads the configuration.
    app = MCPApp(name="basic_client")
    async with app.run() as agent_app:
        context = agent_app.context
        # Add the current directory to the server configuration.
        context.config.mcp.servers["search"].args.extend(["--engine", "brave"])


        # Use gen_client to connect to the 'search' server.
        async with gen_client("search", server_registry=context.server_registry) as client:
            # The client is now connected!  We can use it to interact with the server.
            # Get and print a list of the available tools.
            tools = await client.list_tools()
            logger.info("Available tools:", data=tools)

            # Call the 'search' tool.
            result = await client.call_tool(
                "search", {"query": "latest news about AI"}
            )
            logger.info("Search result:", data=result)


if __name__ == "__main__":
    asyncio.run(main())
```

**Explanation:**

1.  **Import necessary modules:** We import `asyncio`, `MCPApp`, `gen_client`, and the logger.
2.  **Initialize `MCPApp`:** We create an instance of `MCPApp`.  The `async with app.run() as agent_app:` block ensures that the application is properly initialized and cleaned up.
3.  **Access Configuration and Set Server Arguments** We can directly access and modify the configuration values that were loaded from `mcp-agent.config.yaml`.  We use the name "search" that we specified in that config file.  Here we add the `--engine brave` argument to the command.
4.  **`gen_client("search", ...)`:** This is the core of the example. It connects to the `search` server, as defined in your `mcp_agent.config.yaml` file. Because we're using `async with`, the connection will be automatically closed when the block exits.  The `server_registry` must be passed to `gen_client`.
5.  **`client.list_tools()`:** We call the `list_tools` method on the client to retrieve a list of available tools.
6.  **`client.call_tool("search", ...)`:** We call the `search` tool with a query.
7.  **Logging:** We use the `logger` to log messages, making debugging easier.

**Running the Example:**

1.  Make sure you have the `search-server` installed and configured (from Part 1).
2.  Ensure you have the necessary environment variables set (e.g., `BRAVE_API_KEY`).
3.  Create an `mcp_agent.config.yaml` file in the same directory as `client.py`. Here's a minimal example:

    ```yaml
    execution_engine: asyncio

    mcp:
      servers:
        search:
          command: "uvx"
          args: ["mcp-server-search"]
    ```
4. **Important** If you want to use the `search-server`, copy the `search-server` directory from Part 1 to the same directory as `client.py` and `mcp_agent.config.yaml`.  This makes it easy to use with the `uvx` command.
5. Run the script: `python client.py`

You should see output similar to this (the exact search results will vary):

```
INFO:     Started server process [pid]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
...
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     client_example:Available tools: ... # Output from list_tools
INFO:     client_example:Search result: ... # Output from call_tool
...
```

### 2.2. Example: Connecting to `perplexity-advanced-mcp`

You can also connect to the `perplexity-advanced-mcp` server using the same `gen_client` approach.  First, ensure that `perplexity-advanced-mcp` is installed, and that *either* `OPENROUTER_API_KEY` *or* `PERPLEXITY_API_KEY` is set.

Modify your `mcp_agent.config.yaml` file:

```yaml
execution_engine: asyncio

mcp:
  servers:
    perplexity:
      command: "uvx"
      args: ["perplexity-advanced-mcp"]
```

Then, modify `client.py` to connect to `perplexity` instead of `search`:

```python
import asyncio

from mcp_agent.app import MCPApp
from mcp_agent.mcp.gen_client import gen_client
from mcp_agent.logging.logger import get_logger

logger = get_logger("client_example")

async def main():
    app = MCPApp(name="basic_client")
    async with app.run() as agent_app:
        context = agent_app.context
        async with gen_client("perplexity", server_registry=context.server_registry) as client:
            tools = await client.list_tools()
            logger.info("Available tools:", data=tools)

            # Call the 'ask_perplexity' tool.
            result = await client.call_tool(
                "ask_perplexity", {"query": "latest news about AI", "query_type": "simple"}
            )
            logger.info("Search result:", data=result)


if __name__ == "__main__":
    asyncio.run(main())
```

You'll see output showing the available tools (which should include `ask_perplexity`), and the result of the search query.

### 2.3. Listing Tools, Calling Tools

As shown in the examples above, you can use `client.list_tools()` to discover the tools available on a server, and `client.call_tool()` to invoke them. `call_tool` takes the tool name and a dictionary of arguments, as defined by the tool's input schema.  The server will return a result, which will be a dictionary.  If the server returns an error, it will be within the `result`, and `mcp-cli` will format it nicely.  `mcp-agent` will raise an exception for errors.

### 2.4. Persistent Connections with `MCPConnectionManager`

The `gen_client` function is excellent for short-lived connections. However, if you're building an agent that needs to make multiple requests to the same server, it's more efficient to establish a *persistent* connection.  `mcp-agent` provides the `MCPConnectionManager` for this purpose.

The `MCPConnectionManager` keeps track of active server connections. It allows you to retrieve an existing connection or start a new one if none exists. It's also designed to be used within an `async with` block, which ensures connections are gracefully closed.

```python
import asyncio

from mcp_agent.app import MCPApp
from mcp_agent.mcp.mcp_connection_manager import MCPConnectionManager
from mcp_agent.mcp.mcp_agent_client_session import MCPAgentClientSession
from mcp_agent.logging.logger import get_logger

logger = get_logger("persistent_client")

async def main():
    app = MCPApp(name="persistent_client")

    async with app.run() as agent_app:
        context = agent_app.context
        # Add the current directory to the server configuration.
        context.config.mcp.servers["search"].args.extend(["--engine", "brave"])

        # Create a connection manager.
        connection_manager = MCPConnectionManager(context.server_registry)

        # Use an async context manager to ensure connections are closed.
        async with connection_manager:
            # Get a connection to the 'search' server.
            # If a connection doesn't exist, it will be created.
            # If a connection already exists, it will be reused.
            # Specify that we want to use the custom MCPAgentClientSession.
            search_client = await connection_manager.get_server(
                server_name="search", client_session_factory=MCPAgentClientSession
            )

            # Use the connection multiple times.
            tools = await search_client.session.list_tools()
            logger.info("Available tools:", data=tools)

            result1 = await search_client.session.call_tool(
                "search", {"query": "population of Tokyo"}
            )
            logger.info("Search result 1:", data=result1)

            result2 = await search_client.session.call_tool(
                "search", {"query": "population of London"}
            )
            logger.info("Search result 2:", data=result2)

            # Get a *second* connection to the *same* server.
            # This will reuse the existing connection.
            search_client2 = await connection_manager.get_server(
                server_name="search", client_session_factory=MCPAgentClientSession
            )
            assert search_client is search_client2  # Should be the same object

        # When the 'async with' block exits, all connections managed by
        # the connection_manager will be automatically closed.

if __name__ == "__main__":
    asyncio.run(main())

```

Key changes and explanations:

*   **`MCPConnectionManager`:** We create an instance of `MCPConnectionManager`, passing in the `server_registry` from the application context.
*   **`async with connection_manager:`:** This ensures that all connections managed by the `connection_manager` are closed when the block exits, even if errors occur.
*   **`connection_manager.get_server(...)`:** This method is used to obtain a connection to a server.  It takes the server name (as defined in `mcp_agent.config.yaml`) as an argument. Importantly we use the MCPAgentClientSession, instead of the default.  This gives us a logger.
*   **Reusing Connections:** The example demonstrates that calling `get_server` multiple times with the same server name will reuse the existing connection.
* **No manual startup**: `get_server` now handles server startup, so we don't need to run `uvx` separately.

### 2.5. Using `mcp_agent.app.MCPApp` in Client Mode

The examples above show how to use `mcp-agent` to create simple clients, and we've touched on the role of `MCPApp`. The next section will show how to use `mcp-agent` to create MCP servers.

---

This section provides a solid foundation for building MCP clients using `mcp-agent`. The next section will cover building servers.



Okay, let's continue building out Part 3, covering how to create MCP *servers* using `mcp-agent`.

---

## 3. Building an MCP Server with `mcp-agent`

`mcp-agent` not only simplifies building clients but also provides tools for creating MCP servers.  This allows you to expose your own functionality to other MCP clients (including agents built with `mcp-agent`).

### 3.1. The `mcp_agent.mcp.mcp_agent_server` Module

The core of an `mcp-agent` server is the `mcp_agent.mcp.mcp_agent_server` module.  It contains a pre-built `Server` class (based on FastMCP) that you can extend to create your own custom server. It takes care of the boilerplate of setting up the server and handling MCP communication.

### 3.2. Defining Tools with `@server.list_tools()` and `@server.call_tool()`

The `mcp_agent_server` module provides two key decorators:

*   **`@server.list_tools()`:**  This decorator is used to define a function that returns a list of tools exposed by your server.  The function should return a list of `mcp.types.Tool` objects.

*   **`@server.call_tool()`:** This decorator is used to define a function that handles tool calls. The function should accept two arguments:
    *   `name` (str):  The name of the tool being called.
    *   `arguments` (dict):  A dictionary of arguments for the tool.

    The function should return a list of `mcp.types.TextContent` objects representing the result of the tool call.  It can also raise a `ValueError` to indicate an error.

### 3.3. Example: Building a Simple "Echo" Server

Let's create a very simple MCP server that exposes a single tool called "echo".  This tool will simply return the input string as its output.

Create a new Python file (e.g., `echo_server.py`):

```python
import asyncio

from mcp.server.fastmcp import FastMCP
import mcp.types as types

# Create the MCP server
server = FastMCP("echo-server")

@server.list_tools()
async def list_tools():
    # Describe the "echo" tool.
    return [
        types.Tool(
            name="echo",
            description="Repeats the input string.",
            inputSchema={
                "type": "object",
                "properties": {"input": {"type": "string"}},
                "required": ["input"],
            },
        )
    ]


@server.call_tool()
async def call_tool(name: str, arguments: dict | None):
    # Handle tool calls.
    if name == "echo":
        if not arguments or "input" not in arguments:
            raise ValueError("Missing 'input' argument for echo tool.")
        input_str = arguments["input"]
        return [types.TextContent(type="text", text=input_str)]
    else:
        raise ValueError(f"Unknown tool: {name}")

async def main():
     # Configure server and run
    server.run()

if __name__ == "__main__":
    asyncio.run(main())
```

**Explanation:**

1.  **Import `FastMCP` and `mcp.types`:** We import the necessary classes for building the server.
2.  **`server = FastMCP("echo-server")`:** We create an instance of the `FastMCP` class. The string "echo-server" is the server's name.  This is how clients will identify it.
3.  **`@server.list_tools()`:** This decorator registers the `list_tools` function.  This function returns a list containing a single `Tool` object, describing the "echo" tool. The `inputSchema` is defined using JSON Schema, which is a standard way of describing the structure of data.
4.  **`@server.call_tool()`:** This decorator registers the `call_tool` function.  This function checks the `name` of the tool being called.  If it's "echo", it retrieves the "input" argument and returns it wrapped in `TextContent`.
5.  **`server.run()`:** This starts the server, making it listen for incoming connections.

### 3.4. Running the Server with `uvx`

You can run this server using the `uvx` command from within the virtual environment:

```bash
uv run echo_server.py
```

This will start the server, and it will be ready to accept connections from MCP clients.

**Testing with `mcp-cli`:**

You can test your new server using `mcp-cli`:

1.  **Start the server:** In one terminal window, run `uv run echo_server.py`.
2.  **Connect with `mcp-cli`:** In a *separate* terminal window, navigate to your `mcp-cli` directory, activate the virtual environment, and run:

    ```bash
    uv run mcp-cli --server echo-server
    ```

    (Make sure the server name `echo-server` matches the name you used when creating the `FastMCP` instance.)
3.  **List tools:** In the `mcp-cli` interactive prompt, type `list-tools` and press Enter.  You should see the "echo" tool listed.
4.  **Call the tool:** Type `call-tool`, then enter `echo` for the tool name, and `{"input": "Hello, world!"}` for the arguments. You should see the server respond with:

    ```
    Calling tool 'echo' with arguments:

    {
      "input": "Hello, world!"
    }

    ### 3.5. Tool Response

    Hello, world!
    ```

This demonstrates the basic structure of an MCP server built with `mcp-agent`. You can extend this example to create servers that expose any functionality you need.  The key is to define the tools using `@server.list_tools()` and implement the tool logic in the function decorated with `@server.call_tool()`.  The `mcp-agent` framework handles the communication and message routing.

---
## 4. Building Agents with `mcp_agent.agents.Agent`

Now, let's move on to the core of building AI agents with `mcp-agent`: the `Agent` class.

### 4.1. The `Agent` Class: Connecting to MCP Servers

The `Agent` class represents an AI agent that can interact with one or more MCP servers.  You define an agent by giving it:

*   A **name:**  A unique identifier for the agent.
*   An **instruction:** A string that describes the agent's purpose and capabilities. This instruction will be used as part of the system prompt for the LLM.
*   A list of **`server_names`:**  The names of the MCP servers that this agent can connect to.
*   Optionally, a **`human_input_callback`:** A function that can be used to acquire input from the user.

When you create an `Agent` instance and use it within an `async with` block, `mcp-agent` automatically handles:

*   Connecting to the specified MCP servers (using `MCPConnectionManager` internally).
*   Retrieving the list of tools exposed by those servers.
*   Making those tools available to an associated LLM.

### 4.2. Agent Instructions and Capabilities

The `instruction` you provide to an `Agent` is crucial.  It acts as the system prompt for the underlying LLM.  A well-written instruction guides the LLM's behavior and helps it effectively use the available tools.  You can think of it as the agent's "personality" and "job description." The instruction can also be a function that takes a dictionary as input and returns a string.

The `server_names` argument determines which MCP servers the agent can access.  For example, if you specify `server_names=["search", "filesystem"]`, the agent will be able to use the tools provided by the `search` server and the `filesystem` server.

### 4.3. Example: A Simple File-Reading Agent (using `filesystem` server)

Let's create a simple agent that can read files from the local filesystem.  We'll use the built-in `@modelcontextprotocol/server-filesystem` server (which you should already have installed if you followed the setup instructions in previous parts). We do not need to start up this server in a separate process, it will be run by the agent.

First, make sure you have an `mcp_agent.config.yaml` file that defines the `filesystem` server.  Here's an example:

```yaml
execution_engine: asyncio

mcp:
  servers:
    filesystem:
      command: "npx"
      args: ["-y", "@modelcontextprotocol/server-filesystem", "."]  # Allow access to the current directory
```

Create a new Python file (e.g., `file_agent.py`):

```python
import asyncio

from mcp_agent.app import MCPApp
from mcp_agent.agents.agent import Agent
from mcp_agent.workflows.llm.augmented_llm_openai import OpenAIAugmentedLLM  # Or use AnthropicAugmentedLLM
from mcp_agent.logging.logger import get_logger

logger = get_logger("file_agent")

async def main():
    app = MCPApp(name="file_reading_agent")

    async with app.run() as agent_app:
        # Create the agent, specifying the 'filesystem' server.
        file_agent = Agent(
            name="file_reader",
            instruction="You are an agent that can read files from the filesystem. "
            "When asked to read a file, use the 'read_file' tool.",
            server_names=["filesystem"],
        )

        # Use 'async with' to manage the agent's connections.
        async with file_agent:
            # Attach an LLM to the agent.
            llm = await file_agent.attach_llm(OpenAIAugmentedLLM)

            # Create a sample file for the agent to read.
            with open("example.txt", "w") as f:
                f.write("This is a sample file.\nIt contains some text.")

            # Ask the agent to read the file.
            result = await llm.generate_str("Read the contents of example.txt")
            logger.info("Agent response:", data=result)

            # Clean up sample file.
            import os
            os.remove("example.txt")


if __name__ == "__main__":
    asyncio.run(main())

```

**Explanation:**

1.  **Create an `Agent` instance:** We create an `Agent` named "file\_reader" and give it an instruction that explains its role.  We specify `server_names=["filesystem"]` to allow it to connect to the filesystem server.
2.  **Use `async with file_agent:`:**  This is *essential*.  This block ensures that the agent connects to the necessary servers when the block is entered and disconnects when the block is exited (even if errors occur).
3.  **`attach_llm()`:** We associate an `OpenAIAugmentedLLM` instance with the agent.  This gives the agent the ability to use an OpenAI model and the tools provided by the connected servers.  You could also use `AnthropicAugmentedLLM`.
4.  **`llm.generate_str()`:** We ask the agent to read the file.  The agent (powered by the LLM and the `filesystem` server) will figure out how to do this. It will use the available `read_file` tool exposed by the server.
5. A sample file is created and removed in the example, to ensure there is a file to read.

Run the `file_agent.py` script. You should see output indicating that the agent connected to the filesystem server, called the `read_file` tool, and returned the file's content.

### 4.4. The `decorator_app.py` and the `@agent` decorator.

The `decorator_app.py` file provides a simplified way to define and run agents using decorators. This approach can be useful when you have relatively simple agents that don't require complex setup or custom workflows. The `mcp-agent` package includes this, and the example `examples/decorator/main.py` shows how to use it.

First, create a `decorator_app.py` file with the following content:

```python
# decorator_app.py
from mcp_agent.core.decorator_app import MCPAgentDecorator

# Create the application
app = MCPAgentDecorator("root-test")

# Define the agent using the @app.agent decorator
@app.agent(
    name="basic_agent",
    instruction="A simple agent that helps with basic tasks.",
    servers=["sqlite"],  # Corrected server name
)
async def main():
    # Use the app's context manager
    async with app.run() as agent:
        result = await agent.send("basic_agent", "what's the next  number?")
        print("\n\n\n" + result)

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```
Then create a simple `mcp_agent.config.yaml` file:
```
execution_engine: asyncio
logger:
  type: console
  level: error

mcp:
  servers:
    sqlite:
      command: "uvx"
      args: ["mcp-server-sqlite", "--db-path", "test.db"]

openai:
  # Secrets (API keys, etc.) are stored in an mcp_agent.secrets.yaml file
  default_model: o3-mini
  reasoning_effort: low
```

**Explanation:**

*   **`MCPAgentDecorator`:** This class provides a simplified interface for creating and managing agents.  You initialize it with the application name.
*   **`@app.agent(...)`:** This decorator is used to define an agent.  You provide the agent's name, instruction, and a list of server names. The decorated function is treated as a main method.
* **`async with app.run() as agent`:** This runs the application with the agent.

Run `decorator_app.py`. It will connect to the sqlite server (which exposes no tools), use the openai LLM, and print the LLM's response to the console.  The `@agent` decorator takes care of setting up the `Agent` instance and connecting it to the specified servers. This example is not fully complete because it relies on the sqlite server providing some capability, but shows how to define and run a basic agent with the decorator.

### 4.5. Attaching an LLM with `attach_llm`

The `attach_llm` method is a crucial part of the `Agent` class.  It's how you associate a specific LLM implementation with your agent.  This method takes a *factory* as an argument, which is a class that produces `AugmentedLLM` instances.

#### 4.5.1. Using `OpenAIAugmentedLLM`

To use an OpenAI model, you'll use the `OpenAIAugmentedLLM` class:

```python
from mcp_agent.workflows.llm.augmented_llm_openai import OpenAIAugmentedLLM

# inside your agent setup ...

llm = await file_agent.attach_llm(OpenAIAugmentedLLM)
```

By default, `OpenAIAugmentedLLM` will use the `default_model` specified in your `mcp_agent.config.yaml` file (under the `openai` section). You can override this by passing a `model` argument to the `RequestParams`:

```python
result = await llm.generate_str("Read the contents of example.txt", request_params=RequestParams(model="gpt-3.5-turbo"))
```

#### 4.5.2. Using `AnthropicAugmentedLLM`

Similarly, to use an Anthropic Claude model, you'd use `AnthropicAugmentedLLM`:

```python
from mcp_agent.workflows.llm.augmented_llm_anthropic import AnthropicAugmentedLLM

# inside your agent setup ...

llm = await file_agent.attach_llm(AnthropicAugmentedLLM)
```

Again, the default model will be taken from your configuration, but you can override it using `RequestParams`.

#### 4.5.3. Custom LLM Integrations

The `AugmentedLLM` class is abstract.  You can create your own implementations to support other LLM providers.  You would need to:

1.  Subclass `AugmentedLLM`.
2.  Implement the `generate`, `generate_str`, and `generate_structured` methods.
3.  Handle the conversion between your LLM's message format and the MCP message format (using the `type_converter` in the `AugmentedLLM` class).

This allows you to extend `mcp-agent` to work with any LLM that has a suitable API.

---

This completes the section on building agents with the `Agent` class. The next section will move on to the advanced workflow patterns.


Okay, let's continue with Part 3, diving into the advanced workflow patterns provided by `mcp-agent`.

---

## 5. Advanced Agent Patterns with `mcp-agent.workflows`

`mcp-agent` provides a set of pre-built workflow patterns that you can use to build sophisticated agents. These patterns are designed to be *composable*, meaning you can combine them to create even more complex behaviors.  The key to this composability is that all workflow patterns inherit from the `AugmentedLLM` class.  This means that a workflow (like `ParallelLLM`) can be used *anywhere* an `AugmentedLLM` is expected, including as part of another workflow.

### 5.1. The `AugmentedLLM` Base Class

Before we dive into specific workflow patterns, it's important to reiterate what the `AugmentedLLM` class provides.  It's the foundation for all LLM-powered components in `mcp-agent`.  `AugmentedLLM` is *abstract*; you won't use it directly.  Instead, you'll use concrete implementations like `OpenAIAugmentedLLM` or `AnthropicAugmentedLLM`, or the workflow classes that inherit from it.

`AugmentedLLM` provides these key methods:

*   **`generate(message, request_params)`:**  The core method for interacting with the LLM.  It takes a message (or a list of messages) and optional `RequestParams` (to control things like the model, temperature, etc.).  It returns a list of LLM *responses*. The exact type of `message` and the response depends on the specific LLM implementation (e.g., for OpenAI, messages are `ChatCompletionMessageParam` and responses are `ChatCompletionMessage`).
*   **`generate_str(message, request_params)`:**  A convenience method that calls `generate` and returns the LLM's response as a single string.
*   **`generate_structured(message, response_model, request_params)`:**  Uses a library like `instructor` to generate a response and parse it into a Pydantic model (structured output).
*   **`call_tool(request, tool_call_id)`**: Handles calling tools on connected MCP servers.  The `request` is a `CallToolRequest` object, and `tool_call_id` is an optional identifier (used by some LLMs, like OpenAI, to track tool calls).
*   **`history`:** An instance of a `Memory` class (by default, `SimpleMemory`). This stores the conversation history with the LLM.
*   **`select_model(request_params)`:** Selects the appropriate LLM model based on the configured `ModelPreferences` and any overrides in `request_params`.

Now, let's look at some specific workflow patterns.

### 5.2. Parallel Execution with `ParallelLLM`

The `ParallelLLM` workflow allows you to fan out a task to multiple agents (or LLMs) and then combine their results.  This is useful for:

*   **Speed:**  Running multiple agents concurrently can significantly reduce the overall processing time.
*   **Robustness:**  Getting responses from multiple agents can improve the reliability of the result (e.g., by averaging or voting).
*   **Diverse Perspectives:**  Different agents might have different strengths or knowledge bases, allowing you to get a more comprehensive response.

**Key Components:**

*   **`fan_out_agents`:** A list of `Agent` or `AugmentedLLM` instances that will process the input in parallel.
*   **`fan_in_agent`:** An `Agent` or `AugmentedLLM` instance that will combine the results from the `fan_out_agents`.
*   **`llm_factory`:**  A callable that takes an `Agent` and returns an `AugmentedLLM` instance. This is used to attach an LLM to the agents if they are not already `AugmentedLLM` instances.

**Example:**

```python
import asyncio

from mcp_agent.app import MCPApp
from mcp_agent.agents.agent import Agent
from mcp_agent.workflows.llm.augmented_llm_openai import OpenAIAugmentedLLM
from mcp_agent.workflows.parallel.parallel_llm import ParallelLLM

async def main():
    app = MCPApp(name="parallel_example")

    async with app.run() as agent_app:
        # Create three agents with different instructions.
        agent1 = Agent(name="agent1", instruction="Summarize the input text.")
        agent2 = Agent(
            name="agent2", instruction="Extract key entities from the input text."
        )
        agent3 = Agent(
            name="agent3", instruction="Translate the input text to French."
        )

        # Create a fan-in agent to combine the results.
        aggregator = Agent(
            name="aggregator",
            instruction="Combine the summaries, entities, and translation into a single report.",
        )

        # Create the ParallelLLM workflow.
        parallel_llm = ParallelLLM(
            fan_out_agents=[agent1, agent2, agent3],
            fan_in_agent=aggregator,
            llm_factory=OpenAIAugmentedLLM,  # Use OpenAI models
        )

        # Run the workflow.
        input_text = "The quick brown fox jumps over the lazy dog."
        result = await parallel_llm.generate_str(input_text)

        print(result)

if __name__ == "__main__":
    asyncio.run(main())
```

In this example:

1.  We create three "fan-out" agents, each with a different task (summarization, entity extraction, translation).
2.  We create a "fan-in" agent (`aggregator`) to combine the results.
3.  We create a `ParallelLLM` instance, providing the fan-out and fan-in agents.
4.  We call `parallel_llm.generate_str()` with the input text.

`ParallelLLM` will:

1.  Attach an `OpenAIAugmentedLLM` instance to each of the fan-out agents.
2.  Call `generate_str` on each of the fan-out LLMs *concurrently*.
3.  Pass the results of the fan-out agents to the fan-in agent's `generate_str` method.
4.  Return the final result from the fan-in agent.

### 5.3. Routing with `LLMRouter` (OpenAI and Anthropic versions)

The `LLMRouter` workflow allows you to *route* an input request to the most appropriate agent, server, or function based on the content of the request.  This is useful when you have multiple specialized agents or tools, and you want the LLM to decide which one is best suited to handle a particular request.

`mcp-agent` provides two implementations:

*   **`LLMRouter`:**  A base class that uses an LLM to make routing decisions.  You need to provide an `AugmentedLLM` instance to this class.
* **`OpenAILLMRouter`**: Pre-configured with an `OpenAIAugmentedLLM`.
* **`AnthropicLLMRouter`**: Pre-configured with an `AnthropicAugmentedLLM`.

**Key Components:**

* **`llm`:** An `AugmentedLLM` instance used to make the routing decisions.  The router provides a system prompt to this LLM, describing the available options (servers, agents, functions).
* **`server_names`:** (Optional) Names of available servers.
*    **`agents`:** (Optional) A list of available `Agent` instances.
*   **`functions`:** (Optional) A list of Python functions that can be called.
*   **`routing_instruction`:** (Optional) A custom instruction for the LLM to use when making routing decisions. If not provided, a default instruction is used.

**Example:**

```python
import asyncio

from mcp_agent.app import MCPApp
from mcp_agent.agents.agent import Agent
from mcp_agent.workflows.llm.augmented_llm_openai import OpenAIAugmentedLLM
from mcp_agent.workflows.router.router_llm import LLMRouter

async def my_function(input_str: str):
  return f"Function called with input: {input_str}"

async def main():
    app = MCPApp(name="router_example")
    async with app.run() as agent_app:
        context = agent_app.context
        context.config.mcp.servers["search"].args.extend(["--engine", "brave"])

        # Create some agents.
        search_agent = Agent(
            name="search_agent",
            instruction="I can perform web searches.",
            server_names=["search"],
        )
        file_agent = Agent(
            name="file_agent",
            instruction="I can read files.",
            server_names=["filesystem"],
        )

        # Create an LLMRouter.
        router = LLMRouter(
            llm=OpenAIAugmentedLLM(),  # Use an OpenAI LLM for routing
            agents=[search_agent, file_agent],
            functions=[my_function],
        )

        # Route a request.
        request = "What's the population of Tokyo?"
        results = await router.route(request, top_k=1) # route returns a list
        # Inspect the results
        print("Routing results:", results)

        # Assuming we route to the search agent
        chosen_result = results[0]
        if chosen_result and isinstance(chosen_result.result, Agent):
          chosen_agent = chosen_result.result
          async with chosen_agent:
                llm = await chosen_agent.attach_llm(OpenAIAugmentedLLM)
                result_text = await llm.generate_str(request)
                print(result_text)
        elif chosen_result and callable(chosen_result.result):
              result_text = chosen_result.result(request)
              print(result_text)



if __name__ == "__main__":
    asyncio.run(main())

```

In this example:

1.  We create two agents: `search_agent` and `file_agent`.
2.  We create a function: `my_function`.
3.  We create an `LLMRouter`, providing the agents and the function.  We use `OpenAIAugmentedLLM` as the LLM for routing.
4.  We call `router.route()` with a request.  The LLM will analyze the request and decide which agent or function is most appropriate.  The `route` method returns a list of `RouterResult` objects, ordered by confidence.  Each `RouterResult` contains the chosen agent/function/server name (`result`), and optionally a confidence score (`p_score`) and reasoning (`reasoning`).
5. We then use the chosen agent or function.  Note the use of `async with chosen_agent:` to properly manage the agent's connections.

### 5.4. Workflow Orchestration with `Orchestrator`

The `Orchestrator` workflow is designed for more complex tasks that require breaking down a problem into multiple steps, assigning those steps to different agents, and then combining the results. It's more sophisticated than the `ParallelLLM` or `LLMRouter` workflows.

**Key Features:**

*   **Dynamic Plan Generation:** The `Orchestrator` can either generate a complete plan upfront ("full" planning) or generate the next step based on the results of previous steps ("iterative" planning).
*   **Task Dependencies:** The plan can specify dependencies between tasks, ensuring that tasks are executed in the correct order.
*   **Parallel Execution:** Independent tasks within a step are executed in parallel.
*   **Agent Assignment:** Each task is assigned to a specific agent.

**Key Components:**

*   **`llm_factory`:** A callable that takes an `Agent` and returns an `AugmentedLLM` instance (similar to `ParallelLLM`).
*   **`planner`:** (Optional) An `AugmentedLLM` instance used to generate the plan. If not provided, a default planner is created.
*   **`available_agents`:** A list of `Agent` instances that can be used to execute tasks.
*   **`plan_type`:**  Either `"full"` or `"iterative"`.

**Example:**

```python
import asyncio

from mcp_agent.app import MCPApp
from mcp_agent.agents.agent import Agent
from mcp_agent.workflows.llm.augmented_llm_openai import OpenAIAugmentedLLM
from mcp_agent.workflows.orchestrator.orchestrator import Orchestrator

async def main():
    app = MCPApp(name="orchestrator_example")

    async with app.run() as agent_app:
        context = agent_app.context
        context.config.mcp.servers["search"].args.extend(["--engine", "brave"])

        # Create some agents.
        search_agent = Agent(
            name="search_agent",
            instruction="I can perform web searches.",
            server_names=["search"],
        )
        summarizer_agent = Agent(
            name="summarizer_agent",
            instruction="I can summarize text.",
        )
        translator_agent = Agent(
            name="translator_agent",
            instruction="I can translate text to different languages.",
        )

        # Create an Orchestrator.
        orchestrator = Orchestrator(
            llm_factory=OpenAIAugmentedLLM,
            available_agents=[search_agent, summarizer_agent, translator_agent],
            plan_type="iterative",  # Use iterative planning
        )

        # Define the task.
        task = "Find the latest news about AI, summarize it, and translate the summary to French."

        # Run the workflow.
        result = await orchestrator.generate_str(task)
        print(result)

if __name__ == "__main__":
    asyncio.run(main())
```

In this example:

1.  We create three agents: `search_agent`, `summarizer_agent`, and `translator_agent`.
2.  We create an `Orchestrator`, providing the list of available agents and specifying `plan_type="iterative"`.
3.  We call `orchestrator.generate_str()` with the task description.

The `Orchestrator` will:

1.  Use its `planner` LLM to generate the next step (or the full plan, if `plan_type="full"`).  The plan will consist of steps, and each step will contain tasks assigned to specific agents.
2.  Execute the tasks in the current step in parallel (using the `Executor`).
3.  If `plan_type="iterative"`, use the results of the current step to generate the *next* step.
4.  Continue until the plan is complete (or the maximum number of iterations is reached).
5.  Combine intermediate results and return the final result.

### 5.5. Defining Tasks and Dependencies

The `Orchestrator` uses Pydantic models to represent the plan, steps, and tasks:

*   **`Task`:** Represents a single unit of work.  It has a `description`. `AgentTask` is used in the plan, and adds the `agent` attribute to specify the agent name.
*   **`Step`:** Represents a group of tasks that can be executed in parallel.  It has a `description` and a list of `tasks`.
*   **`Plan`:** Represents the overall plan.  It has a list of `steps` and a flag `is_complete` to indicate whether the plan is finished.
*   **`NextStep`:** Used in iterative planning; represents the next step to be executed.

The plan is generated by the `planner` LLM, which is typically an `AugmentedLLM` instance. The prompt given to the planner can be customized, but `mcp-agent` provides default prompts (`FULL_PLAN_PROMPT_TEMPLATE`, `ITERATIVE_PLAN_PROMPT_TEMPLATE`).

### 5.6. Dynamic Plan Generation

The "iterative" planning mode is particularly powerful because it allows the agent to adapt to new information and change its plan as it goes.  For example, if a search query returns unexpected results, the agent can adjust its subsequent steps accordingly.

### 5.7. Evaluator-Optimizer Pattern with `EvaluatorOptimizerLLM`

The `EvaluatorOptimizerLLM` implements a pattern where one LLM (the "optimizer") generates a response, and another LLM (the "evaluator") critiques it.  This process repeats until the response meets a certain quality threshold. This is based on Anthropic's "Building Effective Agents" guide.

**Key Components:**

*   **`optimizer`:** An `Agent` or `AugmentedLLM` that generates the initial response.
*   **`evaluator`:** An `Agent` or `AugmentedLLM` (or simply a string defining the agent's task) that evaluates the response and provides feedback.
*   **`min_rating`:** The minimum acceptable quality rating (e.g., `QualityRating.GOOD`).
*   **`max_refinements`:** The maximum number of refinement iterations.
*  **`llm_factory`:** A factory function that returns an AugmentedLLM, as above.

```python
import asyncio
from mcp_agent.app import MCPApp
from mcp_agent.agents.agent import Agent
from mcp_agent.workflows.llm.augmented_llm_openai import OpenAIAugmentedLLM
from mcp_agent.workflows.evaluator_optimizer.evaluator_optimizer import (
    EvaluatorOptimizerLLM,
    QualityRating
)


async def main():

    app = MCPApp(name="evaluator_optimizer")

    async with app.run():
        optimizer = Agent(name="writer", instruction="Write a job cover letter")
        evaluator = Agent(name="editor", instruction="You are a professional editor. Review and rate the quality of the given text, with specific feedback")
        
        llm = EvaluatorOptimizerLLM(
            optimizer=optimizer,
            evaluator=evaluator,
            llm_factory=OpenAIAugmentedLLM,
            min_rating=QualityRating.GOOD,
            max_refinements=3,
        )

        
        result = await llm.generate_str("Write a job cover letter for an AI framework developer role at LastMile AI.")
        print(result)

if __name__ == "__main__":
    asyncio.run(main())
```

The `EvaluatorOptimizerLLM` will handle the iterative process of generation, evaluation, and refinement.  It uses the `EvaluatorResult` Pydantic model to represent the evaluator's feedback.

### 5.8. The Swarm Pattern with `SwarmAgent` and `AnthropicSwarm`/`OpenAISwarm`

The "Swarm" pattern is a more advanced multi-agent pattern where agents can dynamically spawn and interact with other agents to solve problems. `mcp-agent` provides a `SwarmAgent` class and implementations for both OpenAI and Anthropic.
It's based on [OpenAI's Swarm project](https://github.com/openai/swarm).

*   **`SwarmAgent`:** A specialized `Agent` subclass that can interact with other agents in a swarm.  It has a list of `functions` that it can call, and these functions can return either:
    *   A string (a regular tool result).
    *   An `Agent` (to transfer control to another agent).
    *   An `AgentFunctionResult` (to provide a result and optionally transfer control).
    *   A dictionary (treated as a string result)
* **`AnthropicSwarm` / `OpenAISwarm`:** Concrete implementations of a "swarm" that use a specific LLM provider.

**Example (Conceptual - from `examples/workflow_swarm/main.py`):**

```python
# Simplified and conceptual example

triage_agent = SwarmAgent(
    name="Triage Agent",
    instruction="Figure out what kind of customer request this is.",
    functions=[transfer_to_flight_modification, transfer_to_lost_baggage],
)

flight_modification_agent = SwarmAgent(...)
lost_baggage_agent = SwarmAgent(...)


# Create the swarm (using Anthropic in this example)
swarm = AnthropicSwarm(agent=triage_agent)

# Start the process
result = await swarm.generate_str("My bag was not delivered!")

```

In this example, the `triage_agent` might call the `transfer_to_lost_baggage` function, which would *return* the `lost_baggage_agent`. The `Swarm` would then automatically switch control to the `lost_baggage_agent`. This dynamic agent creation and interaction is the core of the Swarm pattern.

---

This covers the major workflow patterns provided by `mcp-agent`. The key takeaway is that these patterns are composable, allowing you to build complex and powerful agents.  The next sections will cover the executor system and logging.



Okay, let's continue with Part 3, examining `mcp-agent`'s executor system and logging.

---

## 6. `mcp-agent`'s Executor System

The executor system in `mcp-agent` is responsible for running the tasks within your agents and workflows.  It provides a layer of abstraction that allows you to switch between different execution backends (e.g., `asyncio` for local execution, Temporal for distributed execution) without changing your agent code.

### 6.1. The `Executor` Base Class

The `Executor` class is an abstract base class that defines the interface for all executors.  It has these key methods:

*   **`execute(*tasks, **kwargs)`:**  Executes a list of tasks (either coroutines or regular functions) and returns a list of results.  If any task raises an exception, it will be included in the results.
*   **`execute_streaming(*tasks, **kwargs)`:**  Executes a list of tasks and yields results as they become available (an asynchronous iterator).
*   **`map(func, inputs, **kwargs)`:**  Applies a function to a list of inputs, similar to Python's built-in `map` function, but with concurrency.
*   **`signal(signal_name, payload)`:**  Sends a signal.  This is used for inter-agent communication and human-in-the-loop interactions.
*   **`wait_for_signal(signal_name, ...)`:** Pauses execution until a specific signal is received (or a timeout occurs).
*   **`execution_context`**: Provides a way to execute setup/teardown tasks.

### 6.2. `AsyncioExecutor`: The Default Asynchronous Executor

The `AsyncioExecutor` is the default executor used by `mcp-agent`.  It uses Python's built-in `asyncio` library to run tasks concurrently within a single process.

*   **Concurrency:**  It uses `asyncio.create_task` to run tasks concurrently.
*   **Error Handling:**  Exceptions raised by tasks are caught and returned as part of the results.
*   **Rate Limiting:** (Indirectly) Some external services (like LLM providers) may have rate limits.  You'll typically handle this within your `AugmentedLLM` implementation (e.g., using exponential backoff).
* **Concurrency Limiting:** You can limit the number of concurrently executing tasks with the `max_concurrent_activities` setting in the `mcp-agent.config.yaml`

### 6.3. `TemporalExecutor`: Distributed Execution with Temporal (Optional)

The `TemporalExecutor` is a more advanced executor that uses the [Temporal](https://temporal.io/) workflow engine for distributed and reliable execution.  Temporal provides:

*   **Durability:**  Workflow state is persisted, so even if your worker process crashes, the workflow can resume from where it left off.
*   **Scalability:**  You can distribute your workflows across multiple worker processes, even on different machines.
*   **Fault Tolerance:**  Temporal automatically retries failed tasks and handles various failure scenarios.
* **Long-running Workflows:** Temporal is suitable for orchestrating agents and workflows that may require human input and run for a very long time.

To use the `TemporalExecutor`, you'll need to:

1.  **Install the `temporalio` package:** `pip install mcp-agent[temporal]` (or `uv pip install mcp-agent[temporal]`).
2.  **Set up a Temporal server:**  You can run a local Temporal server using Docker (see the Temporal documentation for instructions).
3.  **Configure `mcp-agent`:**  In your `mcp_agent.config.yaml` file, set `execution_engine: temporal` and provide the necessary Temporal connection details:

    ```yaml
    execution_engine: temporal

    temporal:
      host: "localhost:7233"  # Temporal server address
      namespace: "default"
      task_queue: "my_task_queue"  # A unique task queue name for your application
    ```

When you use the `TemporalExecutor`, `mcp-agent` will automatically register your agent's methods as Temporal *activities* and *workflows*. This requires using the decorators, as shown in the following section.

### 6.4. Task Registry and Decorators: `@app.workflow`, `@app.workflow_run`, `@app.workflow_task`

When using the `TemporalExecutor`, you need to use specific decorators to define your workflows and activities:

*   **`@app.workflow`:**  This decorator is used to mark a class as a Temporal workflow.  It's equivalent to Temporal's `@workflow.defn` decorator.

*   **`@app.workflow_run`:** This decorator is used to mark the *main* method of your workflow class (typically `run`).  It's equivalent to Temporal's `@workflow.run` decorator.

* **`@app.workflow_task`:**  This decorator is used to mark a method within a workflow class as a Temporal *activity*.  Activities are the individual units of work within a workflow.  This is conceptually similar to calling tools on an MCP server, but within the Temporal framework. It's equivalent to Temporal's `@activity.defn`.

**Example (Conceptual - adapted for Temporal):**

```python
import asyncio
from datetime import timedelta

from mcp_agent.app import MCPApp
from mcp_agent.executor.workflow import Workflow  # Import the Workflow base class

app = MCPApp(name="temporal_example")

@app.workflow # Registers as Temporal Workflow
class FileProcessingWorkflow(Workflow[str]):  # Inherit from Workflow
    @app.workflow_task(schedule_to_close_timeout=timedelta(minutes=5)) # Defines an Activity
    async def read_file_activity(self, filename: str) -> str:
        # In a real implementation, this would use an MCP server to read the file
        print(f"Reading file: {filename}")
        await asyncio.sleep(1)  # Simulate some work
        return f"Contents of {filename}"

    @app.workflow_run # Marks this method as the workflow's entry point
    async def run(self, filename: str) -> str:
        file_contents = await self.read_file_activity(filename)
        return f"Processed: {file_contents}"

async def main():
    async with app.run() as agent_app:
      # Start the Temporal workflow
      workflow = FileProcessingWorkflow(executor=agent_app.executor)
      result = await workflow.execute("example.txt")
      print(result)

if __name__ == "__main__":
    asyncio.run(main())
```

**Key Points:**

*   The `@app.workflow`, `@app.workflow_run`, and `@app.workflow_task` decorators are *essential* when using the `TemporalExecutor`.  They tell `mcp-agent` how to register your code with Temporal.
*   The `schedule_to_close_timeout` argument to `@app.workflow_task` sets a timeout for the activity.
*   The example shows a very basic workflow with a single activity.  Real-world workflows would likely be much more complex, involving multiple activities, signals, and timers.  All these are supported by the Temporal framework.

## 7. Logging and Tracing

### 7.1. `mcp_agent.logging`: Structured Logging

`mcp-agent` uses structured logging (JSON format) to make it easier to analyze and monitor your agents.  Key features:

*   **`mcp_agent.logging.logger.get_logger(namespace)`:**  Use this function to get a logger instance.  Provide a namespace (e.g., the name of your module or agent) to categorize log messages.  This is the *preferred* way to log within your agents.

*   **Event-Based Logging:** The logger emits *events*, which are structured data containing information about what's happening in your agent.  Events have:
    *   `type`:  A broad category (e.g., "info", "warning", "error", "debug").
    *   `name`:  A more specific name for the event (optional).
    *   `namespace`:  Where the event originated (e.g., your agent's name).
    *   `message`:  A human-readable message.
    *   `timestamp`:  When the event occurred.
    *   `data`:  A dictionary of arbitrary key-value pairs providing additional context.

*   **`mcp_agent.logging.events.EventFilter`:** You can filter events based on their type, name, namespace, or severity level.

### 7.2. Customizing Log Levels and Transports

You can configure logging in your `mcp_agent.config.yaml` file:

```yaml
logger:
  type: file  # Log to a file.  Other options: "console", "http", "none"
  level: debug  # Log level (debug, info, warning, error)
  path: ./mcp-agent.log  # Path to the log file (if type is "file")
  batch_size: 100 # accumulate this many events in memory before flushing
  flush_interval: 2 # or flush every 2 seconds, whichever is first
  max_queue_size: 2048 # drop messages if memory queue gets too full

# For http transport, you can also configure:
#  http_endpoint: "https://your-logging-service.com/logs"
#  http_headers:
#    Authorization: "Bearer your_api_token"
#  http_timeout: 5.0
```

*   **`type`:**  Determines where log messages are sent:
    *   `console`:  Prints log messages to the console (stderr).
    *   `file`:  Writes log messages to a file (JSONL format).
    *   `http`:  Sends log messages to an HTTP endpoint.
    *   `none`:  Disables logging.
*   **`level`:** Sets the minimum severity level to log (debug, info, warning, error).
* **`progress_display`:** By default, `mcp-agent` will render a progress bar in the console, based on structured log messages.
* **`batch_size`, `flush_interval`, `max_queue_size`:** Used by the file transport, to prevent overwhelming the system.

### 7.3. OpenTelemetry Integration for Distributed Tracing

`mcp-agent` integrates with [OpenTelemetry](https://opentelemetry.io/) for distributed tracing. This allows you to:

*   Track the flow of requests across multiple services (including MCP servers and your agents).
*   Identify performance bottlenecks.
*   Debug complex interactions.

To enable OpenTelemetry tracing, you need to:

1.  Install the required packages: `pip install opentelemetry-distro opentelemetry-exporter-otlp-proto-http`
2.  Configure OpenTelemetry in your `mcp_agent.config.yaml` file:

    ```yaml
    otel:
      enabled: true
      service_name: "my-mcp-agent"  # A name for your service
      otlp_endpoint: "http://localhost:4318"  # The address of your OpenTelemetry collector
      console_debug: true # Prints spans to the console as well, useful for debugging
      sample_rate: 1.0 # 1.0 means sample everything, 0.0 means sample nothing.
    ```

    *   **`enabled`:**  Set to `true` to enable tracing.
    *   **`service_name`:**  A name for your service.  This will be used to identify your agent in traces.
    *   **`otlp_endpoint`:**  The address of your OpenTelemetry collector.  If you're running a collector locally, this is often `http://localhost:4318`.  If you're using a managed service (like Honeycomb, Lightstep, or Jaeger), you'll need to provide the appropriate endpoint.
    * **`console_debug`:** Prints all telemetry to the console.

With OpenTelemetry enabled, `mcp-agent` will automatically create spans for:

*   Agent initialization.
*   LLM calls (`generate`, `generate_str`, `generate_structured`).
*   Tool calls.
*   Workflow steps.

You can also add your own custom spans using the `@telemetry.traced` decorator:

```python
from mcp_agent.logging.tracing import telemetry

@telemetry.traced("my_custom_operation")
async def my_function():
  # ... your code ...
```

## 8. The MCP Server Registry

### 8.1. 8.1 Managing multiple MCP servers.

The `mcp_agent.mcp_server_registry.ServerRegistry` class is a utility within the `mcp-agent` framework designed to manage multiple MCP server configurations.  It's used internally by `MCPApp` to keep track of the servers defined in your `mcp_agent.config.yaml` file. It's the `ServerRegistry` that `MCPConnectionManager` uses.

### 8.2. 8.2 Using the MCPAggregator (server-of-servers).

The `mcp_agent.mcp.mcp_aggregator.MCPAggregator` class is used to combine multiple MCP servers, presenting them as a single server.  This simplifies interacting with multiple servers from a client or agent perspective. This has been covered in Section 5, under `ParallelLLM`, but is also a general purpose aggregator.

---

## 9. Conclusion and Next Steps

In this part of the tutorial series, you've learned about:

*   The `mcp-agent` framework and its core components (`MCPApp`, `Agent`, `AugmentedLLM`, `Executor`).
*   How to build simple MCP clients using `gen_client` and `MCPConnectionManager`.
*   How to build MCP servers using `mcp_agent.mcp.mcp_agent_server`.
*   Advanced agent workflow patterns (Parallel, Router, Orchestrator, Evaluator-Optimizer, Swarm).
*  The executor system (Asyncio and Temporal).
*   Logging and tracing with OpenTelemetry.

You now have the tools and knowledge to build sophisticated, reliable, and scalable AI agents using the Model Context Protocol. The next natural step would be using `mcp-agent` to create an AI Agent that leverages the Perplexity and OpenRouter servers that were built in the previous parts of the tutorial series, combining the strengths of all the components covered.
```

## File: mcp/mcp2502-p4.md (Size: 75.87 KB)

```

# AI Assistants part 3: Advanced Agents, Reliability and the Model Context Protocol (MCP)

(Focus on `upsponic`)



1. **Introduction to Upsonic**
   *    1.1.  What is Upsonic?  A Production-Focused Agent Framework.
        *   1.1.1. Addressing Real-World Agent Challenges: Reliability, Scalability, and Modularity
        *   1.1.2. Upsonic's Core Philosophy: Task-Centric, Composable, and Production-Ready
   *    1.2.  Core Concepts: Tasks, Agents, and the Reliability Layer
        *   1.2.1. Tasks: Defining Units of Work for AI Agents
        *   1.2.2. Agents: Orchestrating Tasks and Leveraging Tools
        *   1.2.3. The Reliability Layer: Ensuring Robust and Trustworthy AI Outputs
   *    1.3.  Why Upsonic? Addressing Real-World Agent Challenges
        *   1.3.1.  Limitations of Basic LLM Interactions
        *   1.3.2.  The Need for Reliability in Production AI Agents
        *   1.3.3.  Upsonic as a Solution for Robust Agent Development
   *   1.4. Installing Upsonic: `pip install upsonic`
   *   1.5. Setting up API Keys.
        *   1.5.1. OpenAI, Anthropic, and other providers
        *   1.5.2. Environment Variables vs. Configuration Files
   *  1.6 Conclusion of Chapter 1

2.  **Your First Upsonic Agent: Simple Task Execution**
    *   2.1.  Creating a Basic `Task`
        *   2.1.1. Defining Task Descriptions and Objectives
        *   2.1.2. Exploring Task Parameters and Context
    *   2.2.  Defining an `Agent`
        *   2.2.1. Agent Initialization and Configuration
        *   2.2.2. Attaching an LLM to an Agent
    *   2.3.  Using `agent.print_do()` to Execute and Display Results
        *   2.3.1. Understanding the `do()` Method and Task Execution Flow
        *   2.3.2. Displaying Agent Responses with `print_do()`
    *   2.4.  Exploring `ObjectResponse` and other response types.
        *  2.4.1 Understanding the `ObjectResponse`.
            * 2.4.1.1 Defining Structured Output with `ObjectResponse`
            * 2.4.1.2 Accessing Structured Data from Agent Responses
        *   2.4.2  Exploring `StrResponse`, `IntResponse`, `FloatResponse`, `BoolResponse`, and `StrInListResponse`.
            * 2.4.2.1 Using Basic Response Types for Simple Tasks
            * 2.4.2.2 Choosing the Right Response Type for Your Task
        *  2.4.3 Creating a `CustomTaskResponse`.
            * 2.4.3.1  Building Flexible Output Structures with `CustomTaskResponse`
            * 2.4.3.2  Implementing Custom Output Handling with the `output()` Method
    *   2.5.  Example: A "Hello, World!" Agent
        *   2.5.1. Step-by-Step Construction of a Simple Upsonic Agent
        *   2.5.2. Running the "Hello, World!" Agent and Observing the Output
    *   2.6. Conclusion of Chapter 2

3.  **Leveraging MCP Servers with Upsonic**
    *   3.1.  The Power of Tools: Connecting to the MCP Ecosystem
        *   3.1.1. Expanding Agent Capabilities with External Tools
        *   3.1.2. MCP Servers as Tool Providers for Upsonic Agents
    *   3.2.  Specifying Tools in a `Task`: The `tools` Parameter
        *   3.2.1.  Integrating MCP Tools into Upsonic Tasks
        *   3.2.2.  Understanding Tool Parameters and Usage within Tasks
    *   3.3.  Running an MCP Server (using `search-server` as an example).
        *   3.3.1. Launching `search-server`
        *   3.3.2. Configuring `search-server` API Keys
    *   3.4.  Example: A Web Search Agent using `search-server`.
        *  3.4.1 Create and Run the `Task`.
            * 3.4.1.1 Defining a Task that utilizes the `search` tool
            * 3.4.1.2 Connecting the Task to the `search-server`
        *  3.4.2 See the results.
            * 3.4.2.1 Examining the Output of the Web Search Agent
            * 3.4.2.2 Understanding the Structure of Search Results
    *   3.5.  Understanding Tool Registration: Connecting to the `tools_server`.
        * 3.5.1. Installing tools.
            * 3.5.1.1 Using `tools_server` to Manage and Register Tools
            * 3.5.1.2 Installing Pre-built and Custom Tools
        *  3.5.2 uninstalling tools.
            * 3.5.2.1 Removing Tools from `tools_server`
            * 3.5.2.2 Managing Tool Dependencies
        *  3.5.3. Tool naming conventions.
            * 3.5.3.1 Best Practices for Naming and Organizing Tools
            * 3.5.3.2 Ensuring Clarity and Maintainability in Tool Management
    *   3.6. Using the `ComputerUse`, `BrowserUse` and `Search` tools.
        *   3.6.1.  Exploring `ComputerUse` Tools for Local Interactions
        *   3.6.2.  Utilizing `BrowserUse` Tools for Web Automation
        *   3.6.3.  Advanced Web Searching with `Search` Tools
    *  3.7. Conclusion of Chapter 3

4. **Building Reliable Agents: The Reliability Layer**
    * 4.1 The importance of reliability in AI.
        * 4.1.1. The Challenge of Hallucinations and Inaccuracies in LLMs
        * 4.1.2. Reliability as a Key Requirement for Production AI Agents
    * 4.2 Introducing the reliability layer.
        * 4.2.1. Upsonic's Approach to Enhancing Agent Reliability
        * 4.2.2. The Role of Verification and Validation in Reliable AI
    * 4.3 Exploring the reliability options.
        * 4.3.1 `prevent_hallucination`
            * 4.3.1.1 Implementing Hallucination Prevention Techniques
            * 4.3.1.2 Configuring and Activating `prevent_hallucination`
        * 4.3.2  Understanding the `ValidationResult`
            * 4.3.2.1  Structure and Components of `ValidationResult`
            * 4.3.2.2  Interpreting Validation Feedback and Metrics
        * 4.3.3 `url_validation`, `number_validation`, `information_validation` and `code_validation`.
            * 4.3.3.1  Leveraging Pre-built Validation Types for Common Scenarios
            * 4.3.3.2  Customizing Validation Logic for Specific Needs
    * 4.4 Conclusion of Chapter 4

5.  **Advanced Upsonic: Multi-Agent Workflows**
    *   5.1.  The Need for Multi-Agent Systems
        *   5.1.1.  Addressing Complex Tasks with Collaborative AI
        *   5.1.2.  Benefits of Multi-Agent Workflows: Scalability, Robustness, and Specialization
    *   5.2.  Introducing `MultiAgent` in Upsonic.
        *   5.2.1.  The `MultiAgent` Class: Orchestrating Multiple Agents for Complex Tasks
        *   5.2.2.  Defining Agent Teams and Task Distribution Strategies
    *   5.3.  Example: A Content Creation and Review Workflow
        *   5.3.1.  Defining Multiple Agents
            * 5.3.1.1 Creating Specialized Agents for Content Creation and Review
            * 5.3.1.2 Configuring Agent Instructions and Capabilities
        *   5.3.2.  Creating Interdependent Tasks
            * 5.3.2.1 Designing a Task Workflow for Content Creation and Review
            * 5.3.2.2 Defining Task Dependencies and Execution Order
        *   5.3.3.  Executing with `MultiAgent.do()`
            * 5.3.3.1 Running the Multi-Agent Workflow and Observing the Results
            * 5.3.3.2 Analyzing Task Execution Flow in Multi-Agent Systems
    *   5.4.  Understanding Task Dependencies and Execution Order
        *   5.4.1.  Managing Task Flow and Data Dependencies in Multi-Agent Workflows
        *   5.4.2.  Ensuring Coordinated Execution in Complex Agent Systems
    *  5.5. Conclusion of Chapter 5

6. **Direct LLM and Upsonic**
    * 6.1 Direct LLM call.
        * 6.1.1. Making Simple LLM Calls with `Direct.do()`
        * 6.1.2. When to Use Direct LLM Calls vs. Agent-Based Tasks
    * 6.2 Using `Direct.do()` for quick calls.
        * 6.2.1.  Streamlining Simple Tasks with Direct LLM Interactions
        *  6.2.2.  Example: Quick Fact Retrieval with `Direct.do()`
    * 6.3 Conclusion of Chapter 6

7.  **Conclusion and Next Steps**
    *   8.1.  Review of the Tutorial Series.
        *   8.1.1. Recap of Part 1: Foundational MCP Concepts and `mcp-cli`
        *   8.1.2. Recap of Part 2: Perplexity Advanced MCP Server
        *   8.1.3. Recap of Part 3: Advanced Agents with `mcp-agent` and `Upsonic`
    *   8.2.  The Future of AI Agents and MCP
        *   8.2.1.  Evolving Trends in AI Agent Development
        *   8.2.2.  The Growing MCP Ecosystem and its Potential
    *   8.3.  Further Exploration:
        *   8.3.1.  Contributing to the projects
            * 8.3.1.1.  How to Contribute to `mcp-cli`, `mcp-agent`, and `perplexity-advanced-mcp`
            * 8.3.1.2.  Community and Contribution Guidelines
        *   8.3.2.  Building your own MCP servers
            * 8.3.2.1.  Creating Custom MCP Servers for Specialized Tools
            * 8.3.2.2.  Publishing and Sharing Your MCP Servers
        *   8.3.3.  Exploring advanced agent architectures
            * 8.3.3.1.  Research Directions in Agent Architectures
            * 8.3.3.2.  Experimenting with Advanced Upsonic Workflows
        *   8.3.4.  Addressing the challenges of production deployment.
            * 8.3.4.1.  Scalability and Performance Considerations for Production Agents
            * 8.3.4.2.  Security and Monitoring in Production AI Deployments





Okay, I've reviewed the codebases for `mcp-cli`, `mcp-agent`, `search-server`, and `perplexity-advanced-mcp`, along with Parts 1, 2, and 3 of the tutorial series. I also have the Table of Contents for Part 4, which focuses on `upsonic`.  Here's my analysis and the beginning of the text for Part 4:

**Analysis and Approach for Part 4**

*   **Building on Previous Parts:**  Part 1 covered the foundational concepts of MCP and using `mcp-cli`. Part 2 delved into a specific, more advanced MCP server (`perplexity-advanced-mcp`), focusing on practical integration with clients like Claude Desktop and Cursor. Part 3 introduced `mcp-agent`, a framework for creating more complex, production-ready agents, including concepts like workflows, executors, and connecting to multiple servers.  Part 4 needs to build on these by showing how `upsonic` *further* simplifies and strengthens agent development.
*   **Upsonic's Value Proposition:** The TOC makes it clear that Upsonic's key strengths are *reliability* and *production-readiness*.  The tutorial must emphasize how Upsonic makes agents more robust, easier to manage, and less prone to common LLM issues like hallucinations.
*   **`mcp-agent` as a Foundation:**  Part 4 needs to explicitly state that Upsonic *builds upon* `mcp-agent`.  It's not a replacement, but a higher-level abstraction. Readers need to understand the relationship. The introduction should make this very clear.
*   **Task-Centric Approach:** Upsonic's core concept is the `Task`.  This needs to be explained early and clearly, contrasting it with how agents were defined in `mcp-agent`.  The tutorial should show how tasks simplify agent definition.
*   **Reliability Layer:** This is a key differentiator for Upsonic. The tutorial must clearly explain *what* the reliability layer is, *how* it works (at a conceptual level), and *why* it's important.  The different validation options (`prevent_hallucination`, `url_validation`, etc.) need to be explained with examples.
*   **`do()` and `print_do()`:** These are the primary methods for interacting with Upsonic agents.  The tutorial needs to clearly explain their usage.
*   **`ObjectResponse` and Custom Responses:** These are how Upsonic structures output.  The tutorial should explain how to define custom response types and access the structured data.
*   **MCP Server Integration:** Part 4 needs to show how to use MCP servers (like `search-server` from Part 1, and potentially `perplexity-advanced-mcp` from Part 2) *within* an Upsonic agent. This demonstrates the composability of the entire ecosystem.
*   **Multi-Agent Workflows:** Chapter 5 covers `MultiAgent`, a more advanced concept. The tutorial should provide a clear example of how to define multiple agents and interdependent tasks.
*   **`Direct.do()`:** Chapter 6 covers direct LLM calls. The tutorial should explain when and why to use `Direct.do()` versus a full agent.
*   **Hands-on Examples:** Like the previous parts, Part 4 *must* have clear, runnable code examples that demonstrate each concept.  The examples should build in complexity.
*  **Focus**: The code has a lot of options and features. Part 4 should have focused examples, with clear, practical applications.

**Part 4: AI Assistants - Production-Ready Agents with Upsonic and MCP**

## 1. Introduction to Upsonic

### 1.1. What is Upsonic? A Production-Focused Agent Framework

In the previous parts of this tutorial series, you learned about the Model Context Protocol (MCP) and how it enables AI agents to interact with the external world. You explored MCP using `mcp-cli` (Part 1), built a simple search server (`search-server`, Part 1) and interacted with the more powerful `perplexity-advanced-mcp` server (Part 2).  You also learned how to build more complex agents using the `mcp-agent` framework (Part 3), which provides building blocks for creating workflows, connecting to multiple servers, and managing agent lifecycles.

Upsonic builds upon `mcp-agent`, providing a higher-level, *production-focused* framework for creating reliable and scalable AI agents.  While `mcp-agent` gives you the fundamental tools, Upsonic adds a layer of structure and reliability features specifically designed to address the challenges of deploying AI agents in real-world applications.  Think of `mcp-agent` as providing the engine, and Upsonic as providing the chassis, safety features, and controls.

#### 1.1.1. Addressing Real-World Agent Challenges: Reliability, Scalability, and Modularity

Building AI agents that work reliably in production environments presents several challenges:

*   **Hallucinations and Inaccuracies:** LLMs can sometimes generate incorrect or nonsensical information. This is unacceptable in many applications.
*   **Error Handling:**  Network issues, API errors, and unexpected inputs can cause agents to fail.  Robust error handling and retry mechanisms are essential.
*   **Scalability:**  As your agent's usage grows, you need to ensure it can handle the increased load without performance degradation.
*   **Maintainability:**  Complex agents with many interacting components can become difficult to manage and update.  A modular design is crucial for long-term maintainability.
*   **Reproducibility:** In a production context it is often critical to reproduce issues, and have consistent behaviour.

Upsonic addresses these challenges head-on by providing:

*   **A Task-Centric Approach:**  Upsonic agents are built around the concept of *tasks*, which are well-defined units of work. This simplifies agent design and makes it easier to reason about their behavior.
*   **A Built-in Reliability Layer:** Upsonic includes a *reliability layer* that can automatically validate LLM outputs, prevent hallucinations, and retry failed operations.
*   **Composability:**  Upsonic's components are designed to be easily combined and reused, allowing you to build complex agents from simpler building blocks.
*   **Integration with MCP:**  Upsonic seamlessly integrates with MCP servers, allowing your agents to leverage the power of external tools and data.
*  **Support for Tool Calling**: Upsonic can use tools provided by MCP servers, or Python functions directly.

#### 1.1.2. Upsonic's Core Philosophy: Task-Centric, Composable, and Production-Ready

Upsonic's design is guided by these core principles:

*   **Task-Centricity:**  Everything revolves around the concept of a `Task`. A `Task` defines a specific unit of work, including its description, input parameters, and expected output format. This makes it easy to define, test, and reuse individual agent capabilities.  It's a higher-level abstraction than simply calling an LLM with a prompt.
*   **Composability:**  You can build complex agents by combining simpler `Task` objects and leveraging pre-built components. Upsonic provides building blocks for common agent patterns, such as multi-agent workflows.
*   **Production-Readiness:** Upsonic includes features like the reliability layer, structured logging, and support for distributed execution (via `mcp-agent`'s Temporal integration) that are crucial for deploying agents in real-world scenarios.

### 1.2. Core Concepts: Tasks, Agents, and the Reliability Layer

Let's define the key concepts you'll be working with in Upsonic:

#### 1.2.1. Tasks: Defining Units of Work for AI Agents

A `Task` in Upsonic is a self-contained unit of work that an agent can perform.  It's defined by:

*   **`description`:** A natural language description of the task. This is used as the prompt for the LLM.
*   **`context` (Optional):**  A list of context items for the task. This can be used to pass data to the task.
*   **`response_format` (Optional):**  The expected output format of the task. This can be a simple type (like `str` or `int`) or a Pydantic model for structured output.
*   **`tools` (Optional):**  A list of tools (MCP servers, or functions) that the agent can use to complete the task.
*   **`images` (Optional):** A list of image URLs to pass as context to the LLM.

Unlike `mcp-agent`, where you directly interact with an `AugmentedLLM`, in Upsonic, you define a `Task`, and the `Agent` handles the interaction with the LLM. This separation of concerns makes it easier to manage and test your agent's behavior.

#### 1.2.2. Agents: Orchestrating Tasks and Leveraging Tools

An `Agent` in Upsonic is responsible for executing `Task` objects. It connects to the necessary MCP servers (or uses local functions) to provide the tools required by the task.  The `Agent` handles:

*   Connecting to MCP servers (if needed).
*   Selecting the appropriate LLM model (based on configuration).
*   Invoking the LLM with the task description and any context/tools.
*   Handling the LLM's response.
*   Applying the reliability layer (if enabled).
*   Returning the result in the format specified by the `Task`.

You typically initialize an `Agent` with:
*  A `name`
*  A `model`
*  Other, optional, settings like `reliability_layer`

#### 1.2.3. The Reliability Layer: Ensuring Robust and Trustworthy AI Outputs

The reliability layer is a key feature of Upsonic that helps ensure the quality and correctness of your agent's outputs. It provides several mechanisms for validating and improving LLM responses:

*   **`prevent_hallucination`:**  Attempts to prevent the LLM from generating false or misleading information.  It does this by using another LLM to perform several checks.
*   **`url_validation`:**  Checks if URLs mentioned in the LLM's response are valid and accessible.
*   **`number_validation`:**  Verifies the consistency and accuracy of numerical data.
*   **`information_validation`:**  Performs general information validation, checking for factual errors or inconsistencies.
*   **`code_validation`:**  Validates code snippets generated by the LLM.

These validation checks can be configured and combined to meet the specific needs of your application. The reliability layer adds a crucial layer of trust for production deployments.

### 1.3. Why Upsonic? Addressing Real-World Agent Challenges

#### 1.3.1. Limitations of Basic LLM Interactions

Directly interacting with LLMs (like GPT-4 or Claude) using simple prompts can be powerful, but it also has limitations:

*   **No Structure:**  Plain text prompts and responses lack structure, making it difficult to reliably extract information or integrate with other systems.
*   **No Built-in Tools:**  LLMs have limited knowledge and can't directly access external resources or perform actions.
*   **Unpredictability:** LLM responses can be inconsistent, and there's no guarantee of correctness.

#### 1.3.2. The Need for Reliability in Production AI Agents

When deploying AI agents in production, reliability is paramount.  Imagine:

*   A customer service agent that provides incorrect information.
*   A financial analysis agent that makes inaccurate calculations.
*   A coding agent that generates buggy code.

These scenarios can have serious consequences.  Upsonic's reliability layer directly addresses these concerns.

#### 1.3.3. Upsonic as a Solution for Robust Agent Development

Upsonic provides a structured, reliable, and production-ready framework for building AI agents. It combines the flexibility of LLMs with the robustness of traditional software engineering practices. By using Upsonic, you can:

*   **Define clear tasks:**  Use the `Task` class to define the inputs, outputs, and constraints of your agent's actions.
*   **Leverage external tools:** Connect to MCP servers to give your agents access to real-world data and functionality.
*   **Ensure output quality:** Use the reliability layer to validate and improve the accuracy of LLM responses.
*   **Simplify agent development:**  Focus on defining tasks and let Upsonic handle the complexities of LLM interaction and error handling.
*   **Build for production:**  Deploy your agents with confidence, knowing that they have built-in mechanisms for reliability and scalability.

### 1.4. Installing Upsonic: `pip install upsonic`

To install Upsonic, simply use `pip`:

```bash
pip install upsonic
```

Or, preferably, using `uv`:

```bash
uv pip install upsonic
```

### 1.5. Setting up API Keys

#### 1.5.1. OpenAI, Anthropic, and other providers

Upsonic supports multiple LLM providers, including OpenAI and Anthropic.  To use these providers, you'll need to obtain an API key and set it as an environment variable:

*   **OpenAI:**  `OPENAI_API_KEY`
*   **Anthropic:** `ANTHROPIC_API_KEY`

You can also use OpenRouter, Perplexity, or other providers. Refer to previous sections in the tutorial for set-up instructions.

#### 1.5.2. Environment Variables vs. Configuration Files

While it's possible to set API keys within Upsonic's configuration, the recommended approach is to use environment variables. This keeps your sensitive keys separate from your codebase and makes it easier to manage different configurations for different environments (development, testing, production).

### 1.6. Conclusion of Chapter 1

In this introductory chapter, you've learned about the core concepts of Upsonic: `Task`, `Agent`, and the reliability layer.  You understand the problems Upsonic solves and its benefits for building production-ready AI agents. You also have Upsonic installed and are ready to start building.  In the next chapter, we'll create our first Upsonic agent and see these concepts in action.


## 2. Your First Upsonic Agent: Simple Task Execution

In this chapter, we'll build a simple "Hello, World!" agent using Upsonic. This will introduce you to the basic workflow of defining a `Task`, creating an `Agent`, and executing the task using the `agent.do()` and `agent.print_do()` methods. We'll also explore the different `ObjectResponse` types, including how to create custom response formats.

### 2.1. Creating a Basic `Task`

The `Task` class is the foundation of Upsonic agents. It defines a unit of work that the agent will perform. Let's create a simple task that asks a question:

#### 2.1.1. Defining Task Descriptions and Objectives

The `description` attribute of a `Task` is crucial. It's the natural language instruction that will be passed to the LLM.  A clear and concise description is essential for getting the desired results.

```python
from upsonic import Task

task = Task(description="What is the capital of France?")
```

This creates a task with a simple description.

#### 2.1.2. Exploring Task Parameters and Context

While the `description` is the core of the task, `Task` objects can also have other attributes:

*   **`context`:** (Optional) A list of context items for the task. This can be used to provide additional data or information to the agent, and will be covered in more detail later.
*  **`images`:** (Optional) A list of image URLs to pass as context to the LLM.

For this simple example, we don't need any context.

### 2.2. Defining an `Agent`

The `Agent` class represents an AI agent capable of executing tasks. It connects to MCP servers and manages LLM interactions.

#### 2.2.1. Agent Initialization and Configuration

To create an agent, you need to provide a name. You can optionally specify other settings like a different default model and detailed instructions for the agent.

```python
from upsonic import Agent

agent = Agent("MyFirstAgent")
```
#### 2.2.2. Attaching an LLM to an Agent

Upsonic agents automatically choose the best LLM to use, using the Model Selector (introduced in Part 3). By default, it connects to `openai/gpt-4o-mini`, but you can modify the underlying server configuration to use another model. 

### 2.3. Using `agent.print_do()` to Execute and Display Results

The `agent.print_do()` method is the simplest way to execute a task and print the result. It's a shorthand for:

1.  Calling `agent.do(task)` to execute the task.
2.  Printing the result to the console.
3.  Returning the result.

#### 2.3.1. Understanding the `do()` Method and Task Execution Flow

The `do()` method is the core of Upsonic's task execution.  It handles:

1.  **Connecting to MCP servers (if needed):**  Based on the agent's configuration and the task's requirements, Upsonic automatically manages connections to the necessary MCP servers.
2.  **Sending the task to the LLM:**  The task's `description` (and any context) is formatted and sent to the selected LLM.
3.  **Processing the LLM's response:**  The LLM's response is parsed and validated.
4.  **Applying the reliability layer (if enabled):**  If the reliability layer is configured, Upsonic will perform the specified validation checks.
5.  **Returning the result:** The result is returned in the format specified by the `Task`'s `response_format` (or as a plain string if no format is specified).

#### 2.3.2. Displaying Agent Responses with `print_do()`

```python
result = agent.print_do(task)
print(result)  # You can still access the result directly
```

`agent.print_do()` provides formatted, color-coded output to the console, showing the agent's name, model used, result, response format, estimated cost, and execution time.

### 2.4. Exploring `ObjectResponse` and other response types.

Upsonic provides built-in response types for common use cases, and allows you to define custom response formats using Pydantic models.

#### 2.4.1. Understanding the `ObjectResponse`.

`ObjectResponse` is the base class for creating structured output.  It's a Pydantic `BaseModel`, which means you can define fields with specific types and descriptions, just like you would with any Pydantic model. The `ObjectResponse` handles automatically converting the LLM's response to the desired output.

##### Defining Structured Output with `ObjectResponse`

```python
from upsonic import ObjectResponse

class CapitalCity(ObjectResponse):
    city: str
    country: str
```

This defines a response format with two fields: `city` (a string) and `country` (a string).  When you use this as the `response_format` for a `Task`, Upsonic will automatically parse the LLM's response and populate these fields.

##### Accessing Structured Data from Agent Responses

```python
task = Task(
    description="What is the capital of France?",
    response_format=CapitalCity
)
result = agent.do(task)
print(result.city)
print(result.country)
```

You can access the fields of the `ObjectResponse` directly.

#### 2.4.2. Exploring `StrResponse`, `IntResponse`, `FloatResponse`, `BoolResponse`, and `StrInListResponse`.

Upsonic provides several built-in response types for common scenarios:

*  **`StrResponse`:**  Returns a single string.
*   **`IntResponse`:** Returns a single integer.
*   **`FloatResponse`:**  Returns a single floating-point number.
*   **`BoolResponse`:**  Returns a boolean value (`True` or `False`).
*   **`StrInListResponse`:** Returns a list of strings.

These are convenient shortcuts for simple tasks.

##### Using Basic Response Types for Simple Tasks

```python
from upsonic import Task, StrResponse

task = Task(description="What is the capital of France?", response_format=StrResponse)
result = agent.do(task)
print(result) # Prints directly the content
```

##### Choosing the Right Response Type for Your Task

The choice of response type depends on the expected output of your task:

*   Use `ObjectResponse` (or a custom subclass) for structured data with multiple fields.
*   Use the built-in types (`StrResponse`, `IntResponse`, etc.) for simple, single-value outputs.
*   Use `str` as the `response_format` if you just want the raw, unstructured text output from the LLM.

#### 2.4.3.  2.4.3 Creating a `CustomTaskResponse`.

You can also create custom response formats by subclassing `CustomTaskResponse`. This is very similar to `ObjectResponse`, but has the extra `output` method.

#####  Building Flexible Output Structures with `CustomTaskResponse`

```python
from upsonic import CustomTaskResponse

class MyCustomResponse(CustomTaskResponse):
    capital: str
    population: int

    def output(self):
        return f"The capital is {self.capital} and the population is {self.population}."

task = Task(
    description="What is the capital of France and its population?",
    response_format=MyCustomResponse
)
result = agent.do(task)
# result is a MyCustomResponse instance. Use .output() to retrieve the fields.
print(result.output())
```

#####  Implementing Custom Output Handling with the `output()` Method

The key difference between `CustomTaskResponse` and `ObjectResponse` is the `output()` method.  This allows you to perform custom processing or formatting on the data *after* it's been extracted from the LLM's response, but *before* it's returned by the `agent.do()` method.

### 2.5. Example: A "Hello, World!" Agent

Let's put everything together and create a complete, runnable example:

```python
# hello_world.py
from upsonic import Agent, Task

agent = Agent("Greeter")
task = Task(description="Say hello to the world!")

result = agent.print_do(task)
print(result)

```

To run this example:

1.  Make sure you have Upsonic installed (`pip install upsonic`).
2.  Set your `OPENAI_API_KEY` environment variable.
3.  Run the script: `python hello_world.py`

You should see output similar to this (the exact output will vary):

```
... Upsonic - Call Result ...
LLM Model: openai/gpt-4o-mini
Result:     Hello, world!
Response...
Estimated Cost: ~$0.0000
Time Taken: 2.35 seconds
...
```

### 2.6. Conclusion of Chapter 2

In this chapter, you've created your first Upsonic agent and executed a simple task. You've learned about:

*   Creating `Task` objects with descriptions.
*   Creating `Agent` objects.
*   Using `agent.do()` and `agent.print_do()` to execute tasks.
*   Defining structured output with `ObjectResponse` and custom response classes.

In the next chapter, we'll explore how to connect your Upsonic agents to external tools using MCP servers.


## 3. Leveraging MCP Servers with Upsonic

In the previous chapter, you learned how to create a basic Upsonic agent and execute simple tasks.  Now, we'll see how to connect your agents to external tools and data sources using the Model Context Protocol (MCP).  This is where the real power of AI agents comes into play – by allowing them to interact with the world.

### 3.1. The Power of Tools: Connecting to the MCP Ecosystem

As discussed in Part 1, MCP provides a standardized way for AI agents to interact with external systems.  These systems are exposed as *MCP servers*, and they provide *tools* that agents can use.  Examples of tools include:

*   **Web search:** (e.g., `search-server` from Part 1, `perplexity-advanced-mcp` from Part 2)
*   **File system access:** (e.g., `@modelcontextprotocol/server-filesystem`)
*   **Database queries:** (e.g., `mcp-server-sqlite`)
*   **Code execution:** (e.g., code interpreters)
*   **API interactions:** (e.g., sending emails, accessing cloud services)

By connecting to MCP servers, your Upsonic agents can gain capabilities far beyond what's possible with a standalone LLM.

#### 3.1.1. Expanding Agent Capabilities with External Tools

Imagine you want to build an agent that can:

*   Summarize the latest news about a specific topic.
*   Write a report based on data stored in a file.
*   Answer questions about the current weather.
*   Retrieve information from a company's internal database.

Without MCP, you'd have to write custom code for each of these tasks, handling API authentication, data parsing, and error handling. With MCP, you can simply connect your agent to the appropriate server, and the agent can use the exposed tools directly.

#### 3.1.2. MCP Servers as Tool Providers for Upsonic Agents

Upsonic agents are designed to work seamlessly with MCP servers. You don't need to write any low-level MCP communication code.  Instead, you simply:

1.  **Run the MCP server(s):**  You'll typically run the MCP server in a separate process (or container).
2.  **Configure the server in `mcp_agent.config.yaml`:**  This file (used by `mcp-agent`, which Upsonic builds upon) tells Upsonic how to connect to the server.  This is the *same* configuration you saw in Part 3.
3.  **Specify the tools in your `Task`:** You tell Upsonic which tools your agent needs to complete a particular task.

Upsonic handles the rest: connecting to the server, discovering the available tools, formatting the requests, and parsing the responses.

### 3.2. Specifying Tools in a `Task`: The `tools` Parameter

The `tools` parameter of the `Task` class is how you tell Upsonic which tools your agent needs.  You can provide tools in several ways:

* **Using the string name of an MCP server:** If you're using a known MCP server, use the name.
* **Using a Python class:** You can provide a class, and Upsonic will look for available tools within that class.
* **Using a Python instance:** You can provide an instance of a class, and Upsonic will look for available tools within that instance.

#### 3.2.1. Integrating MCP Tools into Upsonic Tasks

```python
from upsonic import Task
from upsonic import Agent

# Example: Using a search server
task = Task(
    description="Find the population of Tokyo",
    tools=["search"],  # Specify the MCP server by name
)

# Example: Using a class

agent = Agent(
    "Researcher",
    tools=[Search],
)
```

#### 3.2.2. Understanding Tool Parameters and Usage within Tasks

When you specify a tool, Upsonic automatically makes it available to the LLM. The LLM can then choose to call the tool as part of its reasoning process. The LLM will provide the input arguments, and receive the output results to use for further processing,

### 3.3. Running an MCP Server (using `search-server` as an example)

Let's revisit the `search-server` we built in Part 1.  To use it with Upsonic, you need to:

1.  **Have `search-server` installed:**  Make sure you've followed the instructions in Part 1 to set up the `search-server`.
2.  **Configure your API keys:**  Set the appropriate environment variable for the search engine you're using (e.g., `BRAVE_API_KEY`, `METASO_UID`, `BOCHA_API_KEY`).
3.  **Run the server:**  Start the server using the `uvx` command:

#### 3.3.1. Launching `search-server`

```bash
uvx mcp-server-search --engine brave  # Or metaso, or bocha
```

This will start the server, listening on port 8000 (by default).

#### 3.3.2. Configuring `search-server` API Keys

See the instructions in Part 1, Chapter 8, Section 8.4 for details on configuring API keys.  The recommended method is to use a `.env` file or set environment variables directly.

### 3.4. Example: A Web Search Agent using `search-server`

Now, let's create a simple Upsonic agent that uses the `search-server` to perform web searches.

Create a new Python file (e.g., `search_agent.py`):

```python
# search_agent.py
import os
from upsonic import Agent, Task, StrResponse

# Ensure your BRAVE_API_KEY (or other search engine key) is set
os.environ["BRAVE_API_KEY"] = "your_brave_api_key"


agent = Agent("SearchAgent")

task = Task(
    description="What is the population of Tokyo?",
    tools=["search"], # This references the 'search' server we set up.
    response_format=StrResponse
)

result = agent.print_do(task)
print(result)

```

**Explanation:**

1.  **Import necessary classes:**  We import `Agent`, `Task`, and `StrResponse` from Upsonic.
2.  **Set API Key:**  Replace `"your_brave_api_key"` with your actual Brave Search API key (or use Metaso/Bocha as appropriate).
3.  **Create an `Agent`:**  We create an agent named "SearchAgent".
4.  **Create a `Task`:**
    *   `description`:  The question we want the agent to answer.
    *   `tools`:  We specify `"search"`, which corresponds to the name we gave our `search-server` in the `mcp_agent.config.yaml` file.
    *  `response_format`: Use the `StrResponse` for convenience.
5.  **Execute the task:**  We use `agent.print_do()` to execute the task and print the formatted output.
6. **Print the raw response**: We print the raw `result`.

**Running the Example:**

1.  Make sure the `search-server` is running in a separate terminal.
2.  Create an `mcp_agent.config.yaml` file in the same directory as `search_agent.py`:

    ```yaml
    # mcp_agent.config.yaml
    mcp:
      servers:
        search:
          command: "uvx"
          args: ["mcp-server-search", "--engine", "brave"] # Use the brave search engine
    ```

    Make sure the `command` and `args` match how you're running the `search-server`. Adjust `"--engine"` if you are using Metaso or Bocha.

3.  Run the script: `python search_agent.py`

You should see output similar to this (the exact results will vary):

```
... Upsonic - Call Result ...
LLM Model: openai/gpt-4o-mini
Result: The population of Tokyo is approximately 37 million people.
Response Format: str
...

37 million
```

#### 3.4.1. Create and Run the `Task`.

The code above shows how to create a `Task` with a `description` and a set of `tools`.

##### Defining a Task that utilizes the `search` tool

The key is the `tools=["search"]` line.  This tells Upsonic that this task requires the `search` MCP server.

##### Connecting the Task to the `search-server`

Upsonic, using the underlying `mcp-agent` framework, automatically handles connecting to the `search-server` based on the `mcp_agent.config.yaml` file.  You *don't* need to write any code to establish the connection.

#### 3.4.2. See the results.

The `agent.print_do()` method executes the task and prints the result in a user-friendly format. The `result` variable will hold the final output.

##### Examining the Output of the Web Search Agent
##### Understanding the Structure of Search Results

The `result` will contain a string with the LLM output.

### 3.5. Understanding Tool Registration: Connecting to the `tools_server`.

#### 3.5.1. Installing tools.

To add tools to your Upsonic project, you can use the `tools_server` to register and manage your custom tools. This allows you to expand the functionality of your agents and perform actions in your application.

##### Using `tools_server` to Manage and Register Tools
The `tools_server` acts as a central point to manage all available tools in your Upsonic project. You can add, update, remove, and list tools using simple commands.

##### Installing Pre-built and Custom Tools
Upsonic allows you to install both pre-built tools provided by the framework and custom tools developed specifically for your needs.

#### 3.5.2. uninstalling tools.
When you no longer need a specific tool, you can easily remove it using `tools_server`.

##### Removing Tools from `tools_server`
To remove a tool, use the `uninstall_library` command:

```python
tool_manager = ToolManager()
tool_manager.uninstall_library("my_tool")
```

##### Managing Tool Dependencies
When removing tools, ensure you handle any dependencies that other parts of your application might have on those tools to prevent errors.

#### 3.5.3. Tool naming conventions.

##### Best Practices for Naming and Organizing Tools
When creating custom tools, follow clear naming conventions to make your tools easy to understand and use. Here are some best practices:

- Use descriptive names that clearly indicate the tool's purpose.
- Follow a consistent naming scheme (e.g., `tool_name` or `ToolName`).
- Group related tools using a common prefix or module.

##### Ensuring Clarity and Maintainability in Tool Management
Proper naming and organization of tools will help maintain the clarity and maintainability of your project.

### 3.6. Using the `ComputerUse`, `BrowserUse` and `Search` tools.

#### 3.6.1.  Exploring `ComputerUse` Tools for Local Interactions

```python
    computer = ComputerUse()

    await computer(action="left_click", coordinate=(30, 50))

```

#### 3.6.2.  Utilizing `BrowserUse` Tools for Web Automation
```python
    browser = BrowserUse()

    await computer(task="Find the google logo on the screen, click on it", expected_output="Clicking on google logo")

```

#### 3.6.3.  Advanced Web Searching with `Search` Tools
```python
    search = Search()

    await search(query="What is the current status of the stock market?")
    # Or
    from .proxy.brave_search import handle_tool_call as brave_handle_tool, get_tool_descriptions as brave_tools
    result = await brave_handle_tool(name="search", arguments={"query": "openai", "count": 3})
    # Or
    from .proxy.metaso_search import handle_tool_call as metaso_handle_tool, get_tool_descriptions as metaso_tools

```python
    result = await metaso_handle_tool(name="search", arguments={"query": "openai", "mode": "concise"})
    # Or
    from .proxy.bocha_search import handle_tool_call as bocha_handle_tool, get_tool_descriptions as bocha_tools
    result = await bocha_handle_tool(name="search", arguments={"query": "openai", "count": 3})
```
The full details on how to use these have been provided in previous chapters.

### 3.7. Conclusion of Chapter 3

In this chapter, you've learned how to connect your Upsonic agents to external tools and data sources using MCP servers.  You saw how to:

*   Run an MCP server (using `search-server` as an example).
*   Configure Upsonic to connect to the server (using `mcp_agent.config.yaml`).
*   Specify the required tools in your `Task` objects.
* Create an agent that utilizes a search server.
* Use tools of type ComputerUse, BrowserUse and Search.
*  Install and uninstall, and also naming conventions for custom tools.

This ability to interact with the external world is what makes AI agents truly powerful.  In the next chapter, we'll explore Upsonic's reliability layer, a crucial component for building production-ready agents.

---

## 4. Building Reliable Agents: The Reliability Layer

### 4.1. The importance of reliability in AI.

Reliability is paramount when deploying AI agents in real-world applications.  Unlike simple demos or experiments, production systems need to be robust, trustworthy, and predictable.  Users need to be able to depend on the agent's output, and errors can have significant consequences.

#### 4.1.1. The Challenge of Hallucinations and Inaccuracies in LLMs

Large Language Models (LLMs), while powerful, are known to sometimes "hallucinate" – that is, generate information that is factually incorrect, nonsensical, or not grounded in the provided context.  This is a major obstacle to deploying LLM-powered agents in scenarios where accuracy and truthfulness are essential.  Even if an LLM gets the answer right 99% of the time, a 1% error rate can be unacceptable in many applications (e.g., financial advice, medical diagnosis, legal analysis).

#### 4.1.2. Reliability as a Key Requirement for Production AI Agents

Beyond hallucinations, production AI agents must be reliable in other ways:

*   **Consistency:**  Given the same input, the agent should ideally produce the same (or very similar) output.  LLMs are inherently stochastic, but we want to minimize unnecessary variations.
*   **Error Handling:**  The agent should gracefully handle errors, such as network issues, API failures, or invalid input.
*   **Security:**  The agent should be secure and protect sensitive data.
*   **Verifiability:**  It should be possible to verify the agent's output, ideally with references or sources.
*   **Explainability:** To the extent possible, the agent's reasoning process should be understandable to humans.

Upsonic's reliability layer directly addresses these concerns, providing mechanisms to improve the accuracy, consistency, and trustworthiness of your agents.

### 4.2. Introducing the reliability layer.

Upsonic's reliability layer is a set of built-in mechanisms that automatically validate and improve the quality of LLM outputs. It's designed to be configurable, so you can choose the specific checks that are most relevant to your application. The reliability layer operates *after* the LLM generates a response, but *before* that response is returned to the user or used in subsequent steps.

#### 4.2.1. Upsonic's Approach to Enhancing Agent Reliability

The core idea is to add a *verification* step after the LLM's initial response. This verification step can involve:

*   **Additional LLM Calls:**  Using a separate LLM (or the same LLM with a different prompt) to check the original response for specific issues.
*   **Rule-Based Checks:**  Applying pre-defined rules to validate the output (e.g., checking for valid URLs, ensuring numerical ranges are within bounds).
*   **External API Calls:**  Consulting external services to verify facts or perform calculations.

If the verification step finds problems, the reliability layer can:

*   **Reject the response:**  Return an error or a default value instead of the potentially incorrect output.
*   **Retry the original request:**  Ask the LLM to generate a new response, possibly with additional instructions or constraints.
*   **Refine the response:** Use a separate LLM (an "editor" or "optimizer") to revise the original response based on the feedback from the verification step. This is similar to the Evaluator-Optimizer pattern from Part 3.

#### 4.2.2. The Role of Verification and Validation in Reliable AI

The reliability layer is built on the principles of *verification* and *validation*:

*   **Verification:**  Checking that the LLM's output meets certain criteria (e.g., is it a valid URL, is it a number within a specific range, does it contain prohibited words?).
*   **Validation:**  Checking that the LLM's output is *correct* or *appropriate* for the given input and context. This is often a more challenging task, as it requires understanding the semantics of the response.

Upsonic's reliability layer provides tools for both verification and validation, allowing you to build agents that are more robust and trustworthy.

### 4.3. Exploring the reliability options.

The reliability layer is configured through the `reliability_layer` parameter when creating an `Agent`. You create a `ReliabilityLayer` object that includes the options for handling `prevent_hallucination` or different validation types:

```python
from upsonic import Agent, ReliabilityLayer

reliability_layer = ReliabilityLayer(prevent_hallucination=10)

agent = Agent("MyAgent", reliability_layer=reliability_layer)
```

#### 4.3.1. `prevent_hallucination`

This is the most powerful and general-purpose part of the reliability layer. It uses a combination of techniques to reduce the likelihood of the LLM generating false or misleading information.  `prevent_hallucination` takes an integer as an argument.  This integer determines the "strength" of the hallucination prevention:

*   **0:**  No hallucination prevention (default).
*   **1-10:** Increasing levels of hallucination prevention.  Higher values use more sophisticated checks and potentially multiple LLM calls, leading to higher latency and cost but improved accuracy.

The `prevent_hallucination` option works by:

1. **Analyzing the Task Context**: It combines the original task description, context provided, and the desired response format to create a comprehensive understanding of the agent's goal.

2. **Validating the LLM Response**: After the LLM generates a response, the reliability layer employs multiple verification checks, focusing on:
    - **URL validation**: Verifying the existence and relevance of URLs mentioned in the response.
    - **Number validation**: Ensuring numerical data is consistent and accurate.
    - **Information validation**: Checking the factual correctness of the information provided.
    - **Code validation**: Analyzing any code snippets for syntax and logical errors.

3. **Identifying Suspicious Content**: If any discrepancies or issues are found, they are flagged as suspicious.

4. **Iterative Refinement (Editor Agent)**: If any suspicious content is detected, an "Editor Agent" is engaged to refine the response. The original response and the validation feedback are used as input.

##### Implementing Hallucination Prevention Techniques

Behind the scenes, `prevent_hallucination` uses an "Evaluator-Optimizer" pattern, similar to what was covered in Part 3.  A separate LLM (the "evaluator") is used to critique the original response, and another LLM (the "optimizer") is used to refine the response based on the feedback.  This process can be repeated multiple times.

##### Configuring and Activating `prevent_hallucination`

```python
from upsonic import Agent, ReliabilityLayer, Task, StrResponse

reliability_layer = ReliabilityLayer(prevent_hallucination=10)

agent = Agent("MyAgent", reliability_layer=reliability_layer)

task = Task(
    description="What is the population of Tokyo?",
    response_format=StrResponse # The agent will return a string output
)

result = agent.print_do(task)
print(result)
```

#### 4.3.2.  Understanding the `ValidationResult`

When `prevent_hallucination` is enabled, the reliability layer performs several validation checks.  The results of these checks are summarized in a `ValidationResult` object.  This object is a Pydantic model with the following structure:

```python
class ValidationPoint(ObjectResponse):
    is_suspicious: bool
    feedback: str
    suspicious_points: list[str] = Field(description = "Suspicious informations raw name")
    source_reliability: SourceReliability = SourceReliability.UNKNOWN
    verification_method: str = ""
    confidence_score: float = 0.0

class ValidationResult(ObjectResponse):
    url_validation: ValidationPoint
    number_validation: ValidationPoint
    information_validation: ValidationPoint
    code_validation: ValidationPoint
    any_suspicion: bool
    suspicious_points: list[str]
    overall_feedback: str
    overall_confidence: float = 0.0
```

*   **`url_validation`:**  Results of URL validation.
*   **`number_validation`:**  Results of number validation.
*   **`information_validation`:**  Results of general information validation.
*   **`code_validation`:**  Results of code validation.
*   **`any_suspicion`:**  A boolean flag indicating whether *any* of the checks found a problem.
*  **`suspicious_points`:** Raw names of suspicious points.
*   **`overall_feedback`:**  A combined text summary of all the feedback.
*   **`overall_confidence`:**  A numerical score (0.0 to 1.0) representing the overall confidence in the response's correctness.

Each individual validation check (e.g., `url_validation`) returns a `ValidationPoint` object:

*  **`is_suspicious`**: True if the check found an error
*  **`feedback`**: Details about the error
* **`suspicious_points`**: Raw names of suspicious points
*   **`source_reliability`:**  An enum indicating the perceived reliability of the source (e.g., `HIGH`, `MEDIUM`, `LOW`, `UNKNOWN`).
*   **`verification_method`:**  A string describing the method used for verification (e.g., "regex_url_detection").
*   **`confidence_score`:**  A numerical score (0.0 to 1.0) representing the confidence in this specific check.

#### 4.3.3. `url_validation`, `number_validation`, `information_validation` and `code_validation`.

These are the specific validation checks that are performed as part of the reliability layer.  They are all enabled when you set `prevent_hallucination` to a value greater than 0.

#####  Leveraging Pre-built Validation Types for Common Scenarios

Upsonic provides these pre-built validation types to cover common scenarios:

*   **URL Validation:**  Checks if URLs mentioned in the LLM's response are valid and accessible.  This helps prevent the agent from providing broken or non-existent links.
*   **Number Validation:**  Checks for inconsistencies or errors in numerical data.  For example, if the LLM claims that "2 + 2 = 5", this check would flag it as suspicious.
*   **Information Validation:**  This is a more general check that uses an LLM to assess the factual accuracy and consistency of the information provided in the response.  It's less precise than the specific URL and number checks, but it can catch a wider range of errors.
*   **Code Validation:**  If the LLM generates code, this check can verify its syntax and, to a limited extent, its logical correctness.

#####  Customizing Validation Logic for Specific Needs

While Upsonic provides these built-in validation types, you can also extend the reliability layer with your own custom validation logic.  This is beyond the scope of this introductory tutorial, but it allows you to tailor the verification process to the specific requirements of your application.

### 4.4. Conclusion of Chapter 4

This chapter introduced the Upsonic reliability layer, a critical component for building production-ready AI agents.  You learned:

*   Why reliability is essential for real-world applications.
*   How Upsonic's reliability layer works.
*   The different validation options available (`prevent_hallucination`, `url_validation`, etc.).
*   How to enable and configure the reliability layer.
*   The structure of the `ValidationResult` object.

By using the reliability layer, you can significantly improve the accuracy and trustworthiness of your Upsonic agents. In the next chapter, we will explore multi-agent workflows.

---

## 5. Advanced Upsonic: Multi-Agent Workflows

In previous chapters, you've learned how to create individual Upsonic agents that can perform specific tasks. However, many real-world problems require the coordination of *multiple* agents, each with different capabilities and expertise.  This chapter introduces Upsonic's `MultiAgent` class, which allows you to build and manage workflows involving multiple interacting agents.

### 5.1. The Need for Multi-Agent Systems

Why would you want to use multiple agents instead of a single, more powerful agent?  There are several advantages:

*   **Specialization:** Different agents can be specialized in different tasks.  For example, one agent might be good at web search, another at data analysis, and another at writing reports.
*   **Modularity:**  Breaking down a complex problem into smaller, agent-specific tasks makes the overall system easier to design, develop, and maintain.
*   **Robustness:**  If one agent fails or produces an unreliable result, other agents can potentially compensate.
*   **Scalability:**  You can potentially distribute the workload across multiple agents, improving performance and handling larger tasks.
*   **Collaboration:**  Agents can work together, sharing information and coordinating their actions to achieve a common goal.

#### 5.1.1. Addressing Complex Tasks with Collaborative AI

Consider a complex task like "research a topic, analyze the data, and write a report." This could be broken down into:

1.  **Research Agent:**  Performs web searches and gathers relevant information.
2.  **Analysis Agent:**  Processes the gathered information, performs calculations, and extracts key insights.
3.  **Writing Agent:**  Takes the analyzed data and generates a well-structured report.

Each of these agents has a specific role and can be optimized for that role. They can also work in parallel (where appropriate) and exchange information to achieve the overall objective.

#### 5.1.2. Benefits of Multi-Agent Workflows: Scalability, Robustness, and Specialization

Multi-agent systems offer several key benefits:

*   **Scalability:** You can potentially scale individual agents independently based on their workload. For example, if the research agent is the bottleneck, you could add more instances of that agent.
*   **Robustness:** If one agent fails, the others can potentially continue working, or the system can try a different approach.
*   **Specialization:** As mentioned earlier, specialization allows you to create agents that are highly optimized for specific tasks, leading to better overall performance.

### 5.2. Introducing `MultiAgent` in Upsonic.

Upsonic provides the `MultiAgent` class to simplify the creation and management of multi-agent workflows.  `MultiAgent` acts as an orchestrator, coordinating the execution of tasks across multiple agents.

#### 5.2.1. The `MultiAgent` Class: Orchestrating Multiple Agents for Complex Tasks

The `MultiAgent` class has a `do` method that takes:

*   **`agents`:**  A list of `AgentConfiguration` objects, defining the agents to be used in the workflow.  Each agent configuration specifies the agent's `job_title` (which is used as its `instruction`), an optional, separate `client` and various other settings that can be passed to the `Agent`.
*   **`tasks`:** A list of `Task` objects, defining the tasks to be performed.
*   **`llm_model` (Optional):** specifies which LLM to use, which overrides the configurations defined in the agents

The `MultiAgent` class determines the execution order based on task dependencies, as explained in 5.4.

#### 5.2.2. Defining Agent Teams and Task Distribution Strategies

With `MultiAgent`, you don't explicitly assign tasks to agents in your code.  Instead:

1.  **Define the Agents:** Create `AgentConfiguration` objects for each agent, specifying their `job_title` (which serves as the agent's instruction) and other relevant settings.
2.  **Define the Tasks:**  Create `Task` objects, specifying the `description`, `context`, `tools`, and `response_format` for each task. Importantly, you **do not** assign agents to tasks at this stage.
3. **Pass the lists of Agents and Tasks:** Pass both lists to the `MultiAgent.do()` method.

`MultiAgent` uses an LLM to dynamically assign tasks. For each task, the `MultiAgent` asks the LLM which of the agents should perform it.

### 5.3. Example: A Content Creation and Review Workflow

Let's build a multi-agent workflow for creating and reviewing content.  We'll have two agents:

*   **`ContentCreator`:**  Responsible for generating the initial content.
*   **`ContentReviewer`:**  Responsible for reviewing and critiquing the content.

We'll define two tasks:

1.  **`create_content_task`:**  Create the initial content.
2.  **`review_content_task`:**  Review the created content. The result from creating the content is set as context for this task.

#### 5.3.1.  Defining Multiple Agents

First, we define our agents using `AgentConfiguration`.  Note that we're setting `sub_task=False` to prevent infinite loops of task decomposition.  We're also turning off the `reliability_layer` for this example to keep it simple.

```python
from upsonic import AgentConfiguration

creator_agent = AgentConfiguration(
    job_title="Content Creator",
    sub_task=False,  # Prevent infinite loops
    reliability_layer=None, # Keep it simple for this example
)

reviewer_agent = AgentConfiguration(
    job_title="Content Reviewer",
    sub_task=False,  # Prevent infinite loops
    reliability_layer=None, # Keep it simple
)

```

##### Creating Specialized Agents for Content Creation and Review

We've created two agent configurations with distinct `job_title` values.  These will be used as the system prompts for the underlying LLMs, guiding their behavior.

##### Configuring Agent Instructions and Capabilities

The `job_title` acts as the agent's instruction. We could provide more detailed instructions here if needed, such as which tools an agent can use.

#### 5.3.2.  Creating Interdependent Tasks

Now, we define our tasks.  The key here is the `context` parameter.

```python
from upsonic import Task, StrResponse

create_content_task = Task(
    description="Create a short poem about the moon.",
    response_format=StrResponse,
    price_id_="create_content" # To keep track of costs.
)


review_content_task = Task(
    description="Review the provided content and provide feedback.",
    response_format=StrResponse,
    context=[create_content_task],  # The output of the first task is context for the second
    price_id_="review_content" # To keep track of costs.
)
```
##### Designing a Task Workflow for Content Creation and Review

We've defined two tasks. The `create_content_task` doesn't depend on anything else.  The `review_content_task`, however, depends on the `create_content_task`.

##### Defining Task Dependencies and Execution Order
By setting `context=[create_content_task]` for the `review_content_task`, we tell Upsonic that the output of `create_content_task` should be provided as context when executing `review_content_task`. `MultiAgent` will automatically ensure that `create_content_task` is executed *before* `review_content_task`.

#### 5.3.3.  Executing with `MultiAgent.do()`

Finally, we create a `MultiAgent` instance and execute the workflow:

```python
from upsonic import MultiAgent

agents = [creator_agent, reviewer_agent]
tasks = [create_content_task, review_content_task]

result = MultiAgent.do(agents, tasks)

print(result)
```

##### Running the Multi-Agent Workflow and Observing the Results

The `MultiAgent.do()` method handles the entire workflow:

1.  It dynamically assigns `create_content_task` to `creator_agent`.
2.  It executes `create_content_task`.
3.  It takes the output of `create_content_task` and sets it as context for `review_content_task`
4.  It dynamically assigns `review_content_task` to `reviewer_agent`.
5.  It executes `review_content_task`.
6. It returns the result, which should be the reviewed content.

##### Analyzing Task Execution Flow in Multi-Agent Systems

You can examine the output to see how the tasks were executed and how the agents interacted. Upsonic's logging (if enabled) will also provide detailed information about the workflow execution.

**Complete Example (`multi_agent_example.py`):**

```python
# multi_agent_example.py
from upsonic import AgentConfiguration, Task, MultiAgent, StrResponse

creator_agent = AgentConfiguration(
    job_title="Content Creator",
    sub_task=False,  # Prevent infinite loops
    reliability_layer=None,  # Keep it simple
)

reviewer_agent = AgentConfiguration(
    job_title="Content Reviewer",
    sub_task=False,  # Prevent infinite loops
    reliability_layer=None,  # Keep it simple
)

create_content_task = Task(
    description="Create a short poem about the moon.",
    response_format=StrResponse,
    price_id_="create_content"
)

review_content_task = Task(
    description="Review the provided content and provide feedback.",
    response_format=StrResponse,
    context=[create_content_task],  # context is the previous task
    price_id_="review_content"
)

agents = [creator_agent, reviewer_agent]
tasks = [create_content_task, review_content_task]

result = MultiAgent.print_do(agents, tasks)
print(result)
```

Run this with: `python multi_agent_example.py`

### 5.4. Understanding Task Dependencies and Execution Order

The `context` parameter in the `Task` class is how you define dependencies between tasks.  If a task's `context` includes another `Task` object, Upsonic will ensure that the dependency task is executed *before* the dependent task.  The output of the dependency task will then be available as context for the dependent task. This allows for automatic generation of task workflows.

#### 5.4.1. Managing Task Flow and Data Dependencies in Multi-Agent Workflows

Upsonic handles the task flow automatically based on the dependencies you define.  You don't need to manually specify the execution order.

#### 5.4.2. Ensuring Coordinated Execution in Complex Agent Systems

By using the `context` parameter effectively, you can create complex, multi-step workflows involving multiple agents, while ensuring that tasks are executed in the correct order and that data is passed between them correctly.

### 5.5. Conclusion of Chapter 5

In this chapter, you've learned how to use Upsonic's `MultiAgent` class to create multi-agent workflows.  You saw how to:

*   Define multiple agents using `AgentConfiguration`.
*   Create interdependent tasks using the `context` parameter.
*   Execute the workflow using `MultiAgent.do()`.
*   Understand how Upsonic manages task dependencies and execution order.

Multi-agent workflows open up a wide range of possibilities for building sophisticated AI applications. In the next chapter, we will explore making direct LLM calls using `Direct.do()`.

---
## 6. Direct LLM and Upsonic
Sometimes, you may want to call an LLM provider directly, without needing all the features of an Agent.

### 6.1. Direct LLM call.
Upsonic provides the `Direct` class for this.

#### 6.1.1. Making Simple LLM Calls with `Direct.do()`

The `Direct.do()` method allows you to send a task directly to the LLM provider, without creating an `Agent` object. It's a simple and concise for making quick LLM requests.
```python
from upsonic import Direct, StrResponse, Task

task = Task(description="What is the weather like today?", response_format=StrResponse)
result = Direct.do(task)
print(result)

```

#### 6.1.2. When to Use Direct LLM Calls vs. Agent-Based Tasks

*   **Use `Direct.do()` for:**
    *   Simple, one-off LLM requests that don't require tools or complex workflows.
    *   Quick prototyping and experimentation.
    *   Situations where you want the absolute lowest latency and overhead.

*   **Use `Agent` and `Task` for:**
    *   Tasks that require access to tools (MCP servers).
    *   Tasks that require multiple steps or interactions with the LLM.
    *   Workflows involving multiple agents.
    *   When you need the benefits of Upsonic's reliability layer.
    *   When you need structured output using `ObjectResponse` or custom response types.

### 6.2. Using `Direct.do()` for quick calls.

#### 6.2.1.  Streamlining Simple Tasks with Direct LLM Interactions

`Direct.do()` is designed for simplicity and speed.  It bypasses the agent creation and management overhead, making it ideal for quick, one-off LLM requests.

#### 6.2.2.  6.2.2.  Example: Quick Fact Retrieval with `Direct.do()`

```python
from upsonic import Direct, Task, StrResponse

task = Task(description="What is the capital of Australia?", response_format=StrResponse)
result = Direct.do(task)
print(result) # Expected output (may vary slightly): "Canberra"

```
In this, `Direct.do` bypasses the `Agent` creation.

### 6.3. Conclusion of Chapter 6
In this chapter, you've learned:
*  How to use `Direct.do()` to make direct calls to LLM providers.
*  When to use `Direct.do()` versus using a full `Agent` and `Task`.
* The benefits of making direct calls for quick operations.

You have seen how Upsonic offers both a simplified method for running LLMs using `Direct.do()`, while also giving the tools to create more complicated operations with Agents.

---
## 7. Conclusion and Next Steps

### 7.1. Review of the Tutorial Series

#### 7.1.1. Recap of Part 1: Foundational MCP Concepts and `mcp-cli`

In Part 1, you were introduced to:

*   The Model Context Protocol (MCP) and its role in connecting AI agents to external tools and data.
*   The `mcp-cli` tool for exploring and interacting with MCP servers.
*   The basic concepts of MCP: clients, servers, tools, resources, and prompts.
*   The JSON-RPC protocol that underlies MCP communication.
*   How to build a simple search server (`search-server`) and interact with it using `mcp-cli`.

#### 7.1.2. Recap of Part 2: Perplexity Advanced MCP Server

Part 2 focused on:

*   The `perplexity-advanced-mcp` server, which provides a more sophisticated search capability using the Perplexity and OpenRouter APIs.
*   How to install and configure `perplexity-advanced-mcp`.
*   The `ask_perplexity` tool and its different query types (`simple` and `complex`).
*   How to use file attachments to provide context for searches.
*   Integrating `perplexity-advanced-mcp` with Claude Desktop and Cursor.

#### 7.1.3. Recap of Part 3: Advanced Agents with `mcp-agent` and `Upsonic`

Part 3 dived into

* Building more complex agents using `mcp-agent`
* Advanced workflow patterns such as:
    * Parallel
    * Router
    * Orchestrator
    * Evaluator-Optimizer
    * Swarm
* The `mcp-agent` framework and its core components (`MCPApp`, `Agent`, `AugmentedLLM`, `Executor`).
* How to build simple MCP clients using `gen_client` and `MCPConnectionManager`.
* How to build MCP servers using `mcp_agent.mcp.mcp_agent_server`.
* Upsonic, a production-ready framework that builds on top of `mcp-agent`, with a focus on reliability and scalability.
* The concept of a `Task`, how they are constructed, and how to create agents with them.
*  How to install Upsonic and set up API Keys.
* The core concepts of Upsonic - Task, Agent, and the reliability layer.
* The `do()` and `print_do()` methods for executing tasks.
* Using the reliability layer.
* Creating multi-agent workflows with `MultiAgent`
* Making direct LLM calls with `Direct.do()`

### 7.2. The Future of AI Agents and MCP

#### 7.2.1. Evolving Trends in AI Agent Development

The field of AI agents is rapidly evolving. Some key trends include:

*   **Increased Autonomy:** Agents are becoming more capable of independent reasoning and decision-making.
*   **Multi-Agent Systems:**  Complex tasks are increasingly being tackled by teams of collaborating agents.
*   **Integration with Real-World Systems:**  Agents are being connected to a wider range of external tools and data sources.
*   **Focus on Reliability and Safety:**  As agents become more powerful, ensuring their reliability and safety is becoming increasingly important.
*   **Human-in-the-Loop:**  Incorporating human feedback and oversight into agent workflows is crucial for many applications.

#### 7.2.2. The Growing MCP Ecosystem and its Potential

The Model Context Protocol is playing a key role in enabling these trends. By providing a standardized interface for AI agents to interact with the world, MCP:

*   **Simplifies agent development:** Developers can focus on building agent logic, rather than writing custom integrations for each tool.
*   **Promotes interoperability:**  Any MCP client can interact with any MCP server, regardless of who built them or what technologies they use.
*   **Fosters innovation:**  Anyone can create and publish new MCP servers, expanding the capabilities available to the entire ecosystem.

As the MCP ecosystem grows, we can expect to see a proliferation of AI agents capable of performing increasingly complex and useful tasks.

### 7.3. Further Exploration:

#### 7.3.1.  Contributing to the projects

#####  How to Contribute to `mcp-cli`, `mcp-agent`, and `perplexity-advanced-mcp`

All of the projects covered in this tutorial series (`mcp-cli`, `mcp-agent`, and `perplexity-advanced-mcp`, and `search-server`) are open source and welcome contributions. Here's how you can get involved:

1.  **Find a project on GitHub:** Each project has its own repository:
    *   `mcp-cli`: [https://github.com/chrishayuk/mcp-cli](https://github.com/chrishayuk/mcp-cli)
    *   `mcp-agent`: [https://github.com/lastmile-ai/mcp-agent](https://github.com/lastmile-ai/mcp-agent)
    *   `perplexity-advanced-mcp`: [https://github.com/code-yeongyu/perplexity-advanced-mcp](https://github.com/code-yeongyu/perplexity-advanced-mcp)
    *   `search-server`: [https://github.com/fengin/search-server](https://github.com/fengin/search-server)
2.  **Fork the repository:** Create your own copy of the project.
3.  **Create a branch:**  Make your changes in a separate branch.
4.  **Submit a pull request:**  Propose your changes to the original repository.
5.  **Follow the project's contribution guidelines:**  Each project may have specific guidelines for contributing. Look for a `CONTRIBUTING.md` file or similar documentation.

#####  Community and Contribution Guidelines

*   **Report bugs:**  If you find a bug, please report it by opening an issue on the project's GitHub repository.
*   **Suggest features:**  If you have an idea for a new feature, you can also open an issue to discuss it.
*   **Improve documentation:**  Clear and comprehensive documentation is essential for any project.  You can contribute by improving existing documentation or adding new documentation.
*   **Write tests:**  Tests help ensure the quality and stability of the code.  You can contribute by writing new tests or improving existing ones.
*   **Fix bugs:**  If you're able to fix a bug, you can submit a pull request with your changes.
*   **Implement features:**  If you want to implement a new feature, it's often a good idea to discuss it with the project maintainers first (by opening an issue or joining the Discord server).

#### 7.3.2.  Building your own MCP servers

One of the most powerful aspects of MCP is that anyone can create and publish their own servers. This allows you to expose any functionality you need to your AI agents.

#####  Creating Custom MCP Servers for Specialized Tools

You might want to create a custom MCP server if you have:

*   **A private API or data source:**  You can create a server to give your agents access to internal systems.
*   **A specialized tool or algorithm:**  You can wrap your tool in an MCP server to make it available to any MCP client.
*   **A desire to contribute to the MCP ecosystem:**  You can create a server for a publicly available API or service and share it with the community.

#####  Publishing and Sharing Your MCP Servers

Once you've created an MCP server, you can:

*   **Run it locally:**  For private use or testing.
*   **Deploy it to a server:**  For wider access.
*   **Publish it on GitHub:**  To share it with the community.
* **Add it to the MCP server lists**: such as [glama](https://glama.ai/mcp/servers) and [mcprun](https://mcp.run)

#### 7.3.3.  Exploring advanced agent architectures

This tutorial series has covered several agent architectures (e.g., the Evaluator-Optimizer, the Orchestrator, and the Swarm).  There are many other possibilities, and the field is constantly evolving.

#####  Research Directions in Agent Architectures

Some areas of active research include:

*   **Hierarchical agents:**  Agents that are composed of other agents, forming a hierarchy of abstraction.
*   **Multi-modal agents:**  Agents that can process and generate different types of data (text, images, audio, etc.).
*   **Agents with long-term memory:**  Agents that can remember information over extended periods of time.
*   **Agents with theory of mind:**  Agents that can model the beliefs and intentions of other agents (or humans).


8.3.3.2.  Experimenting with Advanced Upsonic Workflows

You can experiment with combining the different workflow patterns provided by `mcp-agent` (which Upsonic builds upon) and Upsonic's features to create new and innovative agent architectures.  For example, you could:

*   Use an `LLMRouter` to choose between different specialized agents.
*   Use a `ParallelLLM` to get multiple perspectives on a problem.
*   Use an `Orchestrator` to manage a complex, multi-step workflow.
*   Use the reliability layer with any agent or workflow.

#### 7.3.4.  Addressing the challenges of production deployment.

Deploying AI agents in production environments presents unique challenges.

#####  Scalability and Performance Considerations for Production Agents

*   **Concurrency:**  Make sure your agent can handle multiple concurrent requests.  `mcp-agent`'s `AsyncioExecutor` and `TemporalExecutor` provide different options for concurrency.  Upsonic builds on top of this.
*   **Rate Limits:**  Be aware of the rate limits of the LLMs and APIs you're using.  Implement appropriate retry mechanisms and consider using multiple API keys.
*   **Caching:**  Cache results where appropriate to reduce API calls and improve performance.
*   **Monitoring:**  Use logging and tracing to monitor your agent's performance and identify bottlenecks.

#####  Security and Monitoring in Production AI Deployments

*   **API Key Management:**  Protect your API keys carefully.  Use environment variables or a secrets management system.  *Never* hardcode API keys in your code.
*   **Input Validation:**  Validate all user inputs to prevent prompt injection attacks.
*   **Output Sanitization:**  Sanitize any output from the LLM before displaying it to users or using it in other systems.
*   **Auditing:**  Log all agent actions and decisions for auditing and debugging purposes.
*   **Monitoring:**  Monitor your agent's performance, error rates, and resource usage.  Use a monitoring system like Prometheus, Grafana, or Datadog. Consider using OpenTelemetry, as described in Part 3, to collect and analyze traces.
* **Error handling:** Ensure your agents, and servers, include robust error handling.

This concludes the tutorial series. You now have a strong understanding of AI agents, the Model Context Protocol, and how to build reliable and scalable agents using Upsonic, and the underlying `mcp-agent` and `mcp-cli` tools. You also have an understanding of more advanced MCP servers (`search-server`, `perplexity-advanced-mcp`) which you can use with your agents. You're well-equipped to start building your own powerful AI applications!
```

## File: refgrab.py (Size: 31.20 KB)

```
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["requests", "fire", "tqdm", "rich", "loguru"]
# ///
# github_downloader.py: Tool for downloading GitHub content intelligently

"""
GitHub Content Downloader - A tool to intelligently download content from GitHub URLs.

This module provides functionality to download files or entire directories from GitHub,
handling various URL formats and preserving directory structure. It respects GitHub's
rate limits and provides detailed logging for troubleshooting.

Usage:
    $ python github_downloader.py --url="https://github.com/user/repo/blob/main/file.py"
    $ python github_downloader.py --file="urls.txt"
"""

import json
import os
import re
import time
from concurrent.futures import ThreadPoolExecutor
from dataclasses import asdict, dataclass, field
from enum import Enum, auto
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple, Union
from urllib.parse import urlparse

import fire
import requests
from loguru import logger
from rich.console import Console
from rich.panel import Panel
from rich.progress import BarColumn, Progress, TaskProgressColumn, TextColumn
from rich.table import Table
from tqdm import tqdm

# Configure loguru for better logging
logger.remove()
logger.add(
    "github_downloader_{time}.log",
    rotation="10 MB",
    level="INFO",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {module}:{function}:{line} - {message}",
)


class ContentType(Enum):
    """Types of content that can be downloaded from GitHub."""

    FILE = auto()
    DIRECTORY = auto()
    REPOSITORY = auto()


@dataclass
class GitHubUrl:
    """
    Parsed representation of a GitHub URL.

    This class parses and validates GitHub URLs, extracting components like owner,
    repo, branch, and path. It determines whether the URL points to a file,
    directory, or entire repository.
    """

    url: str
    owner: str
    repo: str
    branch: str
    path: str = ""
    content_type: ContentType = ContentType.REPOSITORY

    @classmethod
    def from_url(cls, url: str) -> "GitHubUrl":
        """
        Parse a GitHub URL into its components.

        Args:
            url: Full GitHub URL to parse

        Returns:
            GitHubUrl object with parsed components

        Raises:
            ValueError: If the URL is not a valid GitHub URL
        """
        # Handle GitHub URLs
        github_pattern = (
            r"https?://github\.com/([^/]+)/([^/]+)(?:/tree|/blob)?/([^/]+)(?:/(.+))?"
        )
        match = re.match(github_pattern, url)

        if not match:
            raise ValueError(f"Invalid GitHub URL: {url}")

        owner, repo, branch, path = match.groups()
        path = path or ""

        # Determine content type
        content_type = ContentType.REPOSITORY  # Default for repo URLs
        if "/blob/" in url:
            content_type = ContentType.FILE
        elif "/tree/" in url and path:
            content_type = ContentType.DIRECTORY

        return cls(
            url=url,
            owner=owner,
            repo=repo,
            branch=branch,
            path=path,
            content_type=content_type,
        )


@dataclass
class DownloadResult:
    """
    Result of a GitHub content download operation.

    This class tracks metrics and outcomes of downloading content from a GitHub URL,
    including success/failure counts and timing information.
    """

    url: str
    output_dir: str
    files_downloaded: int = 0
    files_failed: int = 0
    duration_seconds: float = 0.0
    error: Optional[str] = None

    @property
    def is_success(self) -> bool:
        """Whether the download was successful (no errors and at least one file downloaded)."""
        return self.error is None and self.files_downloaded > 0


class RateLimiter:
    """
    Rate limiter to manage GitHub API request frequency. This class ensures that requests to the GitHub API don't exceed specified limits,
    helping to avoid rate limit errors that would block further requests.
    """

    def __init__(self, calls_per_second: int = 10):
        """
        Initialize the rate limiter.

        Args:
            calls_per_second: Maximum number of calls allowed per second
        """
        self.calls_per_second = calls_per_second
        self.call_timestamps: List[float] = []

    def wait_if_needed(self) -> None:
        """
        Wait if necessary to respect the rate limit.

        This method tracks API call timestamps and calculates if a delay is needed
        before making another request to stay within the rate limit.
        """
        now = time.time()

        # Remove timestamps older than 1 second
        self.call_timestamps = [t for t in self.call_timestamps if now - t < 1]

        # If we've made too many calls in the last second, wait
        if len(self.call_timestamps) >= self.calls_per_second:
            sleep_time = 1 - (now - self.call_timestamps[0])
            if sleep_time > 0:
                if sleep_time > 0.1:  # Only log if sleep time is significant
                    logger.debug(f"Rate limiting: sleeping for {sleep_time:.2f}s")
                time.sleep(sleep_time)

        # Record this API call
        self.call_timestamps.append(time.time())


class GitHubDownloader:
    """
    Tool to download content from GitHub URLs intelligently.

    This class handles downloading files, directories, or entire repositories from GitHub,
    respecting rate limits and providing detailed logging and progress tracking.

    It uses GitHub's API to get directory listings and raw content, supporting parallel
    downloads for better performance when downloading multiple files.
    """

    def __init__(
        self,
        output_dir: Union[str, Path] = "out",
        rate_limit: int = 10,
        token: Optional[str] = None,
        max_workers: int = 5,
        verbose: bool = False,
    ):
        """
        Initialize the GitHub content downloader.

        Args:
            output_dir: Directory where downloaded content will be stored
            rate_limit: Maximum requests per second to avoid GitHub API rate limiting
            token: GitHub Personal Access Token for higher rate limits (optional)
            max_workers: Maximum number of concurrent download threads
            verbose: Enable verbose logging
        """
        self.output_dir = Path(output_dir)
        self.max_workers = max_workers
        self.verbose = verbose

        # Configure logging level based on verbosity
        if verbose:
            logger.remove()
            logger.add(
                "github_downloader_{time}.log",
                level="DEBUG",
                format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {module}:{function}:{line} - {message}",
            )
            logger.add(lambda msg: print(msg), level="DEBUG")

        # Set up GitHub API headers
        self.headers = {"Accept": "application/vnd.github.v3+json"}
        if token:
            logger.debug("Using GitHub API token for authentication")
            self.headers["Authorization"] = f"token {token}"

        # Create output directory if it doesn't exist
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # Initialize rate limiter
        self.rate_limiter = RateLimiter(calls_per_second=rate_limit)

        # Set up rich console for prettier output
        self.console = Console()

    def _make_request(self, url: str) -> requests.Response:
        """
        Make a rate-limited request to the GitHub API.

        This method handles rate limiting and retries for API requests,
        ensuring we don't exceed GitHub's limits and handling temporary failures.

        Args:
            url: GitHub API URL to request

        Returns:
            Response object

        Raises:
            requests.RequestException: If the request fails after retries
        """
        self.rate_limiter.wait_if_needed()
        logger.debug(f"Making request to: {url}")

        try:
            response = requests.get(url, headers=self.headers, timeout=30)

            # Handle rate limiting
            if (
                response.status_code == 403
                and "rate limit exceeded" in response.text.lower()
            ):
                reset_time = int(response.headers.get("X-RateLimit-Reset", 0))
                wait_time = max(reset_time - time.time(), 0)

                logger.warning(
                    f"Rate limit exceeded. Waiting {wait_time:.1f} seconds..."
                )
                time.sleep(wait_time + 1)  # Add 1 second buffer

                # Try again
                return self._make_request(url)

            # Log response status for debugging
            logger.debug(f"Response status: {response.status_code}")
            return response

        except requests.RequestException as e:
            logger.error(f"Request error: {str(e)}")
            raise

    def _get_repo_contents(
        self, github_url: GitHubUrl, path: str = ""
    ) -> List[Dict[str, Any]]:
        """
        Get contents of a repository directory.

        This method queries the GitHub API to get a listing of files and subdirectories
        at a specific path within a repository.

        Args:
            github_url: Parsed GitHub URL object
            path: Path within the repository (overrides github_url.path if provided)

        Returns:
            List of content items (files and directories)
        """
        content_path = path or github_url.path
        api_url = f"https://api.github.com/repos/{github_url.owner}/{github_url.repo}/contents/{content_path}?ref={github_url.branch}"

        response = self._make_request(api_url)

        if response.status_code == 404 and not content_path:
            # Special handling for repository root when the API returns 404
            # This often happens with large repositories where GitHub doesn't provide direct contents
            # Fetch the git tree instead
            tree_url = f"https://api.github.com/repos/{github_url.owner}/{github_url.repo}/git/trees/{github_url.branch}?recursive=1"
            tree_response = self._make_request(tree_url)

            if tree_response.status_code == 200:
                tree_data = tree_response.json()
                # Convert git tree format to contents format
                result = []
                for item in tree_data.get("tree", []):
                    if item.get("type") == "blob":
                        # Only include items at the root level
                        file_path = item.get("path", "")
                        if "/" not in file_path:
                            result.append(
                                {
                                    "name": file_path,
                                    "path": file_path,
                                    "type": "file",
                                    "download_url": f"https://raw.githubusercontent.com/{github_url.owner}/{github_url.repo}/{github_url.branch}/{file_path}",
                                }
                            )
                    elif item.get("type") == "tree":
                        dir_path = item.get("path", "")
                        if "/" not in dir_path:
                            result.append(
                                {"name": dir_path, "path": dir_path, "type": "dir"}
                            )
                return result

        if response.status_code != 200:
            logger.error(
                f"Failed to get contents: {response.status_code} - {response.text}"
            )
            return []

        # For regular API responses
        content_data = response.json()

        # Handle case where the API returns a single file instead of an array
        if not isinstance(content_data, list):
            return [content_data]

        return content_data

    def _download_file(
        self, github_url: GitHubUrl, path: str, output_path: Path
    ) -> bool:
        """
        Download a specific file from GitHub.

        This method downloads a single file from GitHub and saves it to the specified path,
        creating parent directories as needed.

        Args:
            github_url: Parsed GitHub URL object
            path: Path to the file within the repository
            output_path: Local path where file will be saved

        Returns:
            True if successful, False otherwise
        """
        # Create parent directories if they don't exist
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # Get raw content URL
        raw_url = f"https://raw.githubusercontent.com/{github_url.owner}/{github_url.repo}/{github_url.branch}/{path}"

        try:
            logger.debug(f"Downloading file: {path} to {output_path}")
            response = self._make_request(raw_url)

            if response.status_code != 200:
                logger.error(f"Failed to download {path}: {response.status_code}")
                return False

            with open(output_path, "wb") as f:
                f.write(response.content)

            logger.debug(f"Successfully downloaded: {path}")
            return True

        except Exception as e:
            logger.error(f"Error downloading {path}: {str(e)}")
            return False

    def _download_directory(
        self,
        github_url: GitHubUrl,
        path: str,
        output_dir: Path,
        progress_callback: Optional[callable] = None,
    ) -> Tuple[int, int]:
        """
        Download a complete directory from GitHub.

        This method recursively downloads a directory and all its contents from GitHub,
        preserving the directory structure in the output location.

        Args:
            github_url: Parsed GitHub URL object
            path: Path to the directory within the repository
            output_dir: Local directory where files will be saved
            progress_callback: Callback function to report progress (optional)

        Returns:
            Tuple of (files downloaded successfully, files failed to download)
        """
        # Create the output directory
        output_dir.mkdir(parents=True, exist_ok=True)

        # Get directory contents
        contents = self._get_repo_contents(github_url, path)

        if not contents:
            logger.warning(f"No contents found at: {path}")
            return 0, 0

        files_downloaded = 0
        files_failed = 0

        # Create a task list to process in parallel
        file_tasks = []

        # First process directories (recursively)
        for item in contents:
            item_path = item.get("path", "")
            item_type = item.get("type", "")
            local_path = output_dir / os.path.basename(item_path)

            if item_type == "dir":
                # Process subdirectory recursively
                logger.debug(f"Processing subdirectory: {item_path}")
                subdir_downloaded, subdir_failed = self._download_directory(
                    github_url, item_path, local_path, progress_callback
                )
                files_downloaded += subdir_downloaded
                files_failed += subdir_failed

                # Report progress
                if progress_callback:
                    progress_callback()

            elif item_type == "file":
                # Queue file download for parallel processing
                file_tasks.append((github_url, item_path, local_path))

        # Process file downloads in parallel
        if file_tasks:
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_path = {
                    executor.submit(self._download_file, *task): task[1]
                    for task in file_tasks
                }

                for future in future_to_path:
                    path = future_to_path[future]
                    try:
                        success = future.result()
                        if success:
                            files_downloaded += 1
                        else:
                            files_failed += 1

                        # Report progress
                        if progress_callback:
                            progress_callback()

                    except Exception as e:
                        logger.error(f"Exception while downloading {path}: {str(e)}")
                        files_failed += 1

                        # Report progress
                        if progress_callback:
                            progress_callback()

        return files_downloaded, files_failed

    def download_url(
        self, url: str, folder_name: Optional[str] = None
    ) -> DownloadResult:
        """
        Download content from a GitHub URL.

        This method handles downloading different types of GitHub content (files, directories,
        repositories) based on the URL format, creating an appropriate folder structure.

        Args:
            url: GitHub URL to download content from
            folder_name: Custom folder name (optional)

        Returns:
            DownloadResult containing success/failure metrics
        """
        try:
            # Parse the GitHub URL
            github_url = GitHubUrl.from_url(url)

            # Generate a folder name if not provided
            if not folder_name:
                if github_url.path:
                    folder_name = f"{github_url.owner}_{github_url.repo}_{github_url.path.replace('/', '_')}"
                else:
                    folder_name = f"{github_url.owner}_{github_url.repo}"

            output_path = self.output_dir / folder_name

            # Create the output directory
            output_path.mkdir(parents=True, exist_ok=True)

            # Save URL metadata
            with open(output_path / "source.json", "w") as f:
                json.dump(
                    {
                        "url": url,
                        "owner": github_url.owner,
                        "repo": github_url.repo,
                        "branch": github_url.branch,
                        "path": github_url.path,
                        "content_type": github_url.content_type.name,
                        "download_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                    },
                    f,
                    indent=2,
                )

            # Download the content
            start_time = time.time()
            files_downloaded = 0
            files_failed = 0

            # Simple progress tracking without rich.progress
            logger.info(
                f"Downloading {github_url.content_type.name.lower()} from {url}"
            )

            if github_url.content_type == ContentType.FILE:
                # Download a single file
                success = self._download_file(
                    github_url,
                    github_url.path,
                    output_path / os.path.basename(github_url.path),
                )

                files_downloaded = 1 if success else 0
                files_failed = 0 if success else 1

            else:
                # Download a directory or entire repository
                # First try to get repo contents - this will use our improved method
                contents = self._get_repo_contents(github_url, github_url.path)

                if contents:
                    # We were able to get directory contents, proceed with normal download
                    files_downloaded, files_failed = self._download_directory(
                        github_url,
                        github_url.path,
                        output_path,
                        lambda: None,  # Empty callback for progress
                    )
                elif github_url.content_type == ContentType.REPOSITORY:
                    # For repositories we couldn't get contents for, try to download README and other key files
                    # This is a fallback for when the API doesn't provide directory contents
                    readme_paths = ["README.md", "README", "readme.md", "Readme.md"]
                    for readme in readme_paths:
                        success = self._download_file(
                            github_url, readme, output_path / readme
                        )
                        if success:
                            files_downloaded += 1
                            logger.info(f"Downloaded {readme} from repository")

                    # Try to download additional common files
                    common_files = [
                        "LICENSE",
                        "LICENSE.txt",
                        "requirements.txt",
                        "setup.py",
                        "pyproject.toml",
                        ".gitignore",
                    ]
                    for file in common_files:
                        success = self._download_file(
                            github_url, file, output_path / file
                        )
                        if success:
                            files_downloaded += 1
                            logger.info(f"Downloaded {file} from repository")

                # If still no files downloaded, try to use the git trees API
                if files_downloaded == 0:
                    # Use git trees API as a fallback
                    tree_url = f"https://api.github.com/repos/{github_url.owner}/{github_url.repo}/git/trees/{github_url.branch}?recursive=1"
                    response = self._make_request(tree_url)

                    if response.status_code == 200:
                        tree_data = response.json()
                        max_files = 50  # Limit the number of files to download
                        count = 0

                        for item in tree_data.get("tree", []):
                            if item.get("type") == "blob" and count < max_files:
                                file_path = item.get("path", "")
                                local_path = output_path / file_path
                                success = self._download_file(
                                    github_url, file_path, local_path
                                )

                                if success:
                                    files_downloaded += 1
                                    count += 1
                                else:
                                    files_failed += 1

            duration = time.time() - start_time

            # Create a result summary
            result = DownloadResult(
                url=url,
                output_dir=str(output_path),
                files_downloaded=files_downloaded,
                files_failed=files_failed,
                duration_seconds=round(duration, 2),
            )

            # Log the result
            if files_downloaded > 0:
                logger.info(
                    f"Downloaded {files_downloaded} files from {url} to {output_path}"
                )
                if files_failed > 0:
                    logger.warning(f"Failed to download {files_failed} files")
            else:
                logger.error(f"No files downloaded from {url}")
                result.error = "No files were downloaded"

            return result

        except Exception as e:
            logger.error(f"Error processing {url}: {str(e)}")
            return DownloadResult(url=url, output_dir="", error=str(e))

    def process_file(self, file_path: Union[str, Path]) -> List[DownloadResult]:
        """
        Process a file containing GitHub URLs, one per line.

        This method reads a file containing GitHub URLs and downloads content from each URL,
        tracking success/failure metrics for each download.

        Args:
            file_path: Path to file containing URLs

        Returns:
            List of DownloadResult objects for each URL processed
        """
        file_path = Path(file_path)

        try:
            with open(file_path, "r") as f:
                urls = [
                    line.strip()
                    for line in f
                    if line.strip() and not line.strip().startswith("#")
                ]
        except Exception as e:
            logger.error(f"Error reading URL file: {str(e)}")
            return [
                DownloadResult(
                    url="", output_dir="", error=f"Could not read file: {str(e)}"
                )
            ]

        return self.process_urls(urls)

    def process_urls(self, urls: List[str]) -> List[DownloadResult]:
        """
        Process multiple GitHub URLs.

        This method downloads content from multiple GitHub URLs, tracking metrics for each
        and generating a summary report at the end.

        Args:
            urls: List of GitHub URLs to process

        Returns:
            List of DownloadResult objects for each URL processed
        """
        results: List[DownloadResult] = []

        logger.info(f"Processing {len(urls)} GitHub URLs")

        # Display initial table of URLs to process
        url_table = Table(title="URLs to Process")
        url_table.add_column("Index", justify="right", style="cyan")
        url_table.add_column("URL", style="green")
        url_table.add_column("Type", style="magenta")

        for i, url in enumerate(urls, 1):
            try:
                github_url = GitHubUrl.from_url(url)
                url_type = github_url.content_type.name
            except ValueError:
                url_type = "INVALID"

            url_table.add_row(str(i), url, url_type)

        self.console.print(url_table)

        # Process each URL with tqdm progress bar (simpler than rich.progress)
        for url in tqdm(urls, desc="Processing URLs", unit="url"):
            logger.info(f"Processing: {url}")

            # Download content from the URL
            result = self.download_url(url)
            results.append(result)

        # Print summary table
        summary_table = Table(title="Download Results Summary")
        summary_table.add_column("URL", style="green")
        summary_table.add_column("Status", style="bold")
        summary_table.add_column("Files", justify="right")
        summary_table.add_column("Failed", justify="right")
        summary_table.add_column("Duration", justify="right")
        summary_table.add_column("Output Directory")

        for result in results:
            status = "✅ Success" if result.is_success else f"❌ Failed: {result.error}"

            summary_table.add_row(
                result.url,
                status,
                str(result.files_downloaded),
                str(result.files_failed),
                f"{result.duration_seconds:.2f}s",
                result.output_dir,
                style=None if result.is_success else "dim",
            )

        self.console.print(summary_table)

        # Calculate overall statistics
        successful = sum(1 for r in results if r.is_success)
        total_files = sum(r.files_downloaded for r in results)
        total_failed = sum(r.files_failed for r in results)
        total_duration = sum(r.duration_seconds for r in results)

        stats_panel = Panel(
            f"[bold]Total URLs:[/bold] {len(urls)}  "
            f"[bold green]Successful:[/bold green] {successful}  "
            f"[bold red]Failed:[/bold red] {len(urls) - successful}\n"
            f"[bold]Total Files Downloaded:[/bold] {total_files}  "
            f"[bold red]Failed Files:[/bold red] {total_failed}  "
            f"[bold]Total Duration:[/bold] {total_duration:.2f}s",
            title="Download Statistics",
        )
        self.console.print(stats_panel)

        # Save results summary to JSON
        summary_path = self.output_dir / "download_summary.json"
        with open(summary_path, "w") as f:
            json.dump([asdict(r) for r in results], f, indent=2)

        logger.info(f"Results summary saved to {summary_path}")
        return results


def main(
    url: str = None,
    file: str = None,
    output: str = "out",
    token: str = None,
    rate_limit: int = 10,
    max_workers: int = 5,
    verbose: bool = False,
    fix_invalid: bool = True,
):
    """
    Download content from GitHub URLs.

    This is the main entry point for the GitHub Downloader tool, supporting both
    single URL downloads and batch processing from a file.

    Args:
        url: Single GitHub URL to download
        file: Path to file containing GitHub URLs (one per line)
        output: Output directory for downloaded content
        token: GitHub Personal Access Token (optional)
        rate_limit: Maximum GitHub API requests per second
        max_workers: Maximum number of concurrent download threads
        verbose: Enable verbose logging
        fix_invalid: Try to fix invalid repository URLs by appending /tree/master
    """
    if not url and not file:
        console = Console()
        console.print(
            Panel(
                "[bold red]Error:[/bold red] Please provide either a URL or a file containing URLs",
                title="GitHub Downloader",
            )
        )
        return

    # Get GitHub token from environment if not provided as argument
    if not token and "GITHUB_TOKEN" in os.environ:
        token = os.environ["GITHUB_TOKEN"]

    # Initialize the downloader
    downloader = GitHubDownloader(
        output_dir=output,
        token=token,
        rate_limit=rate_limit,
        max_workers=max_workers,
        verbose=verbose,
    )

    console = Console()

    # Print welcome message
    console.print(
        Panel(
            "GitHub Content Downloader Tool\n"
            "Downloads files, directories, or repositories from GitHub URLs",
            title="🚀 GitHub Downloader",
            subtitle="v1.0.0",
        )
    )

    # Process URLs
    if url:
        # Try to fix invalid repository URL
        if fix_invalid and not re.search(r"/tree/|/blob/", url):
            if url.endswith("/"):
                url = f"{url}tree/master"
            else:
                url = f"{url}/tree/master"
            logger.info(f"Modified URL to: {url}")

        console.print(f"[bold]Processing single URL:[/bold] {url}")
        result = downloader.download_url(url)

        # Display result
        if result.is_success:
            console.print(
                Panel(
                    f"[bold green]Successfully downloaded[/bold green] {result.files_downloaded} files\n"
                    f"Output directory: {result.output_dir}\n"
                    f"Duration: {result.duration_seconds:.2f} seconds",
                    title=f"Download Complete: {url}",
                )
            )
        else:
            console.print(
                Panel(
                    f"[bold red]Error:[/bold red] {result.error}",
                    title=f"Download Failed: {url}",
                )
            )

    if file:
        urls = []

        # Read and process URLs from file
        with open(file, "r") as f:
            raw_urls = [
                line.strip()
                for line in f
                if line.strip() and not line.strip().startswith("#")
            ]

        # Fix invalid repository URLs
        if fix_invalid:
            for raw_url in raw_urls:
                url = raw_url
                if not re.search(r"/tree/|/blob/", url):
                    if url.endswith("/"):
                        url = f"{url}tree/master"
                    else:
                        url = f"{url}/tree/master"
                    logger.info(f"Modified URL: {raw_url} -> {url}")
                urls.append(url)
        else:
            urls = raw_urls

        console.print(f"[bold]Processing URLs from file:[/bold] {file}")
        downloader.process_urls(urls)


if __name__ == "__main__":
    fire.Fire(main)
```

## File: websearch_python_options.txt (Size: 234.74 KB)

```
# Folder Tree Structure

out
├── 47thommy_langgraph_searching.py
│   ├── searching.py
│   └── source.json
├── AI-secure_AgentPoison_ReAct_search.py
│   ├── search.py
│   └── source.json
├── EllAchE_llama-out-loud_brave.py
│   ├── brave.py
│   └── source.json
├── Hovup-FO_TKM-Groq-Connector_agents.py
│   ├── agents.py
│   └── source.json
├── RMNCLDYO_perplexity-ai-toolkit
│   ├── LICENSE
│   ├── README.md
│   ├── requirements.txt
│   └── source.json
├── ShivadityaKr_BraveAI-Scraper
│   ├── LICENSE
│   ├── README.md
│   ├── requirements.txt
│   └── source.json
├── bm611_aisearch_search_backend_api.py
│   ├── api.py
│   └── source.json
├── chenxingyuzealken_AIBootCampCourse_llm_search.py
│   ├── search.py
│   └── source.json
├── cnunescoelho_kiroku_agents_tools.py
│   ├── source.json
│   └── tools.py
├── cohere-ai_cohere-toolkit_src_backend_tools_brave_search
│   ├── __init__.py
│   ├── client.py
│   ├── source.json
│   └── tool.py
├── crewAIInc_crewAI-tools_crewai_tools_tools_brave_search_tool
│   ├── README.md
│   ├── __init__.py
│   ├── brave_search_tool.py
│   └── source.json
├── crewAIInc_crewAI-tools_crewai_tools_tools_firecrawl_search_tool
│   ├── README.md
│   ├── firecrawl_search_tool.py
│   └── source.json
├── crewAIInc_crewAI-tools_crewai_tools_tools_website_search
│   ├── README.md
│   ├── source.json
│   └── website_search_tool.py
├── download_summary.json
├── echohive42_AI-book-maker-with-perplexity-search-grounding
│   ├── README.md
│   ├── requirements.txt
│   └── source.json
├── jsonallen_perplexity-mcp
│   ├── LICENSE
│   ├── README.md
│   ├── pyproject.toml
│   └── source.json
├── leemark_llamaindex_playbook_examples_rag_web_search_brave.py
│   ├── rag_web_search_brave.py
│   └── source.json
├── morispolanco_leyesdeguatemala_tavily.py
│   ├── source.json
│   └── tavily.py
├── mshumer_OpenReasoningEngine_tools.py
│   ├── source.json
│   └── tools.py
├── niship2_news-flow_tools_tavilysearch.py
│   ├── source.json
│   └── tavilysearch.py
├── niship2_news-flow_tools_youcom.py
│   ├── source.json
│   └── youcom.py
├── niship2_newsapi2_subs_youcomsearch.py
│   ├── source.json
│   └── youcomsearch.py
├── oborys_security-ai-agent-brama_braveSearch.py
│   ├── braveSearch.py
│   └── source.json
├── pengfeng_ask.py
│   ├── LICENSE
│   ├── README.md
│   ├── requirements.txt
│   └── source.json
├── rsrohan99_Llama-Researcher_tavily.py
│   ├── source.json
│   └── tavily.py
├── run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-bing-search
│   ├── BUILD
│   ├── CHANGELOG.md
│   ├── Makefile
│   ├── README.md
│   ├── examples
│   │   └── bing_search.ipynb
│   ├── llama_index
│   │   └── tools
│   │       └── bing_search
│   │           ├── BUILD
│   │           ├── __init__.py
│   │           └── base.py
│   ├── pyproject.toml
│   ├── source.json
│   └── tests
│       ├── BUILD
│       ├── __init__.py
│       └── test_tools_bing_search.py
├── run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-brave-search
│   ├── BUILD
│   ├── Makefile
│   ├── README.md
│   ├── examples
│   │   └── brave_search.ipynb
│   ├── llama_index
│   │   └── tools
│   │       └── brave_search
│   │           ├── BUILD
│   │           ├── __init__.py
│   │           └── base.py
│   ├── pyproject.toml
│   ├── source.json
│   └── tests
│       ├── BUILD
│       ├── __init__.py
│       └── test_tools_brave_search.py
├── run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-linkup-research
│   ├── BUILD
│   ├── CHANGELOG.md
│   ├── Makefile
│   ├── README.md
│   ├── examples
│   │   └── linkup.ipynb
│   ├── llama_index
│   │   └── tools
│   │       └── linkup_research
│   │           ├── BUILD
│   │           ├── __init__.py
│   │           └── base.py
│   ├── pyproject.toml
│   ├── source.json
│   └── tests
│       ├── BUILD
│       ├── __init__.py
│       └── test_tools_linkup_research.py
├── run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-tavily-research
│   ├── BUILD
│   ├── CHANGELOG.md
│   ├── Makefile
│   ├── README.md
│   ├── examples
│   │   └── tavily.ipynb
│   ├── llama_index
│   │   └── tools
│   │       └── tavily_research
│   │           ├── BUILD
│   │           ├── __init__.py
│   │           └── base.py
│   ├── pyproject.toml
│   ├── requirements.txt
│   ├── source.json
│   └── tests
│       ├── BUILD
│       ├── __init__.py
│       └── test_tools_tavily_research.py
├── ssiddhantsood_thalamus-backend-hackmit_toolkitutils_search_toolkit.py
│   ├── search_toolkit.py
│   └── source.json
├── steven-haddix_rezai
│   ├── README.md
│   ├── pyproject.toml
│   └── source.json
├── steven-haddix_rezai_rezai_services_youcom_service.py
│   ├── service.py
│   └── source.json
└── tom-doerr_perplexity_search
    ├── LICENSE
    ├── README.md
    ├── pyproject.toml
    ├── requirements.txt
    └── source.json

53 directories, 128 files



# Folder: out

## File: 47thommy_langgraph_searching.py/searching.py (Size: 2.62 KB)

```
from langgraph.graph import StateGraph, START, END
from typing_extensions import TypedDict
from typing import Annotated
from operator import add
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_community.document_loaders import WikipediaLoader
from langchain_community.tools import TavilySearchResults

from dotenv import load_dotenv
import os

load_dotenv()
open_ai_api_key = os.getenv("OPENAI_API_KEY")
tavily_api_key = os.getenv("TAVILY_API_KEY")

llm = ChatOpenAI(api_key=open_ai_api_key)


class State(TypedDict):
    question: str
    answer: str
    context: Annotated[list, add]


# define our search nodes


def search_web(state: State):
    """Retrieves docs from websearch"""

    tavily_search = TavilySearchResults(max_results=3)
    search_docs = tavily_search.invoke(state["question"])
    formatted_search_docs = "\n\n---\n\n".join(
        [
            f'<Document href="{doc["url"]}"/>\n{doc["content"]}\n</Document>'
            for doc in search_docs
        ]
    )

    return {"context": [formatted_search_docs]}


def search_wikipidia(state: State):
    """Retrieves docs from wikipidia"""
    # search
    search_docs = WikipediaLoader(query=state["question"], load_max_docs=2).load()
    # Format
    formatted_search_docs = "\n\n---\n\n".join(
        [
            f'<Document source="{doc.metadata["source"]}" page="{doc.metadata.get("page", "")}"/>\n{doc.page_content}\n</Document>'
            for doc in search_docs
        ]
    )

    return {"context": [formatted_search_docs]}


def generate_answer(state: State):
    """Node to answer a question"""
    context = state["context"]
    question = state["question"]
    answer_template = """Answer the question {question} using this context:{context}"""
    answer_instruction = answer_template.format(question=question, context=context)
    answer = llm.invoke(
        [SystemMessage(content=answer_instruction)]
        + [HumanMessage(content="Answer the question")]
    )

    return {"answer": answer}


# start building our graph

workflow = StateGraph(State)

# register our nodes

workflow.add_node("search_web", search_web)
workflow.add_node("search_wikipidia", search_wikipidia)
workflow.add_node("generate_answer", generate_answer)

# define our flow
workflow.add_edge(START, "search_web")
workflow.add_edge(START, "search_wikipidia")
workflow.add_edge("search_web", "generate_answer")
workflow.add_edge("search_wikipidia", "generate_answer")
workflow.add_edge("generate_answer", END)

graph = workflow.compile()

result = graph.invoke({"question": "How were Nvidia's Q2 2024 earnings"})
print(result["answer"].content)
```

## File: 47thommy_langgraph_searching.py/source.json (Size: 0.23 KB)

```
{
  "url": "https://github.com/47thommy/langgraph/blob/main/searching.py",
  "owner": "47thommy",
  "repo": "langgraph",
  "branch": "main",
  "path": "searching.py",
  "content_type": "FILE",
  "download_time": "2025-02-25 01:16:29"
}
```

## File: AI-secure_AgentPoison_ReAct_search.py/search.py (Size: 1.89 KB)

```
import os
import json
from serpapi import GoogleSearch



SERPAPI_API_KEY = os.environ["SERPAPI_API_KEY"]

def extract_answer(res):
    if "answer_box" in res.keys() and "answer" in res["answer_box"].keys():
        toret = '[' + res["answer_box"]["title"] + '] ' + res["answer_box"]["answer"]
    elif "answer_box" in res.keys() and "snippet" in res["answer_box"].keys():
        toret = '[' + res["answer_box"]["title"] + '] ' + res["answer_box"]["snippet"]
    elif "answer_box" in res.keys() and "snippet_highlighted_words" in res["answer_box"].keys():
        toret = '[' + res["answer_box"]["title"] + '] ' + res["answer_box"]["snippet_highlighted_words"][0]
    elif "organic_results" in res.keys() and "snippet" in res["organic_results"][0].keys():
        toret = '[' + res["organic_results"][0]["title"] + '] ' + res["organic_results"][0]["snippet"]
    else:
        toret = None
    return toret


def search(question):
    current_file = os.path.abspath(__file__)
    current_dir = os.path.dirname(current_file)
    try:
        current_dict = json.load(open((os.path.join(current_dir, "search.json"))))
    except:
        current_dict = {}

    if question in current_dict.keys():
        return current_dict[question]

    params = {
        "api_key": SERPAPI_API_KEY,
        "engine": "google",
        "q": question,
        "google_domain": "google.com",
        "gl": "us",
        "hl": "en"
    }

    search = GoogleSearch(params)
    res = search.get_dict()

    toret = extract_answer(res)
    if toret:
        current_dict[question] = toret
        with open(os.path.join(current_dir, "search.json"), "w") as fout:
            json.dump(current_dict, fout, indent=2)
    return toret


def _test_search():

    queries = [
        "Serianna is a band of metalcore genre. site: wikipedia.org"
    ]

    for q in queries:
        res = search(q)
        print(res)


if __name__ == "__main__":
    _test_search()
```

## File: AI-secure_AgentPoison_ReAct_search.py/source.json (Size: 0.25 KB)

```
{
  "url": "https://github.com/AI-secure/AgentPoison/blob/master/ReAct/search.py",
  "owner": "AI-secure",
  "repo": "AgentPoison",
  "branch": "master",
  "path": "ReAct/search.py",
  "content_type": "FILE",
  "download_time": "2025-02-25 01:16:30"
}
```

## File: EllAchE_llama-out-loud_brave.py/brave.py (Size: 1.19 KB)

```
import requests
import os

'''
Things to ask about
- how big is this now
'''

def brave_req(query: str):
    url = f"https://api.search.brave.com/res/v1/web/search?q={query.replace(' ', '+')}"

    payload = {}
    headers = {
      'Accept': 'application/json',
      'Accept-Encoding': 'gzip',
      'X-Subscription-Token': os.getenv("BRAVE_API_KEY")
    }

    response = requests.request("GET", url, headers=headers, data=payload)

    return response.json()

# take the brave response and concat all of the descriptiosn in the web results
def concat_brave_search_results(brave_search_response):
    results = brave_search_response['web']['results']
    concatted = ""
    for result in results:
        concatted += result['description'] + "\n"

    return concatted

# hits the summarizer endpoint of the brave api
def summarizer(query: str):
    url = f"https://api.search.brave.com/res/v1/web/search?q={query.replace(' ', '+')}&summary=1"

    payload = {}
    headers = {
      'Accept': 'application/json',
      'Accept-Encoding': 'gzip',
      'X-Subscription-Token': os.getenv("BRAVE_API_KEY")
    }

    response = requests.request("GET", url, headers=headers, data=payload)

    print(response.text)
```

## File: EllAchE_llama-out-loud_brave.py/source.json (Size: 0.23 KB)

```
{
  "url": "https://github.com/EllAchE/llama-out-loud/blob/main/brave.py",
  "owner": "EllAchE",
  "repo": "llama-out-loud",
  "branch": "main",
  "path": "brave.py",
  "content_type": "FILE",
  "download_time": "2025-02-25 01:16:41"
}
```

## File: Hovup-FO_TKM-Groq-Connector_agents.py/agents.py (Size: 0.80 KB)

```
from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper
from langchain_community.tools.tavily_search.tool import TavilySearchResults
from langchain_groq import ChatGroq
from langchain.agents import initialize_agent, AgentType
import os

# Configuraciones de API
groq_api_key = os.getenv("GROQ_API_KEY")
tavily_api_key = os.getenv("TAVILY_API_KEY")

def create_tavily_agent(model_id, temperature=0.7):
    os.environ["TAVILY_API_KEY"] = tavily_api_key

    llm = ChatGroq(model=model_id, temperature=temperature)
    search = TavilySearchAPIWrapper()
    tavily_tool = TavilySearchResults(api_wrapper=search)
    agent_chain = initialize_agent(
        [tavily_tool],
        llm,
        agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,
        verbose=True
    )
    return agent_chain
```

## File: Hovup-FO_TKM-Groq-Connector_agents.py/source.json (Size: 0.24 KB)

```
{
  "url": "https://github.com/Hovup-FO/TKM-Groq-Connector/blob/main/agents.py",
  "owner": "Hovup-FO",
  "repo": "TKM-Groq-Connector",
  "branch": "main",
  "path": "agents.py",
  "content_type": "FILE",
  "download_time": "2025-02-25 01:16:41"
}
```

## File: RMNCLDYO_perplexity-ai-toolkit/.gitignore (Size: 3.02 KB)

```
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/#use-with-ide
.pdm.toml

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/
.DS_Store
```

## File: RMNCLDYO_perplexity-ai-toolkit/LICENSE (Size: 1.04 KB)

```
MIT License

Copyright (c) 2024 RMNCLDYO

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

## File: RMNCLDYO_perplexity-ai-toolkit/README.md (Size: 16.40 KB)

```
<p align="center">
    <a href="https://www.perplexity.ai/" title="Go to Perplexity">
        <img src="https://img.shields.io/badge/PERPLEXITY%20AI-20808d?style=for-the-badge&logo=data:image/png;base64,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" alt="Perplexity AI">
    </a>
</p>

<p align="center">
    <a href="https://github.com/RMNCLDYO/perplexity-ai-toolkit" title="Go to repo">
        <img src="https://img.shields.io/badge/dynamic/json?style=for-the-badge&label=Perplexity+AI+Toolkit&query=version&url=https%3A%2F%2Fraw.githubusercontent.com%2FRMNCLDYO%2Fperplexity-ai-toolkit%2Fmain%2F.github%2Fversion.json" alt="Perplexity AI Toolkit">
    </a>
</p>

<p align="center">
    <a href=".github/CHANGELOG.md" title="Go to changelog"><img src="https://img.shields.io/badge/maintained-yes-2ea44f?style=for-the-badge" alt="maintained - yes"></a>
    <a href=".github/CONTRIBUTING.md" title="Go to contributions"><img src="https://img.shields.io/badge/contributions-welcome-2ea44f?style=for-the-badge" alt="contributions - welcome"></a>
</p>

<p align="center">
    <a href="/">
        <picture>
          <source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/RMNCLDYO/perplexity-ai-toolkit/main/.github/pplx-logo-dark.png">
          <source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/RMNCLDYO/perplexity-ai-toolkit/main/.github/pplx-logo-light.png">
          <img alt="Perplexity AI" width="500" src="https://raw.githubusercontent.com/RMNCLDYO/perplexity-ai-toolkit/main/.github/pplx-logo-light.png">
        </picture>
    </a>
</p>

## Overview
The Perplexity AI Toolkit makes it easy to use Perplexity Labs' `Sonar` language models (built on top of Meta's latest and most advanced model `LLama-3.1`) for creating chatbots, generating text, and searching the web (***in real-time***). It's designed for everyone, from beginners to experienced developers, allowing quick addition of AI features to projects with simple commands. While it offers simplicity and lightweight integration, it doesn't compromise on power; experienced developers can access the full suite of advanced options available via the API, ensuring robust customization and control. This toolkit is perfect for those looking to efficiently tap into advanced AI without getting bogged down in technical details, yet it still provides the depth needed for complex project requirements.

## Key Features
- **Conversational AI**: Create interactive, real-time chat experiences (chatbots) or AI assistants.
- **Real-Time Web Search**: Conduct online searches in real-time with precise query responses.
- **Highly Customizable**: Tailor settings like streaming output, system prompts, sampling temperature and more to suit your specific requirements.
- **Lightweight Integration**: Efficiently designed with minimal dependencies, requiring only the `requests` package for core functionality.

## Prerequisites
- `Python 3.x`
- An API key from Perplexity AI

## Dependencies
The following Python packages are required:
- `requests`: For making HTTP requests to the Perplexity API.

The following Python packages are optional:
- `python-dotenv`: For managing API keys and other environment variables.

## Installation
To use the Perplexity AI Toolkit, clone the repository to your local machine and install the required Python packages.

Clone the repository:
```bash
git clone https://github.com/RMNCLDYO/perplexity-ai-toolkit.git
```

Navigate to the repositories folder:
```bash
cd perplexity-ai-toolkit
```

Install the required dependencies:
```bash
pip install -r requirements.txt
```

## Configuration
1. Obtain an API key from [Perplexity](https://www.perplexity.ai/).
2. You have three options for managing your API key:
   <details>
   <summary>Click here to view the API key configuration options</summary>
   
   - **Setting it as an environment variable on your device (recommended for everyday use)**
       - Navigate to your terminal.
       - Add your API key like so:
         ```shell
         export PERPLEXITY_API_KEY=your_api_key
         ```
       This method allows the API key to be loaded automatically when using the wrapper or CLI.
     
   - **Using an .env file (recommended for development):**
       - Install python-dotenv if you haven't already: `pip install python-dotenv`.
       - Create a .env file in the project's root directory.
       - Add your API key to the .env file like so:
         ```makefile
         PERPLEXITY_API_KEY=your_api_key
         ```
       This method allows the API key to be loaded automatically when using the wrapper or CLI, assuming you have python-dotenv installed and set up correctly.
     
   - **Direct Input:**
       - If you prefer not to use a `.env` file, you can directly pass your API key as an argument to the CLI or the wrapper functions.
         
         ***CLI***
         ```shell
         --api_key "your_api_key"
         ```
         ***Wrapper***
         ```shell
         api_key="your_api_key"
         ```
       This method requires manually inputting your API key each time you initiate an API call, ensuring flexibility for different deployment environments.
   </details>

## Usage
The Perplexity AI Toolkit can be used in two different modes: `Chat`, and `Search`. Each mode is designed for specific types of interactions with the language models.

## Chat Mode
Chat mode is intended for chatting with an AI model (similar to a chatbot) or building conversational applications.

#### Example Usage

***CLI***
```bash
python cli.py --chat
```

***Wrapper***
```python
from perplexity import Chat

Chat().run()
```

> An executable version of this example can be found [here](./examples/example_chat.py). (*You must move this file to the root folder before running the program.*)

## Search Mode
Search mode is intended for searching online (in real-time) for a single query as perplexity does not support multi-turn conversations with their online models.

#### Example Usage

***CLI***
```bash
python cli.py --search --query "What is today's date?"
```

***Wrapper***
```python
from perplexity import Search

Search().run(query="What is today's date?")
```

> An executable version of this example can be found [here](./examples/example_search.py). (*You must move this file to the root folder before running the program.*)

*Search mode is limited to 'online' models, such as `llama-3.1-sonar-small-128k-online`, `llama-3.1-sonar-large-128k-online` and `llama-3.1-sonar-huge-128k-online`.*

## Advanced Configuration

### CLI and Wrapper Options
| **Description**                          | **CLI Flags**                | **CLI Usage**                                       | **Wrapper Usage**                                 |
|------------------------------------------|------------------------------|-----------------------------------------------------|---------------------------------------------------|
| Enable chat mode                         | `-c`,  `--chat`              | --chat                                              | *See mode usage above*                            |
| Enable online search mode                | `-s`,  `--search`            | --search                                            | *See mode usage above*                            |
| Online search query                      | `-q`,  `--query`             | --query "What is today's date?"                     | query="What is today's date?"                     |
| User prompt                              | `-p`,  `--prompt`            | --prompt "How many stars are there in our galaxy?"  | prompt="How many stars are there in our galaxy?"  |
| API key for authentication               | `-a`,  `--api_key`           | --api_key your_api_key                              | api_key="your_api_key"                            |
| Model name                               | `-m`,  `--model`             | --model "llama-3.1-sonar-small-128k-chat"           | model="llama-3.1-sonar-small-128k-chat"           |
| Enable streaming mode                    | `-st`, `--stream`            | --stream                                            | stream=True                                       |
| System prompt (instructions)             | `-sp`, `--system_prompt`     | --system_prompt "Be precise and concise."           | system_prompt="Be precise and concise."           |
| Maximum tokens to generate               | `-mt`, `--max_tokens`        | --max_tokens 100                                    | max_tokens=100                                    |
| Sampling temperature                     | `-tm`, `--temperature`       | --temperature 0.7                                   | temperature=0.7                                   |
| Nucleus sampling threshold               | `-tp`, `--top_p`             | --top_p 0.9                                         | top_p=0.9                                         |
| Top-k sampling threshold                 | `-tk`, `--top_k`             | --top_k 40                                          | top_k=40                                          |
| Penalize tokens based on their presence  | `-pp`, `--presence_penalty`  | --presence_penalty 0.5                              | presence_penalty=0.5                              |
| Penalize tokens based on their frequency | `-fp`, `--frequency_penalty` | --frequency_penalty 0.5                             | frequency_penalty=0.5                             |

> *To exit the program at any time, you can type **`exit`** or **`quit`**. This command works similarly whether you're interacting with the program via the CLI or through the Python wrapper ensuring that you can easily and safely conclude your work with the Perplexity AI Toolkit without having to resort to interrupt signals or forcibly closing the terminal or command prompt.*

## Available Models

Perplexity offers both native models and a selection of large, open-source instruct models.

### Online Models

| **Model**                           | **Parameter Count** | **Context Length** |
|-------------------------------------|---------------------|--------------------|
| `llama-3.1-sonar-small-128k-online` | 8B                  | 127,072            |
| `llama-3.1-sonar-large-128k-online` | 70B                 | 127,072            |
| `llama-3.1-sonar-huge-128k-online`  | 405B                | 127,072            |

- *Perplexity makes note that the search subsystem of the Online LLMs do not attend to the system prompt. You can only use the system prompt to provide instructions related to style, tone, and language of the response.*

### Chat Models

| **Model**                         | **Parameter Count** | **Context Length** |
|-----------------------------------|---------------------|--------------------|
| `llama-3.1-sonar-small-128k-chat` | 8B                  | 131,072            |
| `llama-3.1-sonar-large-128k-chat` | 70B                 | 131,072            |

### Open-Source Models

| **Model**                | **Parameter Count** | **Context Length** |
|--------------------------|---------------------|--------------------|
| `llama-3.1-8b-instruct`  | 8B                  | 131,072            |
| `llama-3.1-70b-instruct` | 70B                 | 131,072            |

- *Where possible, Perplexity tries to match the Hugging Face implementation.*

## Contributing
Contributions are welcome!

Please refer to [CONTRIBUTING.md](.github/CONTRIBUTING.md) for detailed guidelines on how to contribute to this project.

## Reporting Issues
Encountered a bug? We'd love to hear about it. Please follow these steps to report any issues:

1. Check if the issue has already been reported.
2. Use the [Bug Report](.github/ISSUE_TEMPLATE/bug_report.md) template to create a detailed report.
3. Submit the report [here](https://github.com/RMNCLDYO/perplexity-ai-toolkit/issues).

Your report will help us make the project better for everyone.

## Feature Requests
Got an idea for a new feature? Feel free to suggest it. Here's how:

1. Check if the feature has already been suggested or implemented.
2. Use the [Feature Request](.github/ISSUE_TEMPLATE/feature_request.md) template to create a detailed request.
3. Submit the request [here](https://github.com/RMNCLDYO/perplexity-ai-toolkit/issues).

Your suggestions for improvements are always welcome.

## Versioning and Changelog
Stay up-to-date with the latest changes and improvements in each version:

- [CHANGELOG.md](.github/CHANGELOG.md) provides detailed descriptions of each release.

## Security
Your security is important to us. If you discover a security vulnerability, please follow our responsible disclosure guidelines found in [SECURITY.md](.github/SECURITY.md). Please refrain from disclosing any vulnerabilities publicly until said vulnerability has been reported and addressed.

## License
Licensed under the MIT License. See [LICENSE](LICENSE) for details.
```

## File: RMNCLDYO_perplexity-ai-toolkit/requirements.txt (Size: 0.11 KB)

```
# Core dependencies
requests

# Optional dependencies
python-dotenv

# Additional dependencies may be added as needed
```

## File: RMNCLDYO_perplexity-ai-toolkit/source.json (Size: 0.24 KB)

```
{
  "url": "https://github.com/RMNCLDYO/perplexity-ai-toolkit/tree/master",
  "owner": "RMNCLDYO",
  "repo": "perplexity-ai-toolkit",
  "branch": "master",
  "path": "",
  "content_type": "REPOSITORY",
  "download_time": "2025-02-25 01:16:55"
}
```

## File: ShivadityaKr_BraveAI-Scraper/LICENSE (Size: 1.04 KB)

```
MIT License

Copyright (c) 2024 Shivaditya Kr

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

## File: ShivadityaKr_BraveAI-Scraper/README.md (Size: 2.40 KB)

```
# BraveAI-Scraper

**BraveAI-Scraper** is a Python tool that scrapes data from Brave AI, allowing users to access model responses without needing an API key. This project offers a free solution for developers and researchers to integrate Brave AI's capabilities into their applications.

## Features

- Scrape data from Brave AI
- Access model responses without an API key
- Headless browsing option

## Installation

1. Clone the repository:
   ```sh
   git clone https://github.com/yourusername/BraveAI-Scraper.git
   cd BraveAI-Scraper
   ```

2. Install the required dependencies:
   ```sh
   pip install playwright
   playwright install
   ```

## Usage

```python
import time
from playwright.sync_api import sync_playwright

class BraveSearcher:
    def __init__(self, headless=True):
        self.playwright = sync_playwright().start()
        self.browser = self.playwright.chromium.launch(headless=headless)
        self.page = self.browser.new_page(bypass_csp=True)

    def search(self, query):
        base_url = "https://search.brave.com"
        search_url = f"{base_url}/search?q={query}&source=llmSuggest&summary=1"
        start_time = time.time()  
        self.page.goto(search_url, timeout=100000)
        self.page.wait_for_selector('//div[@id="chatllm-context"]', timeout=30000)
        page_content = self.page.locator('//div[@id="chatllm-content"]').inner_text()
        end_time = time.time()  
        execution_time = end_time - start_time  
        print(f"Execution time: {execution_time} seconds") 
        return page_content

    def close(self):
        self.browser.close()
        self.playwright.stop()

if __name__ == '__main__':
    searcher = BraveSearcher(headless=True)
    try:
        query = "tell me a joke"
        result = searcher.search(query)
        print(result)
    finally:
        searcher.close()
```

## Example

To use the `BraveSearcher` class, initialize it, call the `search` method with your query, and finally close the browser instance:

```python
searcher = BraveSearcher(headless=True)
try:
    query = "tell me a joke"
    result = searcher.search(query)
    print(result)
finally:
    searcher.close()
```

## Disclaimer

This project is intended for educational purposes only. The developers are not responsible for any misuse of this tool. Please use it responsibly and in accordance with all applicable laws and regulations.

## License

This project is licensed under the MIT License.
```

## File: ShivadityaKr_BraveAI-Scraper/requirements.txt (Size: 0.01 KB)

```
playwright
```

## File: ShivadityaKr_BraveAI-Scraper/source.json (Size: 0.23 KB)

```
{
  "url": "https://github.com/ShivadityaKr/BraveAI-Scraper/tree/master",
  "owner": "ShivadityaKr",
  "repo": "BraveAI-Scraper",
  "branch": "master",
  "path": "",
  "content_type": "REPOSITORY",
  "download_time": "2025-02-25 01:17:18"
}
```

## File: bm611_aisearch_search_backend_api.py/api.py (Size: 4.42 KB)

```
# Helper functions for Gemini, Brave search & BS4 scraping

from dotenv import load_dotenv
import google.generativeai as genai
import os
import requests
from time import sleep
import json
from bs4 import BeautifulSoup

# Load environment variables from .env file
load_dotenv()
BRAVE_API_KEY = os.getenv("BRAVE_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# gemini api config
genai.configure(api_key=GEMINI_API_KEY)
model_json = genai.GenerativeModel(
    "gemini-1.5-flash-002",
    generation_config={"response_mime_type": "application/json"},
)
model = genai.GenerativeModel("gemini-1.5-flash")


def get_search_results(search_query: str):
    headers = {"Accept": "application/json", "X-Subscription-Token": BRAVE_API_KEY}
    response = requests.get(
        "https://api.search.brave.com/res/v1/web/search",
        params={
            "q": search_query,
            "count": 3,  # Max number of results to return
        },
        headers=headers,
        timeout=60,
    )
    if not response.ok:
        raise Exception(f"HTTP error {response.status_code}")
    sleep(1)  # avoid Brave rate limit
    return response.json().get("web", {}).get("results")


def generate_search_response(context, user_question):
    SEARCH_PROMPT = f"""
    You are an expert at summarizing search results and providing concise answers to user question.

    USER QUESTION: {user_question}

    CONTEXT: {context}
    """
    response = model.generate_content(SEARCH_PROMPT).text
    return response


def generate_search_queries(user_question):
    SEARCH_PROMPT = f"""
    You are an expert at generating search queries for the Brave search engine.
    Generate three search queries that are relevant to this question.

    {user_question}

    Format: {{"queries": ["query_1", "query_2", "query_3"]}}
    """
    response = model_json.generate_content(SEARCH_PROMPT)
    search_result = json.loads(response.text)
    return search_result


def get_unique_search_results(search_queries):
    urls_seen = set()
    web_search_results = []
    for query in search_queries["queries"]:
        search_results = get_search_results(query)
        for result in search_results:
            url = result.get("url")
            if not url or url in urls_seen:
                continue

            urls_seen.add(url)
            web_search_results.append(result)
    return web_search_results


def get_text_from_url(url):
    try:
        # Send a GET request to the URL
        response = requests.get(url)

        # Check if the request was successful
        response.raise_for_status()

        # Parse the HTML content
        soup = BeautifulSoup(response.text, "html.parser")

        # Remove all script and style elements
        for script in soup(["script", "style"]):
            script.decompose()

        # Get text
        text = soup.get_text()

        # Break into lines and remove leading and trailing space on each
        lines = (line.strip() for line in text.splitlines())

        # Break multi-headlines into a line each
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))

        # Drop blank lines
        text = "\n".join(chunk for chunk in chunks if chunk)

        return text
    except requests.RequestException as e:
        # print(f"An error occurred: {e}")
        return None


def fetch_metadata(user_question):
    search_queries = generate_search_queries(user_question)
    print(search_queries)
    search_results = get_unique_search_results(search_queries)
    metadata = [
        result.get("profile") for result in search_results if result.get("profile")
    ]
    print(metadata)
    return metadata


# def generate_answer(user_question, metadata):
#     context = ""
#     for profile in metadata:
#         page_text = get_text_from_url(profile.get("url"))
#         if page_text is not None:
#             context += page_text
#     response = generate_search_response(context, user_question)
#     return response


def generate_answer(user_question, metadata):
    context = ""
    for profile in metadata:
        page_text = get_text_from_url(profile.get("url"))
        if page_text is not None:
            context += page_text

    SEARCH_PROMPT = f"""
    You are an expert at summarizing search results and providing concise answers to user questions.

    USER QUESTION: {user_question}

    CONTEXT: {context}
    """

    response = model.generate_content(SEARCH_PROMPT, stream=True)
    for chunk in response:
        yield chunk.text
```

## File: bm611_aisearch_search_backend_api.py/source.json (Size: 0.24 KB)

```
{
  "url": "https://github.com/bm611/aisearch/blob/main/search/backend/api.py",
  "owner": "bm611",
  "repo": "aisearch",
  "branch": "main",
  "path": "search/backend/api.py",
  "content_type": "FILE",
  "download_time": "2025-02-25 01:16:30"
}
```

## File: chenxingyuzealken_AIBootCampCourse_llm_search.py/search.py (Size: 2.42 KB)

```
# search.py

import os
from dotenv import load_dotenv
from langchain.agents import initialize_agent, Tool
from langchain_community.retrievers import TavilySearchAPIRetriever
from langchain.llms import OpenAI

# Load environment variables from the .env file
load_dotenv()

# Retrieve API Keys from environment variables
import streamlit as st
# Retrieve OpenAI API Key from environment variables
openai_api_key = st.secrets["OPENAI_API_KEY"]

tavily_api_key = os.getenv('TAVILY_API_KEY')

if not openai_api_key:
    openai_api_key = input("Please enter your OpenAI API key: ")

# Optionally, raise an error if the user does not provide a key
if not openai_api_key:
    raise ValueError("API key is required to proceed.")

if not tavily_api_key:
    tavily_api_key = input("Please enter your Tavily API key: ")

# Optionally, raise an error if the user does not provide a key
if not tavily_api_key:
    raise ValueError("API key is required to proceed.")

# Initialize LLM (OpenAI) using the API key
llm = OpenAI(temperature=0, api_key=openai_api_key)

from langchain_community.tools import TavilySearchResults

tool = TavilySearchResults(
    max_results=5,
    search_depth="advanced",
    include_answer=True,
    include_raw_content=True,
    include_images=False,
    # include_domains=[...],
    # exclude_domains=[...],
    # name="...",            # overwrite default tool name
    # description="...",     # overwrite default tool description
    # args_schema=...,       # overwrite default args_schema: BaseModel
)

def generate_prose_with_references(response):
    """Generate a prose summary from the search response with references."""
    if not isinstance(response, list) or not response:
        return "No information available to generate prose."
    
    prose = ""
    references = "\nReferences:\n"
    for idx, doc in enumerate(response, start=1):
        prose += f"{doc['content']} "
        references += f"[{idx}] {doc['url']}\n"
    
    return f"Here is something I found from online: {prose.strip()}\n\n{references}"


def perform_search(query):
    """Perform a search using Tavily Search tool and return the response along with URLs as references."""
    # Run the query using the search agent
    response = tool.invoke({"query": query})

    response = generate_prose_with_references(response)

    return response
    
if __name__ == "__main__":
    question = "What happens to the special account at age 55?"
    print(perform_search(question))
```

## File: chenxingyuzealken_AIBootCampCourse_llm_search.py/source.json (Size: 0.26 KB)

```
{
  "url": "https://github.com/chenxingyuzealken/AIBootCampCourse/blob/main/llm/search.py",
  "owner": "chenxingyuzealken",
  "repo": "AIBootCampCourse",
  "branch": "main",
  "path": "llm/search.py",
  "content_type": "FILE",
  "download_time": "2025-02-25 01:16:31"
}
```

## File: cnunescoelho_kiroku_agents_tools.py/source.json (Size: 0.24 KB)

```
{
  "url": "https://github.com/cnunescoelho/kiroku/blob/main/agents/tools.py",
  "owner": "cnunescoelho",
  "repo": "kiroku",
  "branch": "main",
  "path": "agents/tools.py",
  "content_type": "FILE",
  "download_time": "2025-02-25 01:16:31"
}
```

## File: cnunescoelho_kiroku_agents_tools.py/tools.py (Size: 1.16 KB)

```
# Copyright (c) 2024 Claudionor Coelho Jr, Fabrício José Vieira Ceolin, Luiza Nacif Coelho

from langchain_community.utilities import WikipediaAPIWrapper, ArxivAPIWrapper
from langchain_experimental.utilities import PythonREPL
from langchain_community.tools import (
    WikipediaQueryRun,
    ArxivQueryRun,
)
from tavily import TavilyClient
import os

from .pubmed import PubMedAPIWrapper
from langchain_community.tools.pubmed.tool import PubmedQueryRun
from langchain_core.tools import Tool

tavily_api_key = os.environ.get("TAVILY_API_KEY")

if tavily_api_key:
    tavily = TavilyClient(tavily_api_key)
else:
    tavily = None

pubmed = PubmedQueryRun()
pubmed.api_wrapper = PubMedAPIWrapper()
arxiv = ArxivQueryRun()
wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())
python_repl = PythonREPL()
python_repl = Tool(
    name="python_repl",
    description="A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.",
    func=python_repl.run,
)

tools_list = [wikipedia, python_repl, arxiv, pubmed]

tools = {tool.name: tool for tool in tools_list}
```

## File: cohere-ai_cohere-toolkit_src_backend_tools_brave_search/__init__.py (Size: 0.16 KB)

```
from backend.tools.brave_search.client import BraveClient
from backend.tools.brave_search.tool import BraveWebSearch

__all__ = ["BraveWebSearch", "BraveClient"]
```

## File: cohere-ai_cohere-toolkit_src_backend_tools_brave_search/client.py (Size: 9.42 KB)

```
import os
from enum import StrEnum
from typing import Dict, Optional

import httpx
import requests
from pydantic import BaseModel
from tenacity import AsyncRetrying, stop_after_attempt, wait_fixed


class SafeSearchTypes(StrEnum):
    OFF = "off"
    MODERATE = "moderate"
    STRICT = "strict"


class WebSearchResponse(BaseModel):
    """
    Web search response model.
    """

    type: str = "search"
    discussions: Dict = {}
    faq: Dict = {}
    infobox: Dict = {}
    locations: Dict = {}
    mixed: Dict = {}
    news: Dict = {}
    query: Dict = {}
    videos: Dict = {}
    web: Dict = {}
    summarizer: Dict = {}


class BraveApiException(Exception):
    """
    Custom exception for Brave API errors.
    """

    pass


class BraveClient:
    BASE_URL = "https://api.search.brave.com/res/v1/"
    WEB_SEARCH_API_ENDPOINT = "web/search"
    MAX_RESULTS = 20
    MAX_OFFSET = 9
    WAIT_RETRY_SECONDS = 2
    RETRY_ATTEMPTS = 2

    def __init__(self, api_key: Optional[str] = None) -> None:
        self.api_key = api_key if api_key else os.environ.get("BRAVE_API_KEY", None)
        self.web_search_endpoint = f"{self.BASE_URL}{self.WEB_SEARCH_API_ENDPOINT}"

    def _get_headers(self) -> Dict:
        """
        Construct the request headers with the API key.
        """
        return {
            "Accept": "application/json",
            "Accept-Encoding": "gzip",
            "X-Subscription-Token": self.api_key,
        }

    def _prepare_params(
        self,
        q: str,
        country: Optional[str] = None,
        search_lang: Optional[str] = None,
        ui_lang: Optional[str] = None,
        count: Optional[int] = MAX_RESULTS,
        offset: Optional[int] = 0,
        safesearch: Optional[str] = SafeSearchTypes.MODERATE,
        freshness: Optional[str] = None,
        text_decorations: Optional[bool] = False,
        spellcheck: Optional[bool] = True,
        goggles_id: Optional[str] = None,
        extra_snippets: Optional[bool] = False,
        include_domains: Optional[list[str]] = [],
    ) -> Dict:
        """
        Prepare the query parameters for the API request.
        """
        # Validate the query parameter - limits set by the API endpoint
        # see https://api.search.brave.com/app/documentation/web-search/query
        if not q or len(q) > 400 or len(q.split()) > 50:
            raise ValueError("Invalid query parameter 'q'")

        # Construct the query parameters - see here for more details:
        # https://search.brave.com/help/operators
        if include_domains:
            q_domains = " OR ".join([f"site:{item}" for item in include_domains])
            q = f"{q} {q_domains}"

        params = {
            "q": q,
            "country": country,
            "search_lang": search_lang,
            "ui_lang": ui_lang,
            "count": min(count, self.MAX_RESULTS),
            "offset": min(offset, self.MAX_OFFSET),
            "safesearch": safesearch,
            "freshness": freshness,
            "text_decorations": text_decorations,
            "spellcheck": spellcheck,
            "goggles_id": goggles_id,
            "extra_snippets": extra_snippets,
        }
        # Filter None values
        params = {k: v for k, v in params.items() if v is not None}

        return params

    def _get(self, params: Optional[Dict] = None) -> Optional[requests.Response]:
        """
        GET request method placeholder.
        """

        headers = self._get_headers()
        response = requests.get(
            self.web_search_endpoint, headers=headers, params=params
        )

        return response

    def search(
        self,
        q: str,
        country: Optional[str] = None,
        search_lang: Optional[str] = None,
        ui_lang: Optional[str] = None,
        count: Optional[int] = MAX_RESULTS,
        offset: Optional[int] = 0,
        safesearch: Optional[str] = SafeSearchTypes.MODERATE,
        freshness: Optional[str] = None,
        text_decorations: Optional[bool] = False,
        spellcheck: Optional[bool] = True,
        goggles_id: Optional[str] = None,
        extra_snippets: Optional[bool] = False,
        include_domains: Optional[list[str]] = [],
        raw: Optional[bool] = False,
    ) -> WebSearchResponse:
        """
        Perform a web search using the Brave Search API.

        Parameters:
            q: str
                The search query (required).
            country: str
                The 2-character country code.
            search_lang: str
                The search language preference.
            ui_lang: str
                The user interface language preference.
            count: int
                The number of results to return (default: 20, max: 20).
            offset: int
                The number of results to skip.
            safesearch: str
                Filter for adult content ('off', 'moderate', 'strict').
            freshness: str
                Filter for search result freshness.
            text_decorations: bool
                Include decoration markers in result strings.
            spellcheck: bool
                Spellcheck the query (default: True).
            goggles_id: str
                Custom re-ranking goggles ID.
            extra_snippets: bool
                Include extra snippets in the search results.
            include_domains: list[str]
                List of domains to include in the search results.
            raw: bool
                Return the raw API response (default: False).

        Returns:
            WebSearchApiResponse
        """

        params = self._prepare_params(
            q,
            country,
            search_lang,
            ui_lang,
            count,
            offset,
            safesearch,
            freshness,
            text_decorations,
            spellcheck,
            goggles_id,
            extra_snippets,
            include_domains,
        )

        response = self._get(params=params)

        if response.status_code != 200:
            raise BraveApiException(
                f"Brave API Error: {response.status_code} - {response.text}"
            )

        if raw:
            return response.json()
        return WebSearchResponse.model_validate(response.json())

    async def _get_async(self, params: Optional[Dict] = None) -> httpx.Response:
        """
        Asynchronous GET request method.
        """
        headers = self._get_headers()

        async for attempt in AsyncRetrying(
            stop=stop_after_attempt(self.RETRY_ATTEMPTS),
            wait=wait_fixed(self.WAIT_RETRY_SECONDS),
        ):
            with attempt:
                async with httpx.AsyncClient() as client:
                    response = await client.get(
                        self.web_search_endpoint, headers=headers, params=params
                    )
                    return response

    async def search_async(
        self,
        q: str,
        country: Optional[str] = None,
        search_lang: Optional[str] = None,
        ui_lang: Optional[str] = None,
        count: Optional[int] = MAX_RESULTS,
        offset: Optional[int] = 0,
        safesearch: Optional[str] = SafeSearchTypes.MODERATE,
        freshness: Optional[str] = None,
        text_decorations: Optional[bool] = False,
        spellcheck: Optional[bool] = True,
        goggles_id: Optional[str] = None,
        extra_snippets: Optional[bool] = False,
        include_domains: Optional[list[str]] = [],
        raw: Optional[bool] = False,
    ) -> WebSearchResponse:
        """
        Perform a web search using the Brave Search API asynchronously.

        Parameters:
            q: str
                The search query (required).
            country: str
                The 2-character country code.
            search_lang: str
                The search language preference.
            ui_lang: str
                The user interface language preference.
            count: int
                The number of results to return (default: 20, max: 20).
            offset: int
                The number of results to skip.
            safesearch: str
                Filter for adult content ('off', 'moderate', 'strict').
            freshness: str
                Filter for search result freshness.
            text_decorations: bool
                Include decoration markers in result strings.
            spellcheck: bool
                Spellcheck the query (default: True).
            goggles_id: str
                Custom re-ranking goggles ID.
            extra_snippets: bool
                Include extra snippets in the search results.
            include_domains: list[str]
                List of domains to include in the search results.
            raw: bool
                Return the raw API response (default: False).

        Returns:
            WebSearchApiResponse
        """

        # Construct the query parameters
        params = self._prepare_params(
            q,
            country,
            search_lang,
            ui_lang,
            count,
            offset,
            safesearch,
            freshness,
            text_decorations,
            spellcheck,
            goggles_id,
            extra_snippets,
            include_domains,
        )

        response = await self._get_async(params=params)

        if response.status_code != 200:
            raise BraveApiException(
                f"Brave API Error: {response.status_code} - {response.text}"
            )

        if raw:
            return response.json()
        return WebSearchResponse.model_validate(response.json())
```

## File: cohere-ai_cohere-toolkit_src_backend_tools_brave_search/source.json (Size: 0.28 KB)

```
{
  "url": "https://github.com/cohere-ai/cohere-toolkit/tree/main/src/backend/tools/brave_search",
  "owner": "cohere-ai",
  "repo": "cohere-toolkit",
  "branch": "main",
  "path": "src/backend/tools/brave_search",
  "content_type": "DIRECTORY",
  "download_time": "2025-02-25 01:16:31"
}
```

## File: cohere-ai_cohere-toolkit_src_backend_tools_brave_search/tool.py (Size: 2.36 KB)

```
from typing import Any

from backend.config.settings import Settings
from backend.schemas.context import Context
from backend.schemas.tool import ToolCategory, ToolDefinition
from backend.tools.base import BaseTool, ToolArgument
from backend.tools.brave_search.client import BraveClient


class BraveWebSearch(BaseTool):
    ID = "brave_web_search"
    BRAVE_API_KEY = Settings().get('tools.brave_web_search.api_key')

    def __init__(self):
        self.client = BraveClient(api_key=self.BRAVE_API_KEY)
        self.num_results = 20

    @classmethod
    def is_available(cls) -> bool:
        return cls.BRAVE_API_KEY is not None

    @classmethod
    def get_tool_definition(cls) -> ToolDefinition:
        return ToolDefinition(
            name=cls.ID,
            display_name="Brave Web Search",
            implementation=cls,
            parameter_definitions={
                "query": {
                    "description": "Query for retrieval.",
                    "type": "str",
                    "required": True,
                }
            },
            is_visible=False,
            is_available=cls.is_available(),
            error_message=cls.generate_error_message(),
            category=ToolCategory.WebSearch,
            description=(
                "Returns a list of relevant document snippets for a textual query retrieved "
                "from the internet using Brave Search."
            ),
        ) # type: ignore

    async def call(
        self, parameters: dict, ctx: Context, **kwargs: Any
    ) -> list[dict[str, Any]]:
        query = parameters.get("query", "")

        # Get domain filtering from kwargs
        filtered_domains = kwargs.get(ToolArgument.DOMAIN_FILTER, [])

        try:
            response = await self.client.search_async(
                q=query, count=self.num_results, include_domains=filtered_domains
            )
        except Exception as e:
            return self.get_tool_error(details=str(e))

        response = dict(response)

        results = response.get("web", {}).get("results", [])

        if not results:
            return self.get_no_results_error()

        tool_results = []
        for result in results:
            tool_results.append({
                "text": result.get("description"),
                "title": result.get("title"),
                "url": result.get("url"),
            })

        return tool_results
```

## File: crewAIInc_crewAI-tools_crewai_tools_tools_brave_search_tool/README.md (Size: 1.55 KB)

```
# BraveSearchTool Documentation

## Description
This tool is designed to perform a web search for a specified query from a text's content across the internet. It utilizes the Brave Web Search API, which is a REST API to query Brave Search and get back search results from the web. The following sections describe how to curate requests, including parameters and headers, to Brave Web Search API and get a JSON response back.

## Installation
To incorporate this tool into your project, follow the installation instructions below:
```shell
pip install 'crewai[tools]'
```

## Example
The following example demonstrates how to initialize the tool and execute a search with a given query:

```python
from crewai_tools import BraveSearchTool

# Initialize the tool for internet searching capabilities
tool = BraveSearchTool()
```

## Steps to Get Started
To effectively use the `BraveSearchTool`, follow these steps:

1. **Package Installation**: Confirm that the `crewai[tools]` package is installed in your Python environment.
2. **API Key Acquisition**: Acquire a API key [here](https://api.search.brave.com/app/keys).
3. **Environment Configuration**: Store your obtained API key in an environment variable named `BRAVE_API_KEY` to facilitate its use by the tool.

## Conclusion
By integrating the `BraveSearchTool` into Python projects, users gain the ability to conduct real-time, relevant searches across the internet directly from their applications. By adhering to the setup and usage guidelines provided, incorporating this tool into projects is streamlined and straightforward.
```

## File: crewAIInc_crewAI-tools_crewai_tools_tools_brave_search_tool/__init__.py (Size: 0.00 KB)

```

```

## File: crewAIInc_crewAI-tools_crewai_tools_tools_brave_search_tool/brave_search_tool.py (Size: 3.97 KB)

```
import datetime
import os
import time
from typing import Any, ClassVar, Optional, Type

import requests
from crewai.tools import BaseTool
from pydantic import BaseModel, Field


def _save_results_to_file(content: str) -> None:
    """Saves the search results to a file."""
    filename = f"search_results_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.txt"
    with open(filename, "w") as file:
        file.write(content)
    print(f"Results saved to {filename}")


class BraveSearchToolSchema(BaseModel):
    """Input for BraveSearchTool."""

    search_query: str = Field(
        ..., description="Mandatory search query you want to use to search the internet"
    )


class BraveSearchTool(BaseTool):
    """
    BraveSearchTool - A tool for performing web searches using the Brave Search API.

    This module provides functionality to search the internet using Brave's Search API,
    supporting customizable result counts and country-specific searches.

    Dependencies:
        - requests
        - pydantic
        - python-dotenv (for API key management)
    """

    name: str = "Brave Web Search the internet"
    description: str = (
        "A tool that can be used to search the internet with a search_query."
    )
    args_schema: Type[BaseModel] = BraveSearchToolSchema
    search_url: str = "https://api.search.brave.com/res/v1/web/search"
    country: Optional[str] = ""
    n_results: int = 10
    save_file: bool = False
    _last_request_time: ClassVar[float] = 0
    _min_request_interval: ClassVar[float] = 1.0  # seconds

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        if "BRAVE_API_KEY" not in os.environ:
            raise ValueError(
                "BRAVE_API_KEY environment variable is required for BraveSearchTool"
            )

    def _run(
        self,
        **kwargs: Any,
    ) -> Any:
        current_time = time.time()
        if (current_time - self._last_request_time) < self._min_request_interval:
            time.sleep(
                self._min_request_interval - (current_time - self._last_request_time)
            )
        BraveSearchTool._last_request_time = time.time()
        try:
            search_query = kwargs.get("search_query") or kwargs.get("query")
            if not search_query:
                raise ValueError("Search query is required")

            save_file = kwargs.get("save_file", self.save_file)
            n_results = kwargs.get("n_results", self.n_results)

            payload = {"q": search_query, "count": n_results}

            if self.country != "":
                payload["country"] = self.country

            headers = {
                "X-Subscription-Token": os.environ["BRAVE_API_KEY"],
                "Accept": "application/json",
            }

            response = requests.get(self.search_url, headers=headers, params=payload)
            response.raise_for_status()  # Handle non-200 responses
            results = response.json()

            if "web" in results:
                results = results["web"]["results"]
                string = []
                for result in results:
                    try:
                        string.append(
                            "\n".join(
                                [
                                    f"Title: {result['title']}",
                                    f"Link: {result['url']}",
                                    f"Snippet: {result['description']}",
                                    "---",
                                ]
                            )
                        )
                    except KeyError:
                        continue

            content = "\n".join(string)
        except requests.RequestException as e:
            return f"Error performing search: {str(e)}"
        except KeyError as e:
            return f"Error parsing search results: {str(e)}"
        if save_file:
            _save_results_to_file(content)
            return f"\nSearch results: {content}\n"
        else:
            return content
```

## File: crewAIInc_crewAI-tools_crewai_tools_tools_brave_search_tool/source.json (Size: 0.29 KB)

```
{
  "url": "https://github.com/crewAIInc/crewAI-tools/tree/main/crewai_tools/tools/brave_search_tool",
  "owner": "crewAIInc",
  "repo": "crewAI-tools",
  "branch": "main",
  "path": "crewai_tools/tools/brave_search_tool",
  "content_type": "DIRECTORY",
  "download_time": "2025-02-25 01:16:33"
}
```

## File: crewAIInc_crewAI-tools_crewai_tools_tools_firecrawl_search_tool/README.md (Size: 1.31 KB)

```
# FirecrawlSearchTool

## Description

[Firecrawl](https://firecrawl.dev) is a platform for crawling and convert any website into clean markdown or structured data.

## Installation

- Get an API key from [firecrawl.dev](https://firecrawl.dev) and set it in environment variables (`FIRECRAWL_API_KEY`).
- Install the [Firecrawl SDK](https://github.com/mendableai/firecrawl) along with `crewai[tools]` package:

```
pip install firecrawl-py 'crewai[tools]'
```

## Example

Utilize the FirecrawlSearchTool as follows to allow your agent to load websites:

```python
from crewai_tools import FirecrawlSearchTool

tool = FirecrawlSearchTool(query='what is firecrawl?')
```

## Arguments

- `api_key`: Optional. Specifies Firecrawl API key. Defaults is the `FIRECRAWL_API_KEY` environment variable.
- `query`: The search query string to be used for searching.
- `page_options`: Optional. Options for result formatting.
  - `onlyMainContent`: Optional. Only return the main content of the page excluding headers, navs, footers, etc.
  - `includeHtml`: Optional. Include the raw HTML content of the page. Will output a html key in the response.
  - `fetchPageContent`: Optional. Fetch the full content of the page.
- `search_options`: Optional. Options for controlling the crawling behavior.
  - `limit`: Optional. Maximum number of pages to crawl.
```

## File: crewAIInc_crewAI-tools_crewai_tools_tools_firecrawl_search_tool/firecrawl_search_tool.py (Size: 3.92 KB)

```
from typing import TYPE_CHECKING, Any, Dict, Optional, Type

from crewai.tools import BaseTool
from pydantic import BaseModel, ConfigDict, Field, PrivateAttr

if TYPE_CHECKING:
    from firecrawl import FirecrawlApp


try:
    from firecrawl import FirecrawlApp

    FIRECRAWL_AVAILABLE = True
except ImportError:
    FIRECRAWL_AVAILABLE = False


class FirecrawlSearchToolSchema(BaseModel):
    query: str = Field(description="Search query")
    limit: Optional[int] = Field(
        default=5, description="Maximum number of results to return"
    )
    tbs: Optional[str] = Field(default=None, description="Time-based search parameter")
    lang: Optional[str] = Field(
        default="en", description="Language code for search results"
    )
    country: Optional[str] = Field(
        default="us", description="Country code for search results"
    )
    location: Optional[str] = Field(
        default=None, description="Location parameter for search results"
    )
    timeout: Optional[int] = Field(default=60000, description="Timeout in milliseconds")
    scrape_options: Optional[Dict[str, Any]] = Field(
        default=None, description="Options for scraping search results"
    )


class FirecrawlSearchTool(BaseTool):
    model_config = ConfigDict(
        arbitrary_types_allowed=True, validate_assignment=True, frozen=False
    )
    model_config = ConfigDict(
        arbitrary_types_allowed=True, validate_assignment=True, frozen=False
    )
    name: str = "Firecrawl web search tool"
    description: str = "Search webpages using Firecrawl and return the results"
    args_schema: Type[BaseModel] = FirecrawlSearchToolSchema
    api_key: Optional[str] = None
    _firecrawl: Optional["FirecrawlApp"] = PrivateAttr(None)

    def __init__(self, api_key: Optional[str] = None, **kwargs):
        super().__init__(**kwargs)
        self.api_key = api_key
        self._initialize_firecrawl()

    def _initialize_firecrawl(self) -> None:
        try:
            if FIRECRAWL_AVAILABLE:
                self._firecrawl = FirecrawlApp(api_key=self.api_key)
            else:
                raise ImportError
        except ImportError:
            import click

            if click.confirm(
                "You are missing the 'firecrawl-py' package. Would you like to install it?"
            ):
                import subprocess

                try:
                    subprocess.run(["uv", "add", "firecrawl-py"], check=True)
                    from firecrawl import FirecrawlApp

                    self.firecrawl = FirecrawlApp(api_key=self.api_key)
                except subprocess.CalledProcessError:
                    raise ImportError("Failed to install firecrawl-py package")
            else:
                raise ImportError(
                    "`firecrawl-py` package not found, please run `uv add firecrawl-py`"
                )

    def _run(
        self,
        query: str,
        limit: Optional[int] = 5,
        tbs: Optional[str] = None,
        lang: Optional[str] = "en",
        country: Optional[str] = "us",
        location: Optional[str] = None,
        timeout: Optional[int] = 60000,
        scrape_options: Optional[Dict[str, Any]] = None,
    ) -> Any:
        if not self.firecrawl:
            raise RuntimeError("FirecrawlApp not properly initialized")

        options = {
            "limit": limit,
            "tbs": tbs,
            "lang": lang,
            "country": country,
            "location": location,
            "timeout": timeout,
            "scrapeOptions": scrape_options or {},
        }
        return self.firecrawl.search(**options)


try:
    from firecrawl import FirecrawlApp  # type: ignore

    # Only rebuild if the class hasn't been initialized yet
    if not hasattr(FirecrawlSearchTool, "_model_rebuilt"):
        FirecrawlSearchTool.model_rebuild()
        FirecrawlSearchTool._model_rebuilt = True
except ImportError:
    """
    When this tool is not used, then exception can be ignored.
    """
    pass
```

## File: crewAIInc_crewAI-tools_crewai_tools_tools_firecrawl_search_tool/source.json (Size: 0.30 KB)

```
{
  "url": "https://github.com/crewAIInc/crewAI-tools/tree/main/crewai_tools/tools/firecrawl_search_tool",
  "owner": "crewAIInc",
  "repo": "crewAI-tools",
  "branch": "main",
  "path": "crewai_tools/tools/firecrawl_search_tool",
  "content_type": "DIRECTORY",
  "download_time": "2025-02-25 01:16:34"
}
```

## File: crewAIInc_crewAI-tools_crewai_tools_tools_website_search/README.md (Size: 2.15 KB)

```
# WebsiteSearchTool

## Description
This tool is specifically crafted for conducting semantic searches within the content of a particular website. Leveraging a Retrieval-Augmented Generation (RAG) model, it navigates through the information provided on a given URL. Users have the flexibility to either initiate a search across any website known or discovered during its usage or to concentrate the search on a predefined, specific website.

## Installation
Install the crewai_tools package by executing the following command in your terminal:

```shell
pip install 'crewai[tools]'
```

## Example
To utilize the WebsiteSearchTool for different use cases, follow these examples:

```python
from crewai_tools import WebsiteSearchTool

# To enable the tool to search any website the agent comes across or learns about during its operation
tool = WebsiteSearchTool()

# OR

# To restrict the tool to only search within the content of a specific website.
tool = WebsiteSearchTool(website='https://example.com')
```

## Arguments
- `website` : An optional argument that specifies the valid website URL to perform the search on. This becomes necessary if the tool is initialized without a specific website. In the `WebsiteSearchToolSchema`, this argument is mandatory. However, in the `FixedWebsiteSearchToolSchema`, it becomes optional if a website is provided during the tool's initialization, as it will then only search within the predefined website's content.

## Custom model and embeddings

By default, the tool uses OpenAI for both embeddings and summarization. To customize the model, you can use a config dictionary as follows:

```python
tool = WebsiteSearchTool(
    config=dict(
        llm=dict(
            provider="ollama", # or google, openai, anthropic, llama2, ...
            config=dict(
                model="llama2",
                # temperature=0.5,
                # top_p=1,
                # stream=true,
            ),
        ),
        embedder=dict(
            provider="google",
            config=dict(
                model="models/embedding-001",
                task_type="retrieval_document",
                # title="Embeddings",
            ),
        ),
    )
)
```
```

## File: crewAIInc_crewAI-tools_crewai_tools_tools_website_search/source.json (Size: 0.28 KB)

```
{
  "url": "https://github.com/crewAIInc/crewAI-tools/tree/main/crewai_tools/tools/website_search",
  "owner": "crewAIInc",
  "repo": "crewAI-tools",
  "branch": "main",
  "path": "crewai_tools/tools/website_search",
  "content_type": "DIRECTORY",
  "download_time": "2025-02-25 01:16:35"
}
```

## File: crewAIInc_crewAI-tools_crewai_tools_tools_website_search/website_search_tool.py (Size: 1.68 KB)

```
from typing import Any, Optional, Type

from embedchain.models.data_type import DataType
from pydantic import BaseModel, Field

from ..rag.rag_tool import RagTool


class FixedWebsiteSearchToolSchema(BaseModel):
    """Input for WebsiteSearchTool."""

    search_query: str = Field(
        ...,
        description="Mandatory search query you want to use to search a specific website",
    )


class WebsiteSearchToolSchema(FixedWebsiteSearchToolSchema):
    """Input for WebsiteSearchTool."""

    website: str = Field(
        ..., description="Mandatory valid website URL you want to search on"
    )


class WebsiteSearchTool(RagTool):
    name: str = "Search in a specific website"
    description: str = "A tool that can be used to semantic search a query from a specific URL content."
    args_schema: Type[BaseModel] = WebsiteSearchToolSchema

    def __init__(self, website: Optional[str] = None, **kwargs):
        super().__init__(**kwargs)
        if website is not None:
            kwargs["data_type"] = DataType.WEB_PAGE
            self.add(website)
            self.description = f"A tool that can be used to semantic search a query from {website} website content."
            self.args_schema = FixedWebsiteSearchToolSchema
            self._generate_description()

    def add(
        self,
        *args: Any,
        **kwargs: Any,
    ) -> None:
        super().add(*args, **kwargs)

    def _before_run(
        self,
        query: str,
        **kwargs: Any,
    ) -> Any:
        if "website" in kwargs:
            self.add(kwargs["website"])

    def _run(
        self,
        search_query: str,
        **kwargs: Any,
    ) -> Any:
        return super()._run(query=search_query, **kwargs)
```

## File: download_summary.json (Size: 8.42 KB)

```
[
  {
    "url": "https://github.com/47thommy/langgraph/blob/main/searching.py",
    "output_dir": "out/47thommy_langgraph_searching.py",
    "files_downloaded": 1,
    "files_failed": 0,
    "duration_seconds": 0.46,
    "error": null
  },
  {
    "url": "https://github.com/AI-secure/AgentPoison/blob/master/ReAct/search.py",
    "output_dir": "out/AI-secure_AgentPoison_ReAct_search.py",
    "files_downloaded": 1,
    "files_failed": 0,
    "duration_seconds": 0.51,
    "error": null
  },
  {
    "url": "https://github.com/bm611/aisearch/blob/main/search/backend/api.py",
    "output_dir": "out/bm611_aisearch_search_backend_api.py",
    "files_downloaded": 1,
    "files_failed": 0,
    "duration_seconds": 0.4,
    "error": null
  },
  {
    "url": "https://github.com/chenxingyuzealken/AIBootCampCourse/blob/main/llm/search.py",
    "output_dir": "out/chenxingyuzealken_AIBootCampCourse_llm_search.py",
    "files_downloaded": 1,
    "files_failed": 0,
    "duration_seconds": 0.33,
    "error": null
  },
  {
    "url": "https://github.com/cnunescoelho/kiroku/blob/main/agents/tools.py",
    "output_dir": "out/cnunescoelho_kiroku_agents_tools.py",
    "files_downloaded": 1,
    "files_failed": 0,
    "duration_seconds": 0.33,
    "error": null
  },
  {
    "url": "https://github.com/cohere-ai/cohere-toolkit/tree/main/src/backend/tools/brave_search",
    "output_dir": "out/cohere-ai_cohere-toolkit_src_backend_tools_brave_search",
    "files_downloaded": 3,
    "files_failed": 0,
    "duration_seconds": 1.39,
    "error": null
  },
  {
    "url": "https://github.com/crewAIInc/crewAI-tools/tree/main/crewai_tools/tools/brave_search_tool",
    "output_dir": "out/crewAIInc_crewAI-tools_crewai_tools_tools_brave_search_tool",
    "files_downloaded": 3,
    "files_failed": 0,
    "duration_seconds": 1.54,
    "error": null
  },
  {
    "url": "https://github.com/crewAIInc/crewAI-tools/tree/main/crewai_tools/tools/firecrawl_search_tool",
    "output_dir": "out/crewAIInc_crewAI-tools_crewai_tools_tools_firecrawl_search_tool",
    "files_downloaded": 2,
    "files_failed": 0,
    "duration_seconds": 1.1,
    "error": null
  },
  {
    "url": "https://github.com/crewAIInc/crewAI-tools/tree/main/crewai_tools/tools/website_search",
    "output_dir": "out/crewAIInc_crewAI-tools_crewai_tools_tools_website_search",
    "files_downloaded": 2,
    "files_failed": 0,
    "duration_seconds": 1.4,
    "error": null
  },
  {
    "url": "https://github.com/echohive42/AI-book-maker-with-perplexity-search-grounding/tree/master",
    "output_dir": "out/echohive42_AI-book-maker-with-perplexity-search-grounding",
    "files_downloaded": 2,
    "files_failed": 0,
    "duration_seconds": 4.41,
    "error": null
  },
  {
    "url": "https://github.com/EllAchE/llama-out-loud/blob/main/brave.py",
    "output_dir": "out/EllAchE_llama-out-loud_brave.py",
    "files_downloaded": 1,
    "files_failed": 0,
    "duration_seconds": 0.35,
    "error": null
  },
  {
    "url": "https://github.com/Hovup-FO/TKM-Groq-Connector/blob/main/agents.py",
    "output_dir": "out/Hovup-FO_TKM-Groq-Connector_agents.py",
    "files_downloaded": 1,
    "files_failed": 0,
    "duration_seconds": 0.52,
    "error": null
  },
  {
    "url": "https://github.com/jsonallen/perplexity-mcp/tree/master",
    "output_dir": "out/jsonallen_perplexity-mcp",
    "files_downloaded": 4,
    "files_failed": 0,
    "duration_seconds": 5.42,
    "error": null
  },
  {
    "url": "https://github.com/leemark/llamaindex_playbook/blob/main/examples/rag_web_search_brave.py",
    "output_dir": "out/leemark_llamaindex_playbook_examples_rag_web_search_brave.py",
    "files_downloaded": 1,
    "files_failed": 0,
    "duration_seconds": 0.32,
    "error": null
  },
  {
    "url": "https://github.com/morispolanco/leyesdeguatemala/blob/main/tavily.py",
    "output_dir": "out/morispolanco_leyesdeguatemala_tavily.py",
    "files_downloaded": 1,
    "files_failed": 0,
    "duration_seconds": 0.3,
    "error": null
  },
  {
    "url": "https://github.com/mshumer/OpenReasoningEngine/blob/main/tools.py",
    "output_dir": "out/mshumer_OpenReasoningEngine_tools.py",
    "files_downloaded": 1,
    "files_failed": 0,
    "duration_seconds": 0.29,
    "error": null
  },
  {
    "url": "https://github.com/niship2/news-flow/blob/main/tools/tavilysearch.py",
    "output_dir": "out/niship2_news-flow_tools_tavilysearch.py",
    "files_downloaded": 1,
    "files_failed": 0,
    "duration_seconds": 0.53,
    "error": null
  },
  {
    "url": "https://github.com/niship2/news-flow/blob/main/tools/youcom.py",
    "output_dir": "out/niship2_news-flow_tools_youcom.py",
    "files_downloaded": 1,
    "files_failed": 0,
    "duration_seconds": 0.3,
    "error": null
  },
  {
    "url": "https://github.com/niship2/newsapi2/blob/main/subs/youcomsearch.py",
    "output_dir": "out/niship2_newsapi2_subs_youcomsearch.py",
    "files_downloaded": 1,
    "files_failed": 0,
    "duration_seconds": 0.41,
    "error": null
  },
  {
    "url": "https://github.com/oborys/security-ai-agent-brama/blob/main/braveSearch.py",
    "output_dir": "out/oborys_security-ai-agent-brama_braveSearch.py",
    "files_downloaded": 1,
    "files_failed": 0,
    "duration_seconds": 0.51,
    "error": null
  },
  {
    "url": "https://github.com/pengfeng/ask.py/tree/master",
    "output_dir": "out/pengfeng_ask.py",
    "files_downloaded": 4,
    "files_failed": 0,
    "duration_seconds": 5.03,
    "error": null
  },
  {
    "url": "https://github.com/RMNCLDYO/perplexity-ai-toolkit/tree/master",
    "output_dir": "out/RMNCLDYO_perplexity-ai-toolkit",
    "files_downloaded": 4,
    "files_failed": 0,
    "duration_seconds": 4.32,
    "error": null
  },
  {
    "url": "https://github.com/rsrohan99/Llama-Researcher/blob/master/tavily.py",
    "output_dir": "out/rsrohan99_Llama-Researcher_tavily.py",
    "files_downloaded": 1,
    "files_failed": 0,
    "duration_seconds": 0.48,
    "error": null
  },
  {
    "url": "https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/tools/llama-index-tools-bing-search",
    "output_dir": "out/run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-bing-search",
    "files_downloaded": 13,
    "files_failed": 0,
    "duration_seconds": 5.43,
    "error": null
  },
  {
    "url": "https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/tools/llama-index-tools-brave-search",
    "output_dir": "out/run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-brave-search",
    "files_downloaded": 12,
    "files_failed": 0,
    "duration_seconds": 4.46,
    "error": null
  },
  {
    "url": "https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/tools/llama-index-tools-linkup-research",
    "output_dir": "out/run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-linkup-research",
    "files_downloaded": 13,
    "files_failed": 0,
    "duration_seconds": 4.19,
    "error": null
  },
  {
    "url": "https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/tools/llama-index-tools-tavily-research",
    "output_dir": "out/run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-tavily-research",
    "files_downloaded": 14,
    "files_failed": 0,
    "duration_seconds": 4.14,
    "error": null
  },
  {
    "url": "https://github.com/ShivadityaKr/BraveAI-Scraper/tree/master",
    "output_dir": "out/ShivadityaKr_BraveAI-Scraper",
    "files_downloaded": 3,
    "files_failed": 0,
    "duration_seconds": 4.31,
    "error": null
  },
  {
    "url": "https://github.com/ssiddhantsood/thalamus-backend-hackmit/blob/main/toolkitutils/search_toolkit.py",
    "output_dir": "out/ssiddhantsood_thalamus-backend-hackmit_toolkitutils_search_toolkit.py",
    "files_downloaded": 1,
    "files_failed": 0,
    "duration_seconds": 0.51,
    "error": null
  },
  {
    "url": "https://github.com/steven-haddix/rezai/tree/master",
    "output_dir": "out/steven-haddix_rezai",
    "files_downloaded": 3,
    "files_failed": 0,
    "duration_seconds": 4.75,
    "error": null
  },
  {
    "url": "https://github.com/steven-haddix/rezai/blob/main/rezai/services/youcom/service.py",
    "output_dir": "out/steven-haddix_rezai_rezai_services_youcom_service.py",
    "files_downloaded": 1,
    "files_failed": 0,
    "duration_seconds": 0.42,
    "error": null
  },
  {
    "url": "https://github.com/tom-doerr/perplexity_search/tree/master",
    "output_dir": "out/tom-doerr_perplexity_search",
    "files_downloaded": 5,
    "files_failed": 0,
    "duration_seconds": 4.49,
    "error": null
  }
]
```

## File: echohive42_AI-book-maker-with-perplexity-search-grounding/README.md (Size: 5.56 KB)

```
# 📚 AI Book Maker

An intelligent book creation tool that uses GPT-4o and Perplexity AI to automatically generate well-researched, structured books on any topic.

## 🌟 Features

- **Automated Book Outline Generation**: Creates detailed chapter structures with research questions
- **Parallel Research Processing**: Efficiently researches multiple chapters simultaneously
- **Smart Chapter Writing**: Converts research into coherent, engaging chapters
- **Real-time Progress Updates**: Colorful console output showing progress
- **Beautiful Markdown Output**: Final book is formatted in clean, readable markdown

## ❤️ Support & Get 400+ AI Projects

This is one of 400+ fascinating projects in my collection! **[Support me on Patreon](https://www.patreon.com/c/echohive42/membership)** to get:
- 🎯 Access to 400+ AI projects (and growing daily!)
- 📥 Full source code & detailed explanations
- 📚 1000x Cursor Course
- 🎓 Live coding sessions & AMAs
- 💬 1-on-1 consultations (higher tiers)
- 🎁 Exclusive discounts on AI tools

## 🔧 Prerequisites

- Python 3.7+
- OpenAI API key
- Perplexity API key

## ⚡ Quick Start

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd book-maker
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Set up environment variables**
   
   Windows:
   ```cmd
   set OPENAI_API_KEY=your-openai-key
   set PERPLEXITY_API_KEY=your-perplexity-key
   ```
   
   Linux/Mac:
   ```bash
   export OPENAI_API_KEY=your-openai-key
   export PERPLEXITY_API_KEY=your-perplexity-key
   ```

4. **Run the book maker**
   ```bash
   python book_maker.py
   ```

## 🎯 Detailed Process Flow

1. **Book Outline Generation**
   - GPT-4O receives your topic and generates a structured outline
   - Returns a JSON object containing:
     ```json
     {
       "title": "Book Title",
       "chapters": [
         {
           "title": "Chapter Title",
           "description": "Brief chapter overview",
           "research_questions": [
             "Specific question 1?",
             "Specific question 2?",
             "..."
           ]
         }
       ]
     }
     ```

2. **Research Process**
   - Questions for each chapter are processed in parallel
   - For each chapter:
     ```python
     research_data = await perplexity_client.research(questions)
     ```
   - Batching system:
     - Processes 10 chapters simultaneously (configurable)
     - Implements rate limiting (1 second between batches)
     - Handles API throttling gracefully

3. **Chapter Writing**
   - Research data is fed to GPT-4O-Mini
   - Each chapter is written sequentially to maintain narrative flow
   - Real-time updates:
     ```markdown
     🎯 Generating outline...
     📚 Researching Chapter 1...
     ✍️ Writing Chapter 1...
     📝 Updating book.md...
     ```

4. **Output Generation**
   - Markdown file is updated after each chapter
   - Progress is visible in real-time
   - Colored console output shows status

## 📋 Output Format

The generated book will be saved in markdown format with:
- Book title
- Numbered chapters
- Well-formatted content
- Clean hierarchy

Example structure:
```markdown
# Book Title

## Chapter 1: Chapter Title
[Chapter content...]

## Chapter 2: Chapter Title
[Chapter content...]
```

## ⚙️ Configuration

Key constants in `book_maker.py`:
- `MAX_PARALLEL_REQUESTS`: Number of concurrent research requests (default: 10)
- `SLEEP_BETWEEN_REQUESTS`: Delay between batch requests in seconds (default: 1)
- `OUTPUT_FILE`: Output file name (default: "book.md")

## 🚨 Error Handling

The script includes comprehensive error handling:
- API connection issues
- File writing errors
- JSON parsing errors
- All errors are displayed with descriptive colored messages

## 📝 Notes

- The quality of the book depends on the API response quality
- Longer books will take more time to generate
- Keep your API keys secure and never commit them to version control
- Research questions are automatically generated based on chapter context
- Each chapter typically generates 3-5 focused research questions
- Research data is processed and synthesized before writing

## 🔒 API Key Security

Always use environment variables for API keys. Never hardcode them in the script. 

## 🤝 Support me & Premium Access

If you find this tool useful, consider supporting me on [Patreon](https://www.patreon.com/c/echohive42/membership). Your support helps maintain and improve this and other AI tools!

### 🌟 Patreon Tiers

1. **AI Connoisseur ($30/month)**
   - Download code for most projects
   - Access to 1000x Cursor Course
   - Email support for simple questions
   - Access to exclusive videos and blog posts

2. **AI Architect ($50/month)**
   - **Full code access** to all projects
   - Access to 1000x Cursor Course
   - Exclusive AMA and live coding meetings
   - Free usage of echohive webapps
   - All benefits from previous tier

3. **AI Virtuoso 1-on-1 ($150/month)**
   - One hour monthly 1-on-1 consultation
   - Personal coding help and guidance
   - Direct project support
   - All benefits from previous tiers
   - Limited availability (1 spot remaining)

4. **AI Prodigy 1-on-1 ($450/month)**
   - Three hours monthly 1-on-1 consultation
   - Intensive personal tutoring
   - Comprehensive project support
   - All benefits from previous tiers
   - Currently sold out

Join our community and accelerate your AI development journey! 🚀
```

## File: echohive42_AI-book-maker-with-perplexity-search-grounding/requirements.txt (Size: 0.03 KB)

```
openai
termcolor
markdown
```

## File: echohive42_AI-book-maker-with-perplexity-search-grounding/source.json (Size: 0.29 KB)

```
{
  "url": "https://github.com/echohive42/AI-book-maker-with-perplexity-search-grounding/tree/master",
  "owner": "echohive42",
  "repo": "AI-book-maker-with-perplexity-search-grounding",
  "branch": "master",
  "path": "",
  "content_type": "REPOSITORY",
  "download_time": "2025-02-25 01:16:37"
}
```

## File: jsonallen_perplexity-mcp/.gitignore (Size: 0.11 KB)

```
# Python-generated files
__pycache__/
*.py[oc]
build/
dist/
wheels/
*.egg-info

# Virtual environments
.venv
```

## File: jsonallen_perplexity-mcp/LICENSE (Size: 1.04 KB)

```
MIT License

Copyright (c) 2024 Jason Allen

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

## File: jsonallen_perplexity-mcp/README.md (Size: 3.11 KB)

```
# perplexity-mcp MCP server

[![smithery badge](https://smithery.ai/badge/perplexity-mcp)](https://smithery.ai/server/perplexity-mcp)

A Model Context Protocol (MCP) server that provides web search functionality using [Perplexity AI's](https://www.perplexity.ai/) API.  Works with the [Anthropic](https://www.anthropic.com/news/model-context-protocol) Claude desktop client. 

## Example
Let's you use prompts like,  "Search the web to find out what's new at Anthropic in the past week."

## Glama Scores
<a href="https://glama.ai/mcp/servers/ebg0za4hn9"><img width="380" height="200" src="https://glama.ai/mcp/servers/ebg0za4hn9/badge" alt="Perplexity Server MCP server" /></a>

## Components

### Prompts

The server provides a single prompt:
- perplexity_search_web: Search the web using Perplexity AI
  - Required "query" argument for the search query
  - Optional "recency" argument to filter results by time period:
    - 'day': last 24 hours
    - 'week': last 7 days
    - 'month': last 30 days (default)
    - 'year': last 365 days
  - Uses Perplexity's API to perform web searches

### Tools

The server implements one tool:
- perplexity_search_web: Search the web using Perplexity AI
  - Takes "query" as a required string argument
  - Optional "recency" parameter to filter results (day/week/month/year)
  - Returns search results from Perplexity's API

## Installation

### Installing via Smithery

To install Perplexity MCP for Claude Desktop automatically via [Smithery](https://smithery.ai/server/perplexity-mcp):

```bash
npx -y @smithery/cli install perplexity-mcp --client claude
```

### Requires [UV](https://github.com/astral-sh/uv) (Fast Python package and project manager)

If uv isn't installed. 

```bash 
# Using Homebrew on macOS 
brew install uv
```

or

```bash
# On macOS and Linux.
curl -LsSf https://astral.sh/uv/install.sh | sh

# On Windows.
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
```


Next, install the MCP server

```bash
# Install from PyPi
uv pip install perplexity-mcp
```
or 

```bash
# Install from source
uv pip install git+https://github.com/jsonallen/perplexity-mcp.git
```

### Environment Variables

The following environment variable is required in your claude_desktop_config.json. You can obtain an API key from [Perplexity](https://perplexity.ai)

- `PERPLEXITY_API_KEY`: Your Perplexity AI API key


#### Claude Desktop

Add this tool as a mcp server by editing the Claude config file.

On MacOS: `~/Library/Application\ Support/Claude/claude_desktop_config.json`
On Windows: `%APPDATA%/Claude/claude_desktop_config.json`


  ```json
    "perplexity-mcp": {
      "env": {
        "PERPLEXITY_API_KEY": "XXXXXXXXXXXXXXXXXXXX"
      },
      "command": "uv",
      "args": [
        "run",
        "perplexity-mcp"
      ]
    }
  ```
  To verify the server is working.  Open the Claude client and use a prompt like "search the web for news about openai in the past week".  You should see an alert box open to confirm tool usage. Click "Allow for this chat".
  
  <img width="800" alt="mcp_screenshot" src="https://github.com/user-attachments/assets/922d8f6a-8c9a-4978-8be6-788e70b4d049" />
```

## File: jsonallen_perplexity-mcp/pyproject.toml (Size: 0.46 KB)

```
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "perplexity-mcp"
version = "0.1.3"
requires-python = ">=3.11"
readme = "README.md"
description = "Perplexity MCP Server"
dependencies = [
    "aiohttp",
    "pydantic",
    "mcp>=1.0.2",
]

[project.scripts]
perplexity-mcp = "perplexity_mcp.server:cli"

[tool.hatch.build.targets.wheel]
packages = ["src/perplexity_mcp"]

[tool.hatch.metadata]
allow-direct-references = true
```

## File: jsonallen_perplexity-mcp/source.json (Size: 0.23 KB)

```
{
  "url": "https://github.com/jsonallen/perplexity-mcp/tree/master",
  "owner": "jsonallen",
  "repo": "perplexity-mcp",
  "branch": "master",
  "path": "",
  "content_type": "REPOSITORY",
  "download_time": "2025-02-25 01:16:42"
}
```

## File: leemark_llamaindex_playbook_examples_rag_web_search_brave.py/rag_web_search_brave.py (Size: 3.22 KB)

```
import os
import json
import logging
import sys
import requests
from dotenv import load_dotenv
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from llama_index.core import VectorStoreIndex, Document
from llama_index.tools.brave_search import BraveSearchToolSpec
from llama_index.readers.web import SimpleWebPageReader

# Constants
USER_AGENT = 'Mozilla/5.0 (compatible; YourBot/1.0; +http://yourwebsite.com/bot.html)'
HEADERS = {'User-Agent': USER_AGENT}
RETRIES = Retry(total=5, backoff_factor=0.1, status_forcelist=[500, 502, 503, 504])

def setup_logging():
    """
    Initialize logging configuration to output logs to stdout.
    """
    logging.basicConfig(stream=sys.stdout, level=logging.INFO)
    logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))

def load_environment_variables():
    """
    Load environment variables from the .env file.
    :return: The Brave API key.
    """
    load_dotenv()
    return os.getenv('BRAVE_API_KEY')

def perform_search(query, api_key):
    """
    Perform a search using the Brave Search API.
    :param query: The search query.
    :param api_key: The Brave API key.
    :return: The search response.
    """
    tool_spec = BraveSearchToolSpec(api_key=api_key)
    return tool_spec.brave_search(query=query)

def extract_search_results(response):
    """
    Extract search results from the Brave Search API response.
    :param response: The search response.
    :return: A list of search results.
    """
    documents = [doc.text for doc in response]
    search_results = []
    for document in documents:
        response_data = json.loads(document)
        search_results.extend(response_data.get('web', {}).get('results', []))
    return search_results

def scrape_web_pages(search_results):
    """
    Scrape web pages from the URLs obtained from the search results.
    :param search_results: The list of search results.
    :return: A list of scraped documents.
    """
    session = requests.Session()
    session.mount('http://', HTTPAdapter(max_retries=RETRIES))
    session.mount('https://', HTTPAdapter(max_retries=RETRIES))
    all_documents = []

    for result in search_results:
        url = result.get('url')
        try:
            response = session.get(url, headers=HEADERS, timeout=10)
            response.raise_for_status()
            doc = Document(text=response.text, url=url)
            all_documents.append(doc)
        except requests.exceptions.RequestException as e:
            logging.error(f"Failed to scrape {url}: {e}")

    return all_documents

def main():
    """
    Main function to orchestrate the search, scraping, and querying process.
    """
    setup_logging()
    api_key = load_environment_variables()
    my_query = "What is the latest news about llamaindex?"
    response = perform_search(my_query, api_key)
    search_results = extract_search_results(response)
    all_documents = scrape_web_pages(search_results)
    
    # Load all the scraped documents into the vector store
    index = VectorStoreIndex.from_documents(all_documents)
    
    # Use the index to query with the language model
    query_engine = index.as_query_engine()
    response = query_engine.query(my_query)
    print(response)

if __name__ == "__main__":
    main()
```

## File: leemark_llamaindex_playbook_examples_rag_web_search_brave.py/source.json (Size: 0.29 KB)

```
{
  "url": "https://github.com/leemark/llamaindex_playbook/blob/main/examples/rag_web_search_brave.py",
  "owner": "leemark",
  "repo": "llamaindex_playbook",
  "branch": "main",
  "path": "examples/rag_web_search_brave.py",
  "content_type": "FILE",
  "download_time": "2025-02-25 01:16:47"
}
```

## File: morispolanco_leyesdeguatemala_tavily.py/source.json (Size: 0.25 KB)

```
{
  "url": "https://github.com/morispolanco/leyesdeguatemala/blob/main/tavily.py",
  "owner": "morispolanco",
  "repo": "leyesdeguatemala",
  "branch": "main",
  "path": "tavily.py",
  "content_type": "FILE",
  "download_time": "2025-02-25 01:16:48"
}
```

## File: morispolanco_leyesdeguatemala_tavily.py/tavily.py (Size: 4.16 KB)

```
import streamlit as st
import requests
import json
from docx import Document
from docx.shared import Inches
from io import BytesIO

# Configuración de la página
st.set_page_config(page_title="Asistente Legal de Guatemala", page_icon="🇬🇹")

# Título de la aplicación
st.title("Asistente Legal de Guatemala")

# Acceder a las claves de API de los secretos de Streamlit
TOGETHER_API_KEY = st.secrets["TOGETHER_API_KEY"]
TAVILY_API_KEY = st.secrets["TAVILY_API_KEY"]

def buscar_informacion(query):
    url = "https://api.tavily.ai/v1/search"
    payload = json.dumps({
        "query": query + " ley Guatemala"
    })
    headers = {
        'Authorization': f'Bearer {TAVILY_API_KEY}',
        'Content-Type': 'application/json'
    }
    response = requests.request("POST", url, headers=headers, data=payload)
    return response.json()

def generar_respuesta(prompt, contexto):
    url = "https://api.together.xyz/inference"
    payload = json.dumps({
        "model": "mistralai/Mixtral-8x7B-Instruct-v0.1",
        "prompt": f"Contexto: {contexto}\n\nPregunta: {prompt}\n\nResponde la pregunta basándote en el contexto proporcionado y tu conocimiento general sobre las leyes de Guatemala. Si no tienes suficiente información, indica que no puedes responder con certeza.\n\nRespuesta:",
        "max_tokens": 5512,
        "temperature": 0.7,
        "top_p": 0.7,
        "top_k": 50,
        "repetition_penalty": 1,
        "stop": ["Pregunta:"]
    })
    headers = {
        'Authorization': f'Bearer {TOGETHER_API_KEY}',
        'Content-Type': 'application/json'
    }
    response = requests.request("POST", url, headers=headers, data=payload)
    return response.json()['output']['choices'][0]['text'].strip()

def create_docx(pregunta, respuesta, fuentes):
    doc = Document()
    doc.add_heading('Asistente Legal de Guatemala', 0)

    doc.add_heading('Pregunta', level=1)
    doc.add_paragraph(pregunta)

    doc.add_heading('Respuesta', level=1)
    doc.add_paragraph(respuesta)

    doc.add_heading('Fuentes', level=1)
    for fuente in fuentes:
        doc.add_paragraph(fuente, style='List Bullet')

    doc.add_paragraph('\nNota: Este documento fue generado por un asistente de IA. Verifica la información con fuentes oficiales o un abogado para asuntos legales importantes.')

    return doc

# Interfaz de usuario
pregunta = st.text_input("Ingresa tu pregunta sobre la ley de Guatemala:")

if st.button("Obtener respuesta"):
    if pregunta:
        with st.spinner("Buscando información y generando respuesta..."):
            # Buscar información relevante
            resultados_busqueda = buscar_informacion(pregunta)
            contexto = "\n".join([result.get('snippet', '') for result in resultados_busqueda.get('results', [])])

            # Generar respuesta
            respuesta = generar_respuesta(pregunta, contexto)

            # Mostrar respuesta
            st.write("Respuesta:")
            st.write(respuesta)

            # Mostrar fuentes
            st.write("Fuentes:")
            fuentes = []
            for resultado in resultados_busqueda.get('results', [])[:3]:
                fuente = f"{resultado['title']}: {resultado['link']}"
                st.write(f"- [{resultado['title']}]({resultado['link']})")
                fuentes.append(fuente)

            # Crear documento DOCX
            doc = create_docx(pregunta, respuesta, fuentes)

            # Guardar el documento DOCX en memoria
            docx_file = BytesIO()
            doc.save(docx_file)
            docx_file.seek(0)

            # Opción para exportar a DOCX
            st.download_button(
                label="Descargar resultados como DOCX",
                data=docx_file,
                file_name="respuesta_legal_guatemala.docx",
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            )

    else:
        st.warning("Por favor, ingresa una pregunta.")

# Agregar información en el pie de página
st.markdown("---")
st.markdown("**Nota:** Este asistente utiliza IA para generar respuestas basadas en información disponible en línea. "
            "Siempre verifica la información con fuentes oficiales o un abogado para asuntos legales importantes.")
```

## File: mshumer_OpenReasoningEngine_tools.py/source.json (Size: 0.24 KB)

```
{
  "url": "https://github.com/mshumer/OpenReasoningEngine/blob/main/tools.py",
  "owner": "mshumer",
  "repo": "OpenReasoningEngine",
  "branch": "main",
  "path": "tools.py",
  "content_type": "FILE",
  "download_time": "2025-02-25 01:16:48"
}
```

## File: mshumer_OpenReasoningEngine_tools.py/tools.py (Size: 9.06 KB)

```
import os
import requests
from typing import Dict, Any, List, Union, Optional
import sys
from io import StringIO
import traceback
from contextlib import redirect_stdout, redirect_stderr
import json
import wolframalpha
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sympy
import scipy
import sklearn
from sympy import symbols, solve, simplify
from scipy import stats
from sklearn import preprocessing
import math
from e2b_code_interpreter import Sandbox
from serpapi import GoogleSearch
from dotenv import load_dotenv
from urllib.parse import quote
load_dotenv()

serpapi_api_key = os.environ.get("SERPAPI_API_KEY")

# Dictionary of interpreter states, keyed by task hash
interpreter_states = {}

def get_task_hash(task: str) -> str:
    """Generate a unique hash for a task."""
    import hashlib
    return hashlib.md5(task.encode()).hexdigest()

def clear_interpreter_state(task: str = None):
    """
    Clear the interpreter state.
    If task is provided, only clear that task's state.
    If no task is provided, clear all states.
    """
    global interpreter_states
    if task:
        task_hash = get_task_hash(task)
        if task_hash in interpreter_states:
            del interpreter_states[task_hash]
    else:
        interpreter_states = {}

def python_interpreter(code: str, task: str, timeout: int = 10, sandbox: Optional[Sandbox] = None) -> str:
    """
    Safely execute Python code in a restricted environment.
    Maintains separate state for each task.
    """
    if sandbox is None:
        raise ValueError("E2B Sandbox is required for Python code execution but none was provided.")

    print(f"Executing code:\n{code}")
    execution = sandbox.run_code(
        code,
        # timeout=timeout, # Timeout to wait for the whole request to complete
        on_stdout=lambda x: print('[stdout]', x),
        on_stderr=lambda x: print('[stderr]', x)
    )

    if execution.error:
        e = execution.error

        error_msg = (
            f"Error executing code: {e.value}\n"
            f"Error type: {type(e.name)}\n"
            f"Traceback:\n{e.traceback}\n"
            "\nDebugging Suggestions:\n"
            "1. Add print statements to debug the issue\n"
            "2. Use assertions to validate inputs and outputs\n"
            "3. Check variable types with print(type(var))\n"
            "4. For numerical computations, verify inputs are numbers\n"
            "5. For symbolic math, ensure variables are properly defined with symbols()\n"
            "\nNote: Plotting is currently not supported. Instead of visualizing data, consider:\n"
            "1. Printing key numerical results\n"
            "2. Showing data statistics\n"
            "3. Printing array slices or samples\n"
            "\nAvailable packages:\n"
            "- numpy (np): Numerical computing\n"
            "- pandas (pd): Data manipulation\n"
            "- scipy: Scientific computing\n"
            "- sklearn: Machine learning"
        )
        return error_msg

    result = []

    # Results are the output of the code execution besides stdout and stderr
    # Can be text, PNG, JPG, JSON, html, markdown, etc.
    # Results are based on executing code inside the headless Jupyter notebook
    # that's running inside the sandbox.
    # The same way, you'd get result from a Jupyter notebook cell, you get results here.
    # That means any display() calls in the code will be captured as a result,
    # and also the last expression in the code, if there is one.
    code_exec_results = execution.results
    for ce_result in code_exec_results:
        print(ce_result.formats()) # Raw data of results
        # if 'png' in ce_result.formats:
            # Handle PNG images
        # if 'json' in ce_result.formats:
            # Handle JSON
        # ...
        #
        # Text is always present for every result.
        result.append(ce_result.text)

    stdout = execution.logs.stdout
    stderr = execution.logs.stderr
    if stdout:
        result.append(f"Output:\n{''.join(stdout)}")
    if stderr:
        result.append(f"Errors:\n{''.join(stderr)}")
    return "\n\n".join(result) if result else "Code executed successfully with no output."

def find_datapoint_on_web(
    query: str,
    api_key: str = None,
) -> str:
    """
    Perform web search using SERPAPI Google Search.

    Args:
        query: The specific search query
        api_key: API key for SERPAPI
        api_url: Not used for SERPAPI

    Returns:
        str: Search results with citations
    """
    try:
        # Configure the search
        search = GoogleSearch({
            "q": query,
            "api_key": api_key,
            "num": 5  # Get top 5 results
        })
        
        # Get the results
        results = search.get_dict()
        
        if "error" in results:
            return f"Error performing search: {results['error']}"
            
        # Format organic results
        formatted_results = []
        
        if "organic_results" in results:
            for result in results["organic_results"]:
                title = result.get("title", "No title")
                snippet = result.get("snippet", "No description available")
                link = result.get("link", "No link available")
                formatted_results.append(f"Source: {title}\nSummary: {snippet}\nURL: {link}\n")
                
        if formatted_results:
            return "\n".join(formatted_results)
        else:
            return "No relevant results found for the query."
            
    except Exception as e:
        return f"Error performing web search: {str(e)}"

def wolfram(
    query: str,
    wolfram_app_id: str,
    include_pods: List[str] = None,  # e.g., ["Result", "Solution", "Plot"]
    max_width: int = 1000
) -> str:
    """
    Query Wolfram Alpha for computations, math, science, and knowledge.

    Args:
        query: The query to send to Wolfram Alpha
        wolfram_app_id: Your Wolfram Alpha API key
        include_pods: List of pod names to include in result (None for all)
        max_width: Maximum width for plots/images

    Returns:
        str: Formatted response from Wolfram Alpha
    """
    try:
        client = wolframalpha.Client(wolfram_app_id)
        res = client.query(query, width=max_width)

        # Format the response
        result = []
        for pod in res.pods:
            # Skip if we're only interested in specific pods and this isn't one of them
            if include_pods and pod.title not in include_pods:
                continue

            if pod.title and pod.text:
                result.append(f"{pod.title}:\n{pod.text}")

        return "\n\n".join(result) if result else "No results found"

    except Exception as e:
        return f"Error querying Wolfram Alpha: {str(e)}"

def get_webpage_content(url: str, jina_api_key: str = None) -> str:
    """
    Retrieve webpage content using Jina API.
    
    Args:
        url: The webpage URL to fetch content from
        jina_api_key: Jina API key for authentication
        
    Returns:
        str: The webpage content or error message
    """
    if not jina_api_key:
        return "Error: Jina API key not provided"
        
    try:
        # URL encode the target URL and prepend Jina API endpoint
        encoded_url = quote(url, safe='')
        jina_url = f'https://r.jina.ai/{encoded_url}'
        
        headers = {
            'Authorization': f'Bearer {jina_api_key}'
        }
        
        response = requests.get(jina_url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            return response.text
        else:
            return f"Failed to retrieve content. Status code: {response.status_code}"
            
    except requests.RequestException as e:
        return f"Error fetching webpage content: {str(e)}"

def execute_tool(
    tool_name: str,
    parameters: Dict[str, Any],
    task: str = None,
    api_key: str = None,
    model: str = None,
    api_url: str = None,
    wolfram_app_id: str = None,
    sandbox: Optional[Sandbox] = None,
    jina_api_key: str = None
) -> Any:
    """Execute the specified tool with the given parameters."""
    tools = {
        "python": python_interpreter,
        "find_datapoint_on_web": find_datapoint_on_web,
        "wolfram": wolfram,
    }
    
    # Only add get_webpage_content tool if Jina API key is provided
    if jina_api_key:
        tools["get_webpage_content"] = get_webpage_content

    if tool_name not in tools:
        raise ValueError(f"Unknown tool: {tool_name}")

    tool_func = tools[tool_name]

    # Remove thread_id from parameters if it exists
    if 'thread_id' in parameters:
        del parameters['thread_id']

    # Inject appropriate credentials and task
    if tool_name == "python":
        parameters = {**parameters, "task": task, "sandbox": sandbox}
    elif tool_name == "find_datapoint_on_web":
        parameters = {**parameters, "api_key": serpapi_api_key}
    elif tool_name == "wolfram":
        parameters = {**parameters, "wolfram_app_id": wolfram_app_id}
    elif tool_name == "get_webpage_content":
        parameters = {**parameters, "jina_api_key": jina_api_key}

    return tool_func(**parameters)
```

## File: niship2_news-flow_tools_tavilysearch.py/source.json (Size: 0.25 KB)

```
{
  "url": "https://github.com/niship2/news-flow/blob/main/tools/tavilysearch.py",
  "owner": "niship2",
  "repo": "news-flow",
  "branch": "main",
  "path": "tools/tavilysearch.py",
  "content_type": "FILE",
  "download_time": "2025-02-25 01:16:48"
}
```

## File: niship2_news-flow_tools_tavilysearch.py/tavilysearch.py (Size: 0.80 KB)

```
from langchain_community.tools.tavily_search import TavilySearchResults
import streamlit as st


def search_tavily(state):
    
    """ Retrieve docs from web search """

    try:

        # Search
        tavily_search = TavilySearchResults(max_results=20)
        search_docs = tavily_search.invoke(state['question'] + " news")

        # Format
        formatted_search_docs = "\n\n---\n\n".join(
            [
                f'<Document href="{doc["url"]}"/>\n{doc["content"]}\n</Document>'
                for doc in search_docs
            ]
        )
        #print(formatted_search_docs)
        with st.expander("TavilySearch取得データ"):
            st.write(formatted_search_docs,key="tavilysearch")


        return {"context": [formatted_search_docs]} 
    except:
        return {"context": ["-"]}
```

## File: niship2_news-flow_tools_youcom.py/source.json (Size: 0.23 KB)

```
{
  "url": "https://github.com/niship2/news-flow/blob/main/tools/youcom.py",
  "owner": "niship2",
  "repo": "news-flow",
  "branch": "main",
  "path": "tools/youcom.py",
  "content_type": "FILE",
  "download_time": "2025-02-25 01:16:49"
}
```

## File: niship2_news-flow_tools_youcom.py/youcom.py (Size: 1.06 KB)

```
import requests
import json
import streamlit as st
from utils.utils import return_period


def search_youcom(state):
    
    """ Retrieve docs from youcom news search """

    
    try:    
        url = "https://api.ydc-index.io/news"

        querystring = {"query":state['question'],"recency": return_period(nws="youcom", time_op=state["time_op"])}
        #"recency": "month",  # day, week, month, year

        YOUCOM_API_KEY = st.secrets["YOUCOM_API_KEY"]

        headers = {"X-API-Key": YOUCOM_API_KEY}


       # Search
        response = requests.request("GET", url, headers=headers, params=querystring)
        res = json.loads(response.text)

        


        # Format
        formatted_search_docs = "\n\n---\n\n".join(
            [
            f'<Document source="{doc["source_name"]}" date="{doc["page_age"]}"/>\n{doc["title"]}\n{doc["url"]}\n</Document>'
            for doc in res["news"]["results"]
            ])
        #print(formatted_search_docs)
     

        return {"context": [formatted_search_docs]} 
    except:
        return {"context": ["-"]}
```

## File: niship2_newsapi2_subs_youcomsearch.py/source.json (Size: 0.24 KB)

```
{
  "url": "https://github.com/niship2/newsapi2/blob/main/subs/youcomsearch.py",
  "owner": "niship2",
  "repo": "newsapi2",
  "branch": "main",
  "path": "subs/youcomsearch.py",
  "content_type": "FILE",
  "download_time": "2025-02-25 01:16:49"
}
```

## File: niship2_newsapi2_subs_youcomsearch.py/youcomsearch.py (Size: 1.03 KB)

```
import requests
import os
import streamlit as st

from langchain.retrievers.you import YouRetriever
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.chat_models import ChatAnthropic


YOUCOM_API_KEY = st.secrets["YOUCOM_API_KEY"]
os.environ["YDC_API_KEY"] = YOUCOM_API_KEY


def get_news_snippets_for_query(query):
    headers = {"X-API-Key": YOUCOM_API_KEY}
    params = {
        "query": query,
        "recency": "month",  # day, week, month, year
    }
    return requests.get(
        f"https://api.ydc-index.io/news?q={query}",
        params=params,
        headers=headers,
    ).json()


# os.environ["ANTHROPIC_API_KEY"] = userdata.get('anthropic')
# model = "claude-2"
# qa = RetrievalQA.from_chain_type(llm=ChatAnthropic(model=model), chain_type="stuff", retriever=yr)


def get_llm_answer(query):
    model = "gpt-4o"
    yr = YouRetriever()
    qa = RetrievalQA.from_chain_type(
        llm=ChatOpenAI(model=model), chain_type="stuff", retriever=yr
    )
    return qa.invoke(query)
```

## File: oborys_security-ai-agent-brama_braveSearch.py/braveSearch.py (Size: 3.10 KB)

```
import re
import requests
from time import sleep
import json
import os
from langchain_anthropic import ChatAnthropic, Anthropic
from bs4 import BeautifulSoup

BRAVE_API_KEY=os.environ.get('BRAVE_API_KEY')

def generate_phone_number_variants(phone_number):
    # Extract digits from the phone number
    digits = re.sub(r'\D', '', phone_number)

    # Generate different variants
    variants = []
    variants.append(phone_number.strip())  # Original format, without trailing newline
    variants.append(digits.strip())  # No spaces or hyphens, without trailing newline

    # Remove country code if present
    if digits.startswith('+'):
        variants.append(digits[1:].strip())  # No country code, without the '+' sign and trailing newline

    # Remove parentheses and hyphens
    digits_only = re.sub(r'\D', '', digits)

    # Generate variants without area code
    if len(digits_only) > 10:
        variants.append(digits_only[len(digits_only)-10:].strip())  # No area code, without trailing newline
        variants.append(digits_only[len(digits_only)-9:].strip())  # No country code, without the first digit and trailing newline

    return variants

def get_page_content(url: str) -> str:
    try:
        html = requests.get(url).text
        soup = BeautifulSoup(html, 'html.parser')
        text = soup.get_text(strip=True, separator='\n')
        return text[:6000]
    except requests.exceptions.RequestException as e:
        return f"Error: {e}"

def get_search_results(search_query: str):
    headers = {"Accept": "application/json", "X-Subscription-Token": BRAVE_API_KEY}
    search_query_encoded = search_query.encode('utf-8').decode('ascii', 'ignore')
    brave_api_endpoint = "https://api.search.brave.com/res/v1/web/search"

    response = requests.get(brave_api_endpoint, params={'q': search_query_encoded, 'count': 4}, headers=headers, timeout=60)
    if not response.ok:
        raise Exception(f"HTTP error {response.status_code}")
    sleep(1)  # avoid Brave rate limit
    return response.json().get("web", {}).get("results")

    
def checkPhoneLogic(phoneNumberVariants):
    queries_json = {"queries": phoneNumberVariants}
    queries = queries_json["queries"]
    urls_seen = set()
    web_search_results = []
    #country_code = 'US'
    
    for query in queries:
        search_results = get_search_results(query)
        for result in search_results:
            url = result.get("url")
            if not url or url in urls_seen:
                continue
            
            urls_seen.add(url)
            page_content = get_page_content(url)
            if page_content.startswith("Error:"):
                result["page_content"] = page_content
            else:
                result["page_content"] = page_content[:6000]
            web_search_results.append(result)
            

    formatted_search_results = "\n".join(
            [
                f'<item index="{i+1}">\n<source>{result.get("url")}</source>\n<page_content>\n{get_page_content(result.get("url"))}\n</page_content>\n</item>'
                for i, result in enumerate(web_search_results)
            ]
        )

    return(formatted_search_results)
```

## File: oborys_security-ai-agent-brama_braveSearch.py/source.json (Size: 0.26 KB)

```
{
  "url": "https://github.com/oborys/security-ai-agent-brama/blob/main/braveSearch.py",
  "owner": "oborys",
  "repo": "security-ai-agent-brama",
  "branch": "main",
  "path": "braveSearch.py",
  "content_type": "FILE",
  "download_time": "2025-02-25 01:16:50"
}
```

## File: pengfeng_ask.py/.gitignore (Size: 3.09 KB)

```
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/latest/usage/project/#working-with-version-control
.pdm.toml
.pdm-python
.pdm-build/

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

.gradio
.DS_Store
.env*
```

## File: pengfeng_ask.py/LICENSE (Size: 1.04 KB)

```
MIT License

Copyright (c) 2024 pengfeng

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

## File: pengfeng_ask.py/README.md (Size: 11.37 KB)

```

[![License](https://img.shields.io/github/license/pengfeng/ask.py)](LICENSE)

- [🚀 **Updates!** 🚀](#-updates-)
- [Introduction](#introduction)
  - [Demo use cases](#demo-use-cases)
  - [The search-extract-summarize flow](#the-search-extract-summarize-flow)
- [Quick start](#quick-start)
- [Use Different LLM Endpoints](#use-different-llm-endpoints)
  - [Use local Ollama inference and embedding models](#use-local-ollama-inference-and-embedding-models)
  - [Use DeepSeek API inference with OpenAI embedding models](#use-deepseek-api-inference-with-openai-embedding-models)
- [GradIO Deployment](#gradio-deployment)
- [Community](#community)


# 🚀 **Updates!** 🚀

A full version with db support and configurable components is open sourced here:
[LeetTools](https://github.com/leettools-dev/leettools). A demo web site has been setup 
[here](https://svc.leettools.com). Please check them out!

We also added support for local Ollama inference and embedding models, as well as for other API
providers such as DeepSeek. Please see the [`Use Different LLM Endpoints`](#use-different-llm-endpoints) secton for more details.

> [UPDATE]
> - 2025-01-20: add support for separate API endpoints for inference and embedding
> - 2025-01-20: add support for .env file switch and Ollama example
> - 2025-01-20: add support for default search proxy
> - 2024-12-20: add the full function version link
> - 2024-11-20: add Docling converter and local mode to query against local files
> - 2024-11-10: add Chonkie as the default chunker
> - 2024-10-28: add extract function as a new output mode
> - 2024-10-25: add hybrid search demo using DuckDB full-text search
> - 2024-10-22: add GradIO integation
> - 2024-10-21: use DuckDB for the vector search and use API for embedding
> - 2024-10-20: allow to specify a list of input urls
> - 2024-10-18: output-language and output-length parameters for LLM
> - 2024-10-18: date-restrict and target-site parameters for seach

# Introduction

A single Python program to implement the search-extract-summarize flow, similar to AI search
engines such as Perplexity.

- You can run it with local Ollama inference and embedding models.
- You can run it on command line or with a GradIO UI.
- You can control the output behavior, e.g., extract structured data or change output language,
- You can control the search behavior, e.g., restrict to a specific site or date, or just scrape
  a specified list of URLs.
- You can run it in a cron job or bash script to automate complex search/data extraction tasks.
- You can ask questions against local files.

We have a running UI example [in HuggingFace Spaces](https://huggingface.co/spaces/leettools/AskPy).

![image](https://github.com/user-attachments/assets/0483e6a2-75d7-4fbd-813f-bfa13839c836)

## Demo use cases

- [Search like Perplexity](demos/search_and_answer.md)
- [Only use the latest information from a specific site](demos/search_on_site_and_date.md)
- [Extract information from web search results](demos/search_and_extract.md)
- [Ask questions against local files](demos/local_files.md)
- [Use Ollama local LLM and Embedding models](demos/run_with_ollama.md)

> [!NOTE]
>
> - Our main goal is to illustrate the basic concepts of AI search engines with the raw constructs.
>   Performance or scalability is not in the scope of this program.
> - We are planning to open source a real search-enabled AI toolset with real DB setup, real document
>   pipeline, and real query engine soon. Star and watch this repo for updates!

## The search-extract-summarize flow

Given a query, the program will

- in search mode: search Google for the top 10 web pages
- in local mode: use the local files under the 'data' directory
- crawl and scape the result documents for their text content
- chunk the text content into chunks and save them into a vectordb
- perform a hybrid search (vector and BM25 FTS) with the query and find the top 10 matched chunks
- [Optional] use a reranker to re-rank the top chunks
- use the top chunks as the context to ask an LLM to generate the answer
- output the answer with the references

Of course this flow is a very simplified version of the real AI search engines, but it is a good
starting point to understand the basic concepts.

One benefit is that we can manipulate the search function and output format.

For example, we can:

- search with date-restrict to only retrieve the latest information.
- search within a target-site to only create the answer from the contents from it.
- ask LLM to use a specific language to answer the question.
- ask LLM to answer with a specific length.
- crawl a specific list of urls and answer based on those contents only.

This program can serve as a playground to understand and experiment with different components in
the pipeline.

# Quick start

```bash
# recommend to use Python 3.10 or later and use venv or conda to create a virtual environment
% pip install -r requirements.txt

# modify .env file to set the API keys or export them as environment variables as below

# you can use the Google search API, if not set we provide a default search engine proxy for testing
# % export SEARCH_API_KEY="your-google-search-api-key"
# % export SEARCH_PROJECT_KEY="your-google-cx-key"

# right now we use OpenAI API, default using OpenAI
# % export LLM_BASE_URL=https://api.openai.com/v1
% export LLM_API_KEY=<your-openai-api-key>

# By default, the program will start a web UI. See GradIO Deployment section for more info.
# Run the program on command line with -c option
% python ask.py -c -q "What is an LLM agent?"

# You can also query your local files under the 'data' directory using the local mode
% python ask.py -i local -c -q "How does Ask.py work?"

# we can specify more parameters to control the behavior such as date_restrict and target_site
% python ask.py --help
Usage: ask.py [OPTIONS]

  Search web for the query and summarize the results.

Options:
  -q, --query TEXT                Query to search
  -i, --input-mode [search|local]
                                  Input mode for the query, default is search.
                                  When using local, files under 'data' folder
                                  will be used as input.
  -o, --output-mode [answer|extract]
                                  Output mode for the answer, default is a
                                  simple answer
  -d, --date-restrict INTEGER     Restrict search results to a specific date
                                  range, default is no restriction
  -s, --target-site TEXT          Restrict search results to a specific site,
                                  default is no restriction
  --output-language TEXT          Output language for the answer
  --output-length INTEGER         Output length for the answer
  --url-list-file TEXT            Instead of doing web search, scrape the
                                  target URL list and answer the query based
                                  on the content
  --extract-schema-file TEXT      Pydantic schema for the extract mode
  --inference-model-name TEXT     Model name to use for inference
  --vector-search-only            Do not use hybrid search mode, use vector
                                  search only.
  -c, --run-cli                   Run as a command line tool instead of
                                  launching the Gradio UI
  -e, --env TEXT                  The environment file to use, absolute path
                                  or related to package root.
  -l, --log-level [DEBUG|INFO|WARNING|ERROR]
                                  Set the logging level  [default: INFO]
  --help                          Show this message and exit.
```

# Use Different LLM Endpoints

## Use local Ollama inference and embedding models
We can run Ask.py with different env files to use different LLM endpoints and other
related settings. For example, if you have a local Ollama serving instance, you can set
to use it as follows:

```bash
# you may need to pull the models first
% ollama pull llama3.2
% ollama pull nomic-embed-text
% ollama serve

% cat > .env.ollama <<EOF
LLM_BASE_URL=http://localhost:11434/v1
LLM_API_KEY=dummy-key
DEFAULT_INFERENCE_MODEL=llama3.2
EMBEDDING_MODEL=nomic-embed-text
EMBEDDING_DIMENSIONS=768
EOF

# Then run the command with the -e option to specify the .env file to use
% python ask.py -e .env.ollama -c -q "How does Ollama work?"
```

## Use DeepSeek API inference with OpenAI embedding models

We can also use one provider for inference and another for embedding. For example, we can use
DeepSeek API for inference and OpenAI for embedding since DeepSeek does not provide an embedding
endpoint as of Jan 2025:

```bash
% cat > .env.deepseek <<EOF
LLM_BASE_URL=https://api.deepseek.com/v1
LLM_API_KEY=<deepseek-api-key>
DEFAULT_INFERENCE_MODEL=deepseek-chat

EMBED_BASE_URL=https://api.openai.com/v1
EMBED_API_KEY=<openai-api-key>
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_DIMENSIONS=1536
EOF

% python ask.py -e .env.deepseek -c -q "How does DeepSeek work?"
```


# GradIO Deployment

> [!NOTE]
> Original GradIO app-sharing document [here](https://www.gradio.app/guides/sharing-your-app).

**Quick test and sharing**

By default, the program will start a web UI and share through GradIO.

```bash
% python ask.py
* Running on local URL:  http://127.0.0.1:7860
* Running on public URL: https://77c277af0330326587.gradio.live

# you can also specify SHARE_GRADIO_UI to only run locally
% export SHARE_GRADIO_UI=False
% python ask.py
* Running on local URL:  http://127.0.0.1:7860
```

**To share a more permanent link using HuggingFace Spaces**

- First, you need to [create a free HuggingFace account](https://huggingface.co/welcome).
- Then in your [settings/token page](https://huggingface.co/settings/tokens), create a new token with Write permissions.
- In your terminal, run the following commands in you app directory to deploy your program to
  HuggingFace Spaces:

```bash
% pip install gradio
% gradio deploy
Creating new Spaces Repo in '/home/you/ask.py'. Collecting metadata, press Enter to accept default value.
Enter Spaces app title [ask.py]: ask.py
Enter Gradio app file [ask.py]:
Enter Spaces hardware (cpu-basic, cpu-upgrade, t4-small, t4-medium, l4x1, l4x4, zero-a10g, a10g-small, a10g-large, a10g-largex2, a10g-largex4, a100-large, v5e-1x1, v5e-2x2, v5e-2x4) [cpu-basic]:
Any Spaces secrets (y/n) [n]: y
Enter secret name (leave blank to end): SEARCH_API_KEY
Enter secret value for SEARCH_API_KEY: YOUR_SEARCH_API_KEY
Enter secret name (leave blank to end): SEARCH_PROJECT_KEY
Enter secret value for SEARCH_API_KEY: YOUR_SEARCH_PROJECT_KEY
Enter secret name (leave blank to end): LLM_API_KEY
Enter secret value for LLM_API_KEY: YOUR_LLM_API_KEY
Enter secret name (leave blank to end):
Create Github Action to automatically update Space on 'git push'? [n]: n
Space available at https://huggingface.co/spaces/your_user_name/ask.py
```

Now you can use the HuggingFace space app to run your queries.


# Community

**License and Acknowledgment**

The source code is licensed under MIT license. Thanks for these amazing open-source projects and API
providers:

- [Google Search API](https://developers.google.com/custom-search/v1/overview)
- [OpenAI API](https://beta.openai.com/docs/api-reference/completions/create)
- [Jinja2](https://jinja.palletsprojects.com/en/3.0.x/)
- [bs4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [DuckDB](https://github.com/duckdb/duckdb)
- [Docling](https://github.com/DS4SD/docling)
- [GradIO](https://github.com/gradio-app/gradio)
- [Chonkie](https://github.com/bhavnicksm/chonkie)
```

## File: pengfeng_ask.py/requirements.txt (Size: 0.16 KB)

```
click==8.1.7
requests==2.32.3
numpy==1.26.4
jinja2==3.1.3
bs4==0.0.2
python-dotenv==1.0.1
openai==1.57.2
duckdb==1.1.2
gradio==5.3.0
chonkie==0.1.2
docling==2.5.2
```

## File: pengfeng_ask.py/source.json (Size: 0.21 KB)

```
{
  "url": "https://github.com/pengfeng/ask.py/tree/master",
  "owner": "pengfeng",
  "repo": "ask.py",
  "branch": "master",
  "path": "",
  "content_type": "REPOSITORY",
  "download_time": "2025-02-25 01:16:50"
}
```

## File: rsrohan99_Llama-Researcher_tavily.py/source.json (Size: 0.24 KB)

```
{
  "url": "https://github.com/rsrohan99/Llama-Researcher/blob/master/tavily.py",
  "owner": "rsrohan99",
  "repo": "Llama-Researcher",
  "branch": "master",
  "path": "tavily.py",
  "content_type": "FILE",
  "download_time": "2025-02-25 01:16:59"
}
```

## File: rsrohan99_Llama-Researcher_tavily.py/tavily.py (Size: 1.44 KB)

```
import os
from typing import List
import requests
from dotenv import load_dotenv

from llama_index.core.schema import Document


async def get_docs_from_tavily_search(sub_query: str, visited_urls: set[str]):
    load_dotenv()
    api_key = os.getenv("TAVILY_API_KEY")
    base_url = "https://api.tavily.com/search"
    headers = {
        "Content-Type": "application/json",
    }
    data = {
        "query": sub_query,
        "api_key": api_key,
        "include_raw_content": True,
    }

    docs = []
    print(f"\n> Searching Tavily for sub query: {sub_query}\n")
    response = requests.post(base_url, headers=headers, json=data)
    if response.status_code == 200:
        search_results = response.json().get("results", [])
        for search_result in search_results:
            url = search_result.get("url")
            if not search_result.get("raw_content"):
                continue
            if url not in visited_urls:
                visited_urls.add(url)
                docs.append(
                    Document(
                        text=search_result.get("raw_content"),
                        metadata={
                            "source": url,
                            "title": search_result.get("title"),
                        },
                    )
                )
        print(f"\n> Found {len(docs)} docs from Tavily search on {sub_query}\n")
        return docs, visited_urls
    else:
        response.raise_for_status()
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-bing-search/.gitignore (Size: 1.96 KB)

```
llama_index/_static
.DS_Store
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
bin/
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
etc/
include/
lib/
lib64/
parts/
sdist/
share/
var/
wheels/
pip-wheel-metadata/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
.ruff_cache

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
target/

# Jupyter Notebook
.ipynb_checkpoints
notebooks/

# IPython
profile_default/
ipython_config.py

# pyenv
.python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# PEP 582; used by e.g. github.com/David-OConnor/pyflow
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/
pyvenv.cfg

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# Jetbrains
.idea
modules/
*.swp

# VsCode
.vscode

# pipenv
Pipfile
Pipfile.lock

# pyright
pyrightconfig.json
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-bing-search/BUILD (Size: 0.04 KB)

```
poetry_requirements(
    name="poetry",
)
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-bing-search/CHANGELOG.md (Size: 0.10 KB)

```
# CHANGELOG

## [0.1.2] - 2024-02-13

- Add maintainers and keywords from library.json (llamahub)
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-bing-search/Makefile (Size: 0.66 KB)

```
GIT_ROOT ?= $(shell git rev-parse --show-toplevel)

help:	## Show all Makefile targets.
	@grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | awk 'BEGIN {FS = ":.*?## "}; {printf "\033[33m%-30s\033[0m %s\n", $$1, $$2}'

format:	## Run code autoformatters (black).
	pre-commit install
	git ls-files | xargs pre-commit run black --files

lint:	## Run linters: pre-commit (black, ruff, codespell) and mypy
	pre-commit install && git ls-files | xargs pre-commit run --show-diff-on-failure --files

test:	## Run tests via pytest.
	pytest tests

watch-docs:	## Build and watch documentation.
	sphinx-autobuild docs/ docs/_build/html --open-browser --watch $(GIT_ROOT)/llama_index/
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-bing-search/README.md (Size: 1.17 KB)

```
# Bing Search Tool

This tool connects to a Bing account and allows an Agent to perform searches for news, images and videos.

You will need to set up a search key using Azure,learn more here: https://learn.microsoft.com/en-us/bing/search-apis/bing-web-search/overview

## Usage

This tool has a more extensive example usage documented in a Jupyter notebook [here](https://github.com/run-llama/llama_index/blob/main/llama-index-integrations/tools/llama-index-tools-bing-search/examples/bing_search.ipynb)

Here's an example usage of the BingSearchToolSpec.

```python
from llama_index.tools.bing_search import BingSearchToolSpec
from llama_index.agent.openai import OpenAIAgent

tool_spec = BingSearchToolSpec(api_key="your-key")

agent = OpenAIAgent.from_tools(tool_spec.to_tool_list())

agent.chat("what's the latest news about superconductors")
agent.chat("what does lk-99 look like")
agent.chat("is there any videos of it levitating")
```

`bing_news_search`: Search for news results related to a query
`bing_image_search`: Search for images related to a query
`bing_video_search`: Search for videos related to a query

This loader is designed to be used as a way to load data as a Tool in a Agent.
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-bing-search/examples/bing_search.ipynb (Size: 7.53 KB)

```
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2408d6e-8e07-47e5-a7e3-daf3022a44de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup OpenAI Agent\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"sk-your-key\"\n",
    "from llama_index.agent import OpenAIAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e91e13e-d7da-47d1-8122-5e11bb1b5a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: bing_news_search with args: {\n",
      "  \"query\": \"latest news about superconductors\"\n",
      "}\n",
      "Got output: [['Scientists skeptical of superconductor claims that went viral', 'A recent claim by South Korean researchers that they have created a material which works as a superconductor at room temperature—long a holy grail of physics—has been met with huge excitement on social media but skepticism from scientists.', 'https://phys.org/news/2023-08-scientists-skeptical-superconductor-viral.html'], [\"Room-temperature superconductors: The facts behind the 'holy grail' of physics\", 'Onto this crowded scene comes LK-99, a material whose resistivity, its researchers claim, drops to near zero at 86 F (30C). The material is made up of mixed powders containing lead, oxygen, sulfur and phosphorus which is doped with copper. It is also relatively easy to manufacture and test.', 'https://www.msn.com/en-us/news/technology/room-temperature-superconductors-the-facts-behind-the-holy-grail-of-physics/ar-AA1eVoxP'], ['Scientists Sceptical Of Superconductor Claims That Went Viral', 'A recent claim by South Korean researchers that they have created a material which works as a superconductor at room temperature -- long a holy grail of physics -- has been met with huge excitement on social media but scepticism from scientists.', 'https://www.barrons.com/news/scientists-sceptical-of-superconductor-claims-that-went-viral-ee5e0918?refsec=topics_afp-news']]\n",
      "========================\n",
      "Here are some of the latest news articles about superconductors:\n",
      "\n",
      "1. \"Scientists skeptical of superconductor claims that went viral\" - This article discusses a recent claim by South Korean researchers that they have created a material which works as a superconductor at room temperature. The claim has generated excitement on social media but skepticism from scientists. [Read more](https://phys.org/news/2023-08-scientists-skeptical-superconductor-viral.html)\n",
      "\n",
      "2. \"Room-temperature superconductors: The facts behind the 'holy grail' of physics\" - This article provides information about LK-99, a material that claims to exhibit superconductivity at room temperature. The material is made up of mixed powders containing lead, oxygen, sulfur, and phosphorus, and it is relatively easy to manufacture and test. [Read more](https://www.msn.com/en-us/news/technology/room-temperature-superconductors-the-facts-behind-the-holy-grail-of-physics/ar-AA1eVoxP)\n",
      "\n",
      "3. \"Scientists Sceptical Of Superconductor Claims That Went Viral\" - This article also discusses the claim made by South Korean researchers about a room temperature superconductor. It highlights the excitement on social media but the skepticism from scientists. [Read more](https://www.barrons.com/news/scientists-sceptical-of-superconductor-claims-that-went-viral-ee5e0918?refsec=topics_afp-news)\n",
      "\n",
      "You can click on the links to read the full articles.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.tools.bing_search.base import BingSearchToolSpec\n",
    "\n",
    "bing_tool = BingSearchToolSpec(api_key=\"your-key\")\n",
    "agent = OpenAIAgent.from_tools(\n",
    "    bing_tool.to_tool_list(),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(agent.chat(\"whats the latest news about superconductors\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a8981a-ba62-4c86-a755-a4ec26051805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: bing_image_search with args: {\n",
      "  \"query\": \"LK-99 superconductor\"\n",
      "}\n",
      "Got output: [['What is the LK-99 superconductor and why is it called the Holy Grail of ...', 'https://cdn.computerhoy.com/sites/navi.axelspringer.es/public/media/image/2023/08/superconductor-lk-99-3103376.jpg?tf=1200x'], ['The LK-99 controversy: between Nobel prospects and skeptical scrutiny ...', 'https://media.cybernews.com/images/featured-big/2023/08/LK-99-superconductor.png'], ['Will the LK-99 superconductor change the world?', 'https://images.moneycontrol.com/static-mcnews/2023/08/lk-99-superconductor-bbo-770x433.jpg?impolicy=website&width=770&height=431']]\n",
      "========================\n",
      "Here are some images of the LK-99 superconductor:\n",
      "\n",
      "1. ![LK-99 superconductor](https://cdn.computerhoy.com/sites/navi.axelspringer.es/public/media/image/2023/08/superconductor-lk-99-3103376.jpg?tf=1200x)\n",
      "\n",
      "2. ![LK-99 superconductor](https://media.cybernews.com/images/featured-big/2023/08/LK-99-superconductor.png)\n",
      "\n",
      "3. ![LK-99 superconductor](https://images.moneycontrol.com/static-mcnews/2023/08/lk-99-superconductor-bbo-770x433.jpg?impolicy=website&width=770&height=431)\n",
      "\n",
      "Please note that these images are for illustrative purposes and may not represent the exact appearance of the LK-99 superconductor.\n"
     ]
    }
   ],
   "source": [
    "print(agent.chat(\"what does lk-99 look like\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa378d0-b797-4323-a91c-06baf19fe4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: bing_video_search with args: {\n",
      "  \"query\": \"LK-99 superconductor levitation\"\n",
      "}\n",
      "Got output: [['LK-99, a candidate for room temperature superconductivity', 'https://www.youtube.com/watch?v=AFDH4asSdqk'], ['「超電導体の磁気浮上を室温かつ常圧で確認した」とされるムービー【LK-99】', 'https://www.youtube.com/watch?v=aw5sw5TDihU'], ['Boaz Almog \"levitates\" a superconductor', 'https://www.youtube.com/watch?v=PXHczjOg06w']]\n",
      "========================\n",
      "Here are some videos of the LK-99 superconductor levitating:\n",
      "\n",
      "1. [LK-99, a candidate for room temperature superconductivity](https://www.youtube.com/watch?v=AFDH4asSdqk)\n",
      "\n",
      "2. [「超電導体の磁気浮上を室温かつ常圧で確認した」とされるムービー【LK-99】](https://www.youtube.com/watch?v=aw5sw5TDihU)\n",
      "\n",
      "3. [Boaz Almog \"levitates\" a superconductor](https://www.youtube.com/watch?v=PXHczjOg06w)\n",
      "\n",
      "You can click on the links to watch the videos.\n"
     ]
    }
   ],
   "source": [
    "print(agent.chat(\"is there any videos of it levitating\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-bing-search/llama_index/tools/bing_search/BUILD (Size: 0.02 KB)

```
python_sources()
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-bing-search/llama_index/tools/bing_search/__init__.py (Size: 0.16 KB)

```
## init
from llama_index.tools.bing_search.base import (
    ENDPOINT_BASE_URL,
    BingSearchToolSpec,
)

__all__ = ["BingSearchToolSpec", "ENDPOINT_BASE_URL"]
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-bing-search/llama_index/tools/bing_search/base.py (Size: 1.96 KB)

```
"""Bing Search tool spec."""

from typing import List, Optional

import requests
from llama_index.core.tools.tool_spec.base import BaseToolSpec

ENDPOINT_BASE_URL = "https://api.bing.microsoft.com/v7.0/"


class BingSearchToolSpec(BaseToolSpec):
    """Bing Search tool spec."""

    spec_functions = ["bing_news_search", "bing_image_search", "bing_video_search"]

    def __init__(
        self, api_key: str, lang: Optional[str] = "en-US", results: Optional[int] = 3
    ) -> None:
        """Initialize with parameters."""
        self.api_key = api_key
        self.lang = lang
        self.results = results

    def _bing_request(self, endpoint: str, query: str, keys: List[str]):
        response = requests.get(
            ENDPOINT_BASE_URL + endpoint,
            headers={"Ocp-Apim-Subscription-Key": self.api_key},
            params={"q": query, "mkt": self.lang, "count": self.results},
        )
        response_json = response.json()
        return [[result[key] for key in keys] for result in response_json["value"]]

    def bing_news_search(self, query: str):
        """
        Make a query to bing news search. Useful for finding news on a query.

        Args:
            query (str): The query to be passed to bing.

        """
        return self._bing_request("news/search", query, ["name", "description", "url"])

    def bing_image_search(self, query: str):
        """
        Make a query to bing images search. Useful for finding an image of a query.

        Args:
            query (str): The query to be passed to bing.

        returns a url of the images found
        """
        return self._bing_request("images/search", query, ["name", "contentUrl"])

    def bing_video_search(self, query: str):
        """
        Make a query to bing video search. Useful for finding a video related to a query.

        Args:
            query (str): The query to be passed to bing.

        """
        return self._bing_request("videos/search", query, ["name", "contentUrl"])
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-bing-search/pyproject.toml (Size: 1.44 KB)

```
[build-system]
build-backend = "poetry.core.masonry.api"
requires = ["poetry-core"]

[tool.codespell]
check-filenames = true
check-hidden = true
skip = "*.csv,*.html,*.json,*.jsonl,*.pdf,*.txt,*.ipynb"

[tool.llamahub]
contains_example = false
import_path = "llama_index.tools.bing_search"

[tool.llamahub.class_authors]
BingSearchToolSpec = "ajhofmann"

[tool.mypy]
disallow_untyped_defs = true
exclude = ["_static", "build", "examples", "notebooks", "venv"]
ignore_missing_imports = true
python_version = "3.8"

[tool.poetry]
authors = ["Your Name <you@example.com>"]
description = "llama-index tools bing_search integration"
exclude = ["**/BUILD"]
license = "MIT"
maintainers = ["ajhofmann"]
name = "llama-index-tools-bing-search"
readme = "README.md"
version = "0.3.0"

[tool.poetry.dependencies]
python = ">=3.9,<4.0"
llama-index-core = "^0.12.0"

[tool.poetry.group.dev.dependencies]
ipython = "8.10.0"
jupyter = "^1.0.0"
mypy = "0.991"
pre-commit = "3.2.0"
pylint = "2.15.10"
pytest = "7.2.1"
pytest-mock = "3.11.1"
ruff = "0.0.292"
tree-sitter-languages = "^1.8.0"
types-Deprecated = ">=0.1.0"
types-PyYAML = "^6.0.12.12"
types-protobuf = "^4.24.0.4"
types-redis = "4.5.5.0"
types-requests = "2.28.11.8"
types-setuptools = "67.1.0.0"

[tool.poetry.group.dev.dependencies.black]
extras = ["jupyter"]
version = "<=23.9.1,>=23.7.0"

[tool.poetry.group.dev.dependencies.codespell]
extras = ["toml"]
version = ">=v2.2.6"

[[tool.poetry.packages]]
include = "llama_index/"
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-bing-search/source.json (Size: 0.33 KB)

```
{
  "url": "https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/tools/llama-index-tools-bing-search",
  "owner": "run-llama",
  "repo": "llama_index",
  "branch": "main",
  "path": "llama-index-integrations/tools/llama-index-tools-bing-search",
  "content_type": "DIRECTORY",
  "download_time": "2025-02-25 01:17:00"
}
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-bing-search/tests/BUILD (Size: 0.01 KB)

```
python_tests()
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-bing-search/tests/__init__.py (Size: 0.00 KB)

```

```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-bing-search/tests/test_tools_bing_search.py (Size: 0.27 KB)

```
from llama_index.core.tools.tool_spec.base import BaseToolSpec
from llama_index.tools.bing_search import BingSearchToolSpec


def test_class():
    names_of_base_classes = [b.__name__ for b in BingSearchToolSpec.__mro__]
    assert BaseToolSpec.__name__ in names_of_base_classes
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-brave-search/.gitignore (Size: 1.96 KB)

```
llama_index/_static
.DS_Store
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
bin/
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
etc/
include/
lib/
lib64/
parts/
sdist/
share/
var/
wheels/
pip-wheel-metadata/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
.ruff_cache

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
target/

# Jupyter Notebook
.ipynb_checkpoints
notebooks/

# IPython
profile_default/
ipython_config.py

# pyenv
.python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# PEP 582; used by e.g. github.com/David-OConnor/pyflow
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/
pyvenv.cfg

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# Jetbrains
.idea
modules/
*.swp

# VsCode
.vscode

# pipenv
Pipfile
Pipfile.lock

# pyright
pyrightconfig.json
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-brave-search/BUILD (Size: 0.04 KB)

```
poetry_requirements(
    name="poetry",
)
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-brave-search/Makefile (Size: 0.66 KB)

```
GIT_ROOT ?= $(shell git rev-parse --show-toplevel)

help:	## Show all Makefile targets.
	@grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | awk 'BEGIN {FS = ":.*?## "}; {printf "\033[33m%-30s\033[0m %s\n", $$1, $$2}'

format:	## Run code autoformatters (black).
	pre-commit install
	git ls-files | xargs pre-commit run black --files

lint:	## Run linters: pre-commit (black, ruff, codespell) and mypy
	pre-commit install && git ls-files | xargs pre-commit run --show-diff-on-failure --files

test:	## Run tests via pytest.
	pytest tests

watch-docs:	## Build and watch documentation.
	sphinx-autobuild docs/ docs/_build/html --open-browser --watch $(GIT_ROOT)/llama_index/
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-brave-search/README.md (Size: 0.80 KB)

```
# LlamaIndex Tools Integration: Brave_Search

This tool enables agents to search and retrieve results from the Brave search engine.

You will need to set up an Brave account to get an search api key. Please check more here: https://brave.com/search/api

## Usage

This tool has a more extensive example usage documented in a Jupyter notebook [here](./examples/brave_search.ipynb)

Here's an example usage of the BraveSearchToolSpec.

```python
from llama_index.tools.brave_search import BraveSearchToolSpec
from llama_index.agent.openai import OpenAIAgent

tool_spec = BraveSearchToolSpec(api_key="your-key")

agent = OpenAIAgent.from_tools(tool_spec.to_tool_list())

agent.chat("what's the latest news about superconductors")
agent.chat("what does lk-99 look like")
agent.chat("is there any videos of it levitating")
```
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-brave-search/examples/brave_search.ipynb (Size: 1.15 KB)

```
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup OpenAI Agent\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"sk-xxx\"\n",
    "from llama_index.agent.openai import OpenAIAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools.brave_search.base import BraveSearchToolSpec\n",
    "\n",
    "brave_tool = BraveSearchToolSpec(api_key=\"your-api-key\")\n",
    "agent = OpenAIAgent.from_tools(\n",
    "    brave_tool.to_tool_list(),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(agent.chat(\"whats the latest news about superconductors\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app-aCoYrWM5-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-brave-search/llama_index/tools/brave_search/BUILD (Size: 0.02 KB)

```
python_sources()
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-brave-search/llama_index/tools/brave_search/__init__.py (Size: 0.11 KB)

```
## init
from llama_index.tools.brave_search.base import BraveSearchToolSpec

__all__ = ["BraveSearchToolSpec"]
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-brave-search/llama_index/tools/brave_search/base.py (Size: 1.92 KB)

```
import requests
import urllib.parse
from typing import Dict
from llama_index.core.schema import Document
from llama_index.core.tools.tool_spec.base import BaseToolSpec

SEARCH_URL_TMPL = "https://api.search.brave.com/res/v1/web/search?{params}"


class BraveSearchToolSpec(BaseToolSpec):
    """
    Brave Search tool spec.
    """

    spec_functions = ["brave_search"]

    def __init__(self, api_key: str) -> None:
        """
        Initialize with parameters.
        """
        self.api_key = api_key

    def _make_request(self, params: Dict) -> requests.Response:
        """
        Make a request to the Brave Search API.

        Args:
            params (dict): The parameters to be passed to the API.

        Returns:
            requests.Response: The response from the API.
        """
        headers = {
            "Accept": "application/json",
            "Accept-Encoding": "gzip",
            "X-Subscription-Token": self.api_key,
        }
        url = SEARCH_URL_TMPL.format(params=urllib.parse.urlencode(params))

        response = requests.get(url, headers=headers)
        response.raise_for_status()
        return response

    def brave_search(
        self, query: str, search_lang: str = "en", num_results: int = 5
    ) -> [Document]:
        """
        Make a query to the Brave Search engine to receive a list of results.

        Args:
            query (str): The query to be passed to Brave Search.
            search_lang (str): The search language preference (ISO 639-1), default is "en".
            num_results (int): The number of search results returned in response, default is 5.

        Returns:
            [Document]: A list of documents containing search results.
        """
        search_params = {
            "q": query,
            "search_lang": search_lang,
            "count": num_results,
        }

        response = self._make_request(search_params)
        return [Document(text=response.text)]
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-brave-search/pyproject.toml (Size: 1.56 KB)

```
[build-system]
build-backend = "poetry.core.masonry.api"
requires = ["poetry-core"]

[tool.codespell]
check-filenames = true
check-hidden = true
# Feel free to un-skip examples, and experimental, you will just need to
# work through many typos (--write-changes and --interactive will help)
skip = "*.csv,*.html,*.json,*.jsonl,*.pdf,*.txt,*.ipynb"

[tool.llamahub]
contains_example = false
import_path = "llama_index.tools.brave_search"

[tool.llamahub.class_authors]
BraveSearchToolSpec = "leehuwuj"

[tool.mypy]
disallow_untyped_defs = true
# Remove venv skip when integrated with pre-commit
exclude = ["_static", "build", "examples", "notebooks", "venv"]
ignore_missing_imports = true
python_version = "3.8"

[tool.poetry]
authors = ["Your Name <you@example.com>"]
description = "llama-index tools brave_search integration"
exclude = ["**/BUILD"]
license = "MIT"
name = "llama-index-tools-brave-search"
packages = [{include = "llama_index/"}]
readme = "README.md"
version = "0.3.0"

[tool.poetry.dependencies]
python = ">=3.9,<4.0"
llama-index-core = "^0.12.0"

[tool.poetry.group.dev.dependencies]
black = {extras = ["jupyter"], version = "<=23.9.1,>=23.7.0"}
codespell = {extras = ["toml"], version = ">=v2.2.6"}
ipython = "8.10.0"
jupyter = "^1.0.0"
mypy = "0.991"
pre-commit = "3.2.0"
pylint = "2.15.10"
pytest = "7.2.1"
pytest-mock = "3.11.1"
ruff = "0.0.292"
tree-sitter-languages = "^1.8.0"
types-Deprecated = ">=0.1.0"
types-PyYAML = "^6.0.12.12"
types-protobuf = "^4.24.0.4"
types-redis = "4.5.5.0"
types-requests = "2.28.11.8"  # TODO: unpin when mypy>0.991
types-setuptools = "67.1.0.0"
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-brave-search/source.json (Size: 0.34 KB)

```
{
  "url": "https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/tools/llama-index-tools-brave-search",
  "owner": "run-llama",
  "repo": "llama_index",
  "branch": "main",
  "path": "llama-index-integrations/tools/llama-index-tools-brave-search",
  "content_type": "DIRECTORY",
  "download_time": "2025-02-25 01:17:05"
}
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-brave-search/tests/BUILD (Size: 0.01 KB)

```
python_tests()
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-brave-search/tests/__init__.py (Size: 0.00 KB)

```

```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-brave-search/tests/test_tools_brave_search.py (Size: 0.28 KB)

```
from llama_index.core.tools.tool_spec.base import BaseToolSpec
from llama_index.tools.brave_search import BraveSearchToolSpec


def test_class():
    names_of_base_classes = [b.__name__ for b in BraveSearchToolSpec.__mro__]
    assert BaseToolSpec.__name__ in names_of_base_classes
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-linkup-research/.gitignore (Size: 1.96 KB)

```
llama_index/_static
.DS_Store
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
bin/
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
etc/
include/
lib/
lib64/
parts/
sdist/
share/
var/
wheels/
pip-wheel-metadata/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
.ruff_cache

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
target/

# Jupyter Notebook
.ipynb_checkpoints
notebooks/

# IPython
profile_default/
ipython_config.py

# pyenv
.python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# PEP 582; used by e.g. github.com/David-OConnor/pyflow
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/
pyvenv.cfg

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# Jetbrains
.idea
modules/
*.swp

# VsCode
.vscode

# pipenv
Pipfile
Pipfile.lock

# pyright
pyrightconfig.json
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-linkup-research/BUILD (Size: 0.04 KB)

```
poetry_requirements(
    name="poetry",
)
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-linkup-research/CHANGELOG.md (Size: 0.10 KB)

```
# CHANGELOG

## [0.1.2] - 2024-02-13

- Add maintainers and keywords from library.json (llamahub)
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-linkup-research/Makefile (Size: 0.66 KB)

```
GIT_ROOT ?= $(shell git rev-parse --show-toplevel)

help:	## Show all Makefile targets.
	@grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | awk 'BEGIN {FS = ":.*?## "}; {printf "\033[33m%-30s\033[0m %s\n", $$1, $$2}'

format:	## Run code autoformatters (black).
	pre-commit install
	git ls-files | xargs pre-commit run black --files

lint:	## Run linters: pre-commit (black, ruff, codespell) and mypy
	pre-commit install && git ls-files | xargs pre-commit run --show-diff-on-failure --files

test:	## Run tests via pytest.
	pytest tests

watch-docs:	## Build and watch documentation.
	sphinx-autobuild docs/ docs/_build/html --open-browser --watch $(GIT_ROOT)/llama_index/
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-linkup-research/README.md (Size: 1.42 KB)

```
# Linkup Research Tool

[Linkup](https://app.linkup.so/) is a robust research API tailored specifically for LLM Agents. It seamlessly integrates with diverse data sources to ensure a superior, relevant research experience.

- you need to obtain an API key on the [Linkup dashboard](https://app.linkup.com/)

### Quick Start:

```bash
pip install llama-index-tools-linkup-research
```

```python
import os
from llama_index.core.agent import FunctionCallingAgent
from llama_index.llms.openai import OpenAI
from llama_index.tools.linkup_research.base import LinkupToolSpec


# structured_schema=json.dumps(your schema here) # Only if output type is structured
# Initialisation of the tool
linkup_tool = LinkupToolSpec(
    api_key="your Linkup API Key",
    depth="",  # Choose (standard) for a faster result (deep) for a slower but more complete result.
    output_type="",  # Choose (searchResults) for a list of results relative to your query, (sourcedAnswer) for an answer and a list of sources, or (structured) if you want a specific schema.
    # structured_output_schema=structured_schema # Only if output type is structured
)

# Creation of the agent
agent = FunctionCallingAgent.from_tools(
    linkup_tool.to_tool_list(),
    llm=OpenAI(model="gpt-4o"),
)

# Query for the agent
agent.chat("Can you tell me which women were awarded the Physics Nobel Prize")
```

This loader is designed to be used as a way to load data as a Tool in an Agent.
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-linkup-research/examples/linkup.ipynb (Size: 3.13 KB)

```
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Linkup Data Agent\n",
    "\n",
    "This tutorial walks through using the LLM tools provided by the [Linkup API](https://app.linkup.so/) to allow LLMs to easily search and retrieve relevant content from the Internet.\n",
    "\n",
    "To get started, you will need an [OpenAI api key](https://platform.openai.com/account/api-keys) and a [Linkup API key](https://app.linkup.so)\n",
    "\n",
    "We will import the relevant agents and tools and pass them our keys here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.13.1)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/juliettesivan/llama_index/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-tools-linkup-research llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your openai key, if using openai\n",
    "\n",
    "import os\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Linkup tool\n",
    "\n",
    "from llama_index.tools.linkup_research.base import LinkupToolSpec\n",
    "\n",
    "# structured_schema=json.dumps(your schema here) # Only if output type is structured\n",
    "# Initialisation of the tool\n",
    "linkup_tool = LinkupToolSpec(\n",
    "    api_key=\"your Linkup API Key\",\n",
    "    depth=\"\",  # Choose (standard) for a faster result (deep) for a slower but more complete result.\n",
    "    output_type=\"\",  # Choose (searchResults) for a list of results relative to your query, (sourcedAnswer) for an answer and a list of sources, or (structured) if you want a specific shema.\n",
    "    # structured_output_schema=structured_schema # Only if output type is structured\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the agent\n",
    "\n",
    "from llama_index.core.agent import FunctionCallingAgent\n",
    "\n",
    "agent = FunctionCallingAgent.from_tools(\n",
    "    linkup_tool.to_tool_list(),\n",
    "    llm=OpenAI(model=\"gpt-4o\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query for the agent\n",
    "\n",
    "print(agent.chat(\"Can you tell me which women were awarded the Physics Nobel Prize\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-linkup-research/llama_index/tools/linkup_research/BUILD (Size: 0.02 KB)

```
python_sources()
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-linkup-research/llama_index/tools/linkup_research/__init__.py (Size: 0.09 KB)

```
from llama_index.tools.linkup_research.base import LinkupToolSpec

__all__ = ["LinkupToolSpec"]
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-linkup-research/llama_index/tools/linkup_research/base.py (Size: 1.15 KB)

```
"""Linkup tool spec."""

from llama_index.core.tools.tool_spec.base import BaseToolSpec


class LinkupToolSpec(BaseToolSpec):
    """Linkup tool spec."""

    spec_functions = [
        "search",
    ]

    def __init__(self, api_key: str, depth: str, output_type: str) -> None:
        """Initialize with parameters."""
        from linkup import LinkupClient

        self.client = LinkupClient(api_key=api_key)
        self.depth = depth
        self.output_type = output_type

    def search(self, query: str):
        """
        Run query through Linkup Search and return metadata.

        Args:
            query: The query to search for.
        """
        api_params = {
            "query": query,
            "depth": self.depth,
            "output_type": self.output_type,
        }
        if self.output_type == "structured":
            if not self.structured_output_schema:
                raise ValueError(
                    "structured_output_schema must be provided when output_type is 'structured'."
                )
            api_params["structured_output_schema"] = self.structured_output_schema
        return self.client.search(**api_params)
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-linkup-research/pyproject.toml (Size: 1.50 KB)

```
[build-system]
build-backend = "poetry.core.masonry.api"
requires = ["poetry-core"]

[tool.codespell]
check-filenames = true
check-hidden = true
skip = "*.csv,*.html,*.json,*.jsonl,*.pdf,*.txt,*.ipynb"

[tool.llamahub]
contains_example = false
import_path = "llama_index.tools.linkup_research"

[tool.llamahub.class_authors]
LinkupToolSpec = "juliette0704"

[tool.mypy]
disallow_untyped_defs = true
exclude = ["_static", "build", "examples", "notebooks", "venv"]
ignore_missing_imports = true
python_version = "3.8"

[tool.poetry]
authors = ["Your Name <you@example.com>"]
description = "llama-index tools linkup_research integration"
exclude = ["**/BUILD"]
keywords = ["search"]
license = "MIT"
maintainers = ["juliette0704"]
name = "llama-index-tools-linkup-research"
readme = "README.md"
version = "0.3.0"

[tool.poetry.dependencies]
python = ">=3.9,<4.0"
linkup-sdk = ">=0.2.2"
llama-index-core = "^0.12.0"

[tool.poetry.group.dev.dependencies]
ipython = "8.10.0"
jupyter = "^1.0.0"
mypy = "0.991"
pre-commit = "3.2.0"
pylint = "2.15.10"
pytest = "7.2.1"
pytest-mock = "3.11.1"
ruff = "0.0.292"
tree-sitter-languages = "^1.8.0"
types-Deprecated = ">=0.1.0"
types-PyYAML = "^6.0.12.12"
types-protobuf = "^4.24.0.4"
types-redis = "4.5.5.0"
types-requests = "2.28.11.8"
types-setuptools = "67.1.0.0"

[tool.poetry.group.dev.dependencies.black]
extras = ["jupyter"]
version = "<=23.9.1,>=23.7.0"

[tool.poetry.group.dev.dependencies.codespell]
extras = ["toml"]
version = ">=v2.2.6"

[[tool.poetry.packages]]
include = "llama_index/"
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-linkup-research/source.json (Size: 0.34 KB)

```
{
  "url": "https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/tools/llama-index-tools-linkup-research",
  "owner": "run-llama",
  "repo": "llama_index",
  "branch": "main",
  "path": "llama-index-integrations/tools/llama-index-tools-linkup-research",
  "content_type": "DIRECTORY",
  "download_time": "2025-02-25 01:17:10"
}
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-linkup-research/tests/BUILD (Size: 0.01 KB)

```
python_tests()
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-linkup-research/tests/__init__.py (Size: 0.00 KB)

```

```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-linkup-research/tests/test_tools_linkup_research.py (Size: 0.27 KB)

```
from llama_index.core.tools.tool_spec.base import BaseToolSpec
from llama_index.tools.linkup_research import LinkupToolSpec


def test_class():
    names_of_base_classes = [b.__name__ for b in LinkupToolSpec.__mro__]
    assert BaseToolSpec.__name__ in names_of_base_classes
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-tavily-research/.gitignore (Size: 1.96 KB)

```
llama_index/_static
.DS_Store
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
bin/
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
etc/
include/
lib/
lib64/
parts/
sdist/
share/
var/
wheels/
pip-wheel-metadata/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
.ruff_cache

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
target/

# Jupyter Notebook
.ipynb_checkpoints
notebooks/

# IPython
profile_default/
ipython_config.py

# pyenv
.python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# PEP 582; used by e.g. github.com/David-OConnor/pyflow
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/
pyvenv.cfg

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# Jetbrains
.idea
modules/
*.swp

# VsCode
.vscode

# pipenv
Pipfile
Pipfile.lock

# pyright
pyrightconfig.json
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-tavily-research/BUILD (Size: 0.08 KB)

```
poetry_requirements(
    name="poetry",
)

python_requirements(
    name="reqs",
)
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-tavily-research/CHANGELOG.md (Size: 0.10 KB)

```
# CHANGELOG

## [0.1.2] - 2024-02-13

- Add maintainers and keywords from library.json (llamahub)
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-tavily-research/Makefile (Size: 0.66 KB)

```
GIT_ROOT ?= $(shell git rev-parse --show-toplevel)

help:	## Show all Makefile targets.
	@grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | awk 'BEGIN {FS = ":.*?## "}; {printf "\033[33m%-30s\033[0m %s\n", $$1, $$2}'

format:	## Run code autoformatters (black).
	pre-commit install
	git ls-files | xargs pre-commit run black --files

lint:	## Run linters: pre-commit (black, ruff, codespell) and mypy
	pre-commit install && git ls-files | xargs pre-commit run --show-diff-on-failure --files

test:	## Run tests via pytest.
	pytest tests

watch-docs:	## Build and watch documentation.
	sphinx-autobuild docs/ docs/_build/html --open-browser --watch $(GIT_ROOT)/llama_index/
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-tavily-research/README.md (Size: 2.40 KB)

```
# Tavily Research Tool

[Tavily](https://app.tavily.com/) is a robust research API tailored specifically for LLM Agents. It seamlessly integrates with diverse data sources to ensure a superior, relevant research experience.

To begin, you need to obtain an API key on the [Tavily's developer dashboard](https://app.tavily.com/).

## Why Choose Tavily Research API?

1. **Purpose-Built**: Tailored just for LLM Agents, we ensure our features and results resonate with your unique needs. We take care of all the burden in searching, scraping, filtering and extracting information from online sources. All in a single API call!
2. **Versatility**: Beyond just fetching results, Tavily Research API offers precision. With customizable search depths, domain management, and parsing html content controls, you're in the driver's seat.
3. **Performance**: Committed to rapidity and efficiency, our API guarantees real-time outcomes without sidelining accuracy. Please note that we're just getting started, so performance may vary and improve over time.
4. **Integration-friendly**: We appreciate the essence of adaptability. That's why integrating our API with your existing setup is a breeze. You can choose our Python library or a simple API call or any of our supported partners such as [Langchain](https://python.langchain.com/docs/integrations/tools/tavily_search) and [LLamaIndex](https://llamahub.ai/l/tools-tavily).
5. **Transparent & Informative**: Our detailed documentation ensures you're never left in the dark. From setup basics to nuanced features, we've got you covered.

## Usage

This tool has a more extensive example usage documented in a Jupyter notebook [here](https://github.com/run-llama/llama_index/blob/main/llama-index-integrations/tools/llama-index-tools-tavily-research/examples/tavily.ipynb)

Here's an example usage of the TavilyToolSpec.

```python
from llama_index.tools.tavily_research import TavilyToolSpec
from llama_index.core.agent import FunctionCallingAgent
from llama_index.llms.openai import OpenAI

tavily_tool = TavilyToolSpec(
    api_key="your-key",
)
agent = FunctionCallingAgent.from_tools(
    tavily_tool.to_tool_list(),
    llm=OpenAI(model="gpt-4o"),
)

agent.chat("What happened in the latest Burning Man festival?")
```

`search`: Search for relevant dynamic data based on a query. Returns a list of urls and their relevant content.

This loader is designed to be used as a way to load data as a Tool in an Agent.
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-tavily-research/examples/tavily.ipynb (Size: 11.91 KB)

```
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd2b2eba-b7fd-4856-960f-f2cbadcc12af",
   "metadata": {},
   "source": [
    "# Building a Tavily Data Agent\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/llama-index-integrations/tools/llama-index-tools-tavily-research/examples/tavily.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "This tutorial walks through using the LLM tools provided by the [Tavily API](https://app.tavily.com/) to allow LLMs to easily search and retrieve relevant content from the Internet.\n",
    "\n",
    "To get started, you will need an [OpenAI api key](https://platform.openai.com/account/api-keys) and a [Tavily API key](https://app.tavily.com)\n",
    "\n",
    "We will import the relevant agents and tools and pass them our keys here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b2678c-09a5-4bf5-b45a-0543d2c100f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-tools-tavily-research llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c90f5ea-9ed0-43c5-a658-1a3d93382eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your openai key, if using openai\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab26eee-63ed-42ec-972a-419a9a5c1d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search\n"
     ]
    }
   ],
   "source": [
    "# Set up Tavily tool\n",
    "from llama_index.tools.tavily_research.base import TavilyToolSpec\n",
    "\n",
    "tavily_tool = TavilyToolSpec(\n",
    "    api_key=\"tvly-api-key\",\n",
    ")\n",
    "\n",
    "tavily_tool_list = tavily_tool.to_tool_list()\n",
    "for tool in tavily_tool_list:\n",
    "    print(tool.metadata.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8e3012-bab0-4e55-858a-e3721282552c",
   "metadata": {},
   "source": [
    "## Testing the Tavily search tool\n",
    "\n",
    "We've imported our OpenAI agent, set up the api key, and initialized our tool. Let's test out the tool before setting up our Agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64da618-b4ab-42d7-903d-f4eeb624f43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='47fad889-3ced-4a91-bc4b-8af9939c6535', embedding=None, metadata={'url': 'https://www.vox.com/culture/2023/9/6/23861675/burning-man-2023-mud-stranded-climate-change-playa-foot'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='879353cf1df00d8e20618086e44683d83ed17bf093524e2ef60ac111291d6094', text='Has Burning Man finally lost its glamour?\\n\\nGive\\nNewsletters\\nSite search\\nVox main menu\\nFiled under:\\nThe Burning Man flameout, explained\\nClimate change — and schadenfreude\\xa0— finally caught up to the survivalist cosplayers.\\nShare this story\\nShare\\n\\nSeptember 1, after most of the scheduled events and live performances were canceled due to the weather, Burning Man organizers closed routes in and out of the area, forcing attendees to stay behind\\n\\nShare\\nAll sharing options for:\\nThe Burning Man flameout, explained\\n\\nFor most of its existence, Burning Man, the week-long desert experience that’s part utopian performance, part survival quest, and part drug trip, has dodged serious cultural pushback, beyond eye', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e26008c2-508e-42bc-b7bb-5e75ba8d450b', embedding=None, metadata={'url': 'https://www.insider.com/photos-what-it-was-really-like-at-burning-man-2023-2023-9'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='612b38bc411db0a05acd7dc5e9cc335743e307427527e5cb5f25aeb57a8bbeb6', text='I\\'ve seen Burning Man 2023 called a \"total disaster\" and \"Fyre Festival 2.0.\" I\\'ve read stories about people \"clamoring to escape\" and leaving behind their cars and trash.\\n\\nMy first days at Burning Man featured plenty of parties.\\nBurning Man is known for its incredible art and music, and we spent the first few days exploring as much of it as possible.\\n\\nI went to Burning Man for the first time and it was one of the best weeks of my life.\\n\\nThe bulk of my incredible Burning Man experience was due to Android Oasis, once a six-person crew of friends that has since blossomed into a 70-person camp.\\n\\nI found myself dancing in everything from a towering pyramid to a fluffy cloud.\\nLife at Burning Man often felt like living in a video game —\\xa0or a fairy tale.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='83b1399d-b9d9-4f7c-9351-49666785e4a4', embedding=None, metadata={'url': 'https://www.insider.com/photos-what-it-was-really-like-at-burning-man-2023-2023-9'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='612b38bc411db0a05acd7dc5e9cc335743e307427527e5cb5f25aeb57a8bbeb6', text='I\\'ve seen Burning Man 2023 called a \"total disaster\" and \"Fyre Festival 2.0.\" I\\'ve read stories about people \"clamoring to escape\" and leaving behind their cars and trash.\\n\\nMy first days at Burning Man featured plenty of parties.\\nBurning Man is known for its incredible art and music, and we spent the first few days exploring as much of it as possible.\\n\\nI went to Burning Man for the first time and it was one of the best weeks of my life.\\n\\nThe bulk of my incredible Burning Man experience was due to Android Oasis, once a six-person crew of friends that has since blossomed into a 70-person camp.\\n\\nI found myself dancing in everything from a towering pyramid to a fluffy cloud.\\nLife at Burning Man often felt like living in a video game —\\xa0or a fairy tale.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tavily_tool.search(\"What happened in the latest Burning Man festival?\", max_results=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1210906d-87a7-466a-9712-1d17dba2c2ec",
   "metadata": {},
   "source": [
    "### Using the Search tool in an Agent\n",
    "\n",
    "We can create an agent with access to the Tavily search tool start testing it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d88c2ee-184a-4371-995b-a086b34db24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgent\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "agent = FunctionCallingAgent.from_tools(\n",
    "    tavily_tool_list,\n",
    "    llm=OpenAI(model=\"gpt-4o\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69a53fd-55c4-4e18-8fbe-6a29d5f3cef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Analysis of the Latest Burning Man Floods\n",
      "\n",
      "## Introduction\n",
      "The Burning Man festival, held annually in the Black Rock Desert of Nevada, is known for its unique art installations, music performances, and community spirit. However, in recent years, the festival has faced challenges due to extreme weather conditions, including floods. This analysis aims to provide a deep understanding of the latest Burning Man floods, their causes, impacts, and the response of the festival organizers.\n",
      "\n",
      "## Overview of the Latest Burning Man Floods\n",
      "According to a [news article](https://www.today.com/news/what-is-burning-man-flood-death-rcna103231), the latest Burning Man festival experienced heavy rains that resulted in muddy conditions, making it difficult for festivalgoers to leave the Nevada desert. The article mentions that tens of thousands of festival attendees were slowly making their way home as the muddy conditions made it nearly impossible to leave over the weekend. The festival organizers provided a \"Wet Playa Survival Guide\" with tips to those still on-site, indicating that the conditions were improving.\n",
      "\n",
      "## Causes of the Floods\n",
      "1. Heavy Rainfall: The primary cause of the floods at Burning Man was heavy rainfall. The article mentions that the festival experienced significant rain, leading to muddy conditions and making it challenging for attendees to navigate the desert.\n",
      "2. Flash Floods: The Black Rock Desert is prone to flash floods due to its flat topography and lack of vegetation. When rainwater accumulates faster than it can be absorbed or evaporated, it quickly forms torrents that flood the festival grounds.\n",
      "\n",
      "## Impacts of the Floods\n",
      "1. Transportation Challenges: The muddy conditions caused by the floods made it difficult for festivalgoers to leave the desert. This created transportation challenges and delays for tens of thousands of attendees.\n",
      "2. Disruption of Activities: The floods disrupted the planned activities and performances at Burning Man. The muddy conditions may have made it challenging for artists and performers to showcase their work, impacting the overall experience of the festival.\n",
      "3. Safety Risks: The muddy and slippery conditions posed safety risks to festival attendees. The article does not mention any specific incidents, but the increased risk of slips, falls, and injuries is a concern during such weather conditions.\n",
      "\n",
      "## Response of the Festival Organizers\n",
      "The article does not provide detailed information about the specific response of the festival organizers to the floods. However, it mentions that the festival organizers provided a \"Wet Playa Survival Guide\" with tips to those still on-site, indicating that they were actively addressing the challenges posed by the floods. It is likely that the organizers were monitoring the situation, providing updates, and taking necessary measures to ensure the safety and well-being of the attendees.\n",
      "\n",
      "## Conclusion\n",
      "The latest Burning Man floods, caused by heavy rainfall and resulting in muddy conditions, created challenges for festivalgoers and disrupted the planned activities at the event. The transportation difficulties, disruption of activities, and safety risks highlight the impact of extreme weather events on the festival. While the specific response of the festival organizers is not detailed in the available information, it can be assumed that they took measures to address the challenges and ensure the safety of the attendees. As extreme weather events become more frequent and intense, it is crucial for the festival organizers to continue evaluating and implementing strategies to mitigate the impact of floods and other weather-related challenges at Burning Man.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    agent.chat(\n",
    "        \"Write a deep analysis in markdown syntax about the latest burning man floods\"\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-index-caVs7DDe-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-tavily-research/llama_index/tools/tavily_research/BUILD (Size: 0.02 KB)

```
python_sources()
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-tavily-research/llama_index/tools/tavily_research/__init__.py (Size: 0.09 KB)

```
from llama_index.tools.tavily_research.base import TavilyToolSpec

__all__ = ["TavilyToolSpec"]
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-tavily-research/llama_index/tools/tavily_research/base.py (Size: 1.18 KB)

```
"""Tavily tool spec."""

from typing import List, Optional

from llama_index.core.schema import Document
from llama_index.core.tools.tool_spec.base import BaseToolSpec


class TavilyToolSpec(BaseToolSpec):
    """Tavily tool spec."""

    spec_functions = [
        "search",
    ]

    def __init__(self, api_key: str) -> None:
        """Initialize with parameters."""
        from tavily import TavilyClient

        self.client = TavilyClient(api_key=api_key)

    def search(self, query: str, max_results: Optional[int] = 6) -> List[Document]:
        """
        Run query through Tavily Search and return metadata.

        Args:
            query: The query to search for.
            max_results: The maximum number of results to return.

        Returns:
            results: A list of dictionaries containing the results:
                url: The url of the result.
                content: The content of the result.

        """
        response = self.client.search(
            query, max_results=max_results, search_depth="advanced"
        )
        return [
            Document(text=result["content"], extra_info={"url": result["url"]})
            for result in response["results"]
        ]
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-tavily-research/pyproject.toml (Size: 1.48 KB)

```
[build-system]
build-backend = "poetry.core.masonry.api"
requires = ["poetry-core"]

[tool.codespell]
check-filenames = true
check-hidden = true
skip = "*.csv,*.html,*.json,*.jsonl,*.pdf,*.txt,*.ipynb"

[tool.llamahub]
contains_example = false
import_path = "llama_index.tools.tavily_research"

[tool.llamahub.class_authors]
TavilyToolSpec = "rotemweiss57"

[tool.mypy]
disallow_untyped_defs = true
exclude = ["_static", "build", "examples", "notebooks", "venv"]
ignore_missing_imports = true
python_version = "3.8"

[tool.poetry]
authors = ["Your Name <you@example.com>"]
description = "llama-index tools tavily_research integration"
exclude = ["**/BUILD"]
license = "MIT"
maintainers = ["rotemweiss57"]
name = "llama-index-tools-tavily-research"
readme = "README.md"
version = "0.3.0"

[tool.poetry.dependencies]
python = ">=3.9,<4.0"
tavily-python = ">=0.2.4"
llama-index-core = "^0.12.0"

[tool.poetry.group.dev.dependencies]
ipython = "8.10.0"
jupyter = "^1.0.0"
mypy = "0.991"
pre-commit = "3.2.0"
pylint = "2.15.10"
pytest = "7.2.1"
pytest-mock = "3.11.1"
ruff = "0.0.292"
tree-sitter-languages = "^1.8.0"
types-Deprecated = ">=0.1.0"
types-PyYAML = "^6.0.12.12"
types-protobuf = "^4.24.0.4"
types-redis = "4.5.5.0"
types-requests = "2.28.11.8"
types-setuptools = "67.1.0.0"

[tool.poetry.group.dev.dependencies.black]
extras = ["jupyter"]
version = "<=23.9.1,>=23.7.0"

[tool.poetry.group.dev.dependencies.codespell]
extras = ["toml"]
version = ">=v2.2.6"

[[tool.poetry.packages]]
include = "llama_index/"
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-tavily-research/requirements.txt (Size: 0.02 KB)

```
tavily-python>=0.2.4
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-tavily-research/source.json (Size: 0.34 KB)

```
{
  "url": "https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/tools/llama-index-tools-tavily-research",
  "owner": "run-llama",
  "repo": "llama_index",
  "branch": "main",
  "path": "llama-index-integrations/tools/llama-index-tools-tavily-research",
  "content_type": "DIRECTORY",
  "download_time": "2025-02-25 01:17:14"
}
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-tavily-research/tests/BUILD (Size: 0.01 KB)

```
python_tests()
```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-tavily-research/tests/__init__.py (Size: 0.00 KB)

```

```

## File: run-llama_llama_index_llama-index-integrations_tools_llama-index-tools-tavily-research/tests/test_tools_tavily_research.py (Size: 0.27 KB)

```
from llama_index.core.tools.tool_spec.base import BaseToolSpec
from llama_index.tools.tavily_research import TavilyToolSpec


def test_class():
    names_of_base_classes = [b.__name__ for b in TavilyToolSpec.__mro__]
    assert BaseToolSpec.__name__ in names_of_base_classes
```

## File: ssiddhantsood_thalamus-backend-hackmit_toolkitutils_search_toolkit.py/search_toolkit.py (Size: 6.34 KB)

```
import requests
import nltk
from urllib.parse import quote_plus
from bs4 import BeautifulSoup
import arxiv
import trafilatura
import fitz 
import io
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer

def extract_pdf_text(pdf_url: str) -> str:
    """
    Download and extract text from a PDF URL.
    
    :param pdf_url: The URL of the PDF to download and extract text from
    :return: Extracted text or a failure message
    """
    try:
        # Download the PDF
        response = requests.get(pdf_url)
        response.raise_for_status()
        
        # Save the PDF to a temporary file
        with open("temp.pdf", "wb") as pdf_file:
            pdf_file.write(response.content)
        
        # Open the PDF with PyMuPDF
        with fitz.open("temp.pdf") as pdf_document:
            text = ""
            for page in pdf_document:
                text += page.get_text()  # Extract text from each page
        
        # Return the extracted text
        return text if text.strip() else "No text found in PDF"
    
    except Exception as e:
        return f"Error extracting PDF content: {str(e)}"

BRAVE_API_KEY = "BSAp319zErkYaMrxMq8kB92rgq7lENy"
def summarize_text(text: str, sentence_count: int = 3) -> str:
    """
    Summarize the given text using LsaSummarizer.
    
    :param text: The text to summarize
    :param sentence_count: Number of sentences to include in the summary
    :return: The summarized text
    """
    if not text.strip():
        return "No content to summarize"
    
    parser = PlaintextParser.from_string(text, Tokenizer("english"))
    summarizer = LsaSummarizer()
    summary = summarizer(parser.document, sentence_count)
    
    return ' '.join(str(sentence) for sentence in summary)

def arxiv_search_and_scrape(query: str, max_results: int = 5) -> list:
    """
    Search arXiv for papers matching the query and scrape full article information.
    
    :param query: The search query
    :param max_results: Maximum number of results to return (default 5)
    :return: A list of dictionaries containing the search results with full text
    """
    client = arxiv.Client()
    search = arxiv.Search(
        query=query,
        max_results=max_results,
        sort_by=arxiv.SortCriterion.Relevance
    )
    
    results = []
    for paper in client.results(search):
        paper_info = {
            "title": paper.title,
            "authors": ", ".join(author.name for author in paper.authors),
            "summary": paper.summary,
            "link": paper.pdf_url,
            "full_text": extract_pdf_text(paper.pdf_url)  # Extract PDF content
        }
        results.append(paper_info)
    
    return results

def brave_search_and_scrape(query: str, num_results: int = 5) -> list:
    """
    Perform a web search using the Brave Search API and scrape full article content.
    
    :param query: The search query
    :param num_results: Number of results to return (default 5)
    :return: A list of dictionaries containing the search results with full text
    """
    url = "https://api.search.brave.com/res/v1/web/search"
    headers = {
        "Accept": "application/json",
        "X-Subscription-Token": BRAVE_API_KEY
    }
    params = {
        "q": query,
        "count": num_results
    }
    
    response = requests.get(url, headers=headers, params=params)
    
    if response.status_code != 200:
        return [{"error": f"Unable to fetch results (Status code: {response.status_code})"}]
    
    data = response.json()
    web_results = data.get('web', {}).get('results', [])
    
    results = []
    for result in web_results:
        title = result.get('title', 'No title')
        description = result.get('description', 'No description available')
        url = result.get('url', 'No URL available')
        
        # Scrape the full text content
        full_text = scrape_web_content(url)
        summarized_text = summarize_text(full_text)  # Add summarization here
        
        result_info = {
            "title": title,
            "description": description,
            "url": url,
            "full_text": full_text,
            "summary_excerpt": summarized_text
        }
        results.append(result_info)
    
    return results

def scrape_web_content(url: str) -> str:
    """
    Scrape the main content of a web page.
    
    :param url: The URL of the web page to scrape
    :return: The extracted main content as a string
    """
    try:
        downloaded = trafilatura.fetch_url(url)
        text = trafilatura.extract(downloaded, include_links=False, include_images=False, include_tables=False)
        return text if text else "Failed to extract content"
    except Exception as e:
        return f"Error scraping content: {str(e)}"


def test_search_and_scrape_functions():
    print("Testing arXiv Search and Scrape:")
    print("-" * 50)
    
    query = "quantum computing"
    results = arxiv_search_and_scrape(query, max_results=2)
    for i, result in enumerate(results, 1):
        print(f"{i}. Title: {result['title']}")
        print(f"   Authors: {result['authors']}")
        print(f"   Summary: {result['summary'][:200]}...")
        print(f"   Link: {result['link']}")
        print(f"   Full Text: {result['full_text']}")
        print(f"   Summary Excerpt: {result['summary_excerpt']}")
        print()
    
    print("Testing Brave Search and Scrape:")
    print("-" * 50)
    
    query = "latest advancements in artificial intelligence"
    results = brave_search_and_scrape(query, num_results=2)
    for i, result in enumerate(results, 1):
        print(f"{i}. Title: {result['title']}")
        print(f"   Description: {result['description']}")
        print(f"   URL: {result['url']}")
        print(f"   Full Text (first 200 chars): {result['full_text'][200]}...")
        print(f"   Summary Excerpt: {result['summary_excerpt']}")
        print()

if __name__ == "__main__":
    #test_search_and_scrape_functions()
    query = "quantum computing"
    results = arxiv_search_and_scrape(query, max_results=2)
    
    for i, result in enumerate(results, 1):
        print(f"{i}. Title: {result['title']}")
        print(f"   Authors: {result['authors']}")
        print(f"   Summary: {result['summary'][:200]}...")
        print(f"   Link: {result['link']}")
        print(f"   Full Text (first 500 chars): {result['full_text']}...")
        print()
```

## File: ssiddhantsood_thalamus-backend-hackmit_toolkitutils_search_toolkit.py/source.json (Size: 0.30 KB)

```
{
  "url": "https://github.com/ssiddhantsood/thalamus-backend-hackmit/blob/main/toolkitutils/search_toolkit.py",
  "owner": "ssiddhantsood",
  "repo": "thalamus-backend-hackmit",
  "branch": "main",
  "path": "toolkitutils/search_toolkit.py",
  "content_type": "FILE",
  "download_time": "2025-02-25 01:17:22"
}
```

## File: steven-haddix_rezai/.gitignore (Size: 2.02 KB)

```
### Python template

.idea/
.vscode/
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
*.sqlite3
*.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# PEP 582; used by e.g. github.com/David-OConnor/pyflow
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/
```

## File: steven-haddix_rezai/README.md (Size: 4.35 KB)

```
# rezai

This project was generated using fastapi_template.

## Poetry

This project uses poetry. It's a modern dependency management
tool.

To run the project use this set of commands:

```bash
poetry install
poetry run python -m rezai
```

This will start the server on the configured host.

You can find swagger documentation at `/api/docs`.

You can read more about poetry here: https://python-poetry.org/

## Docker

You can start the project with docker using this command:

```bash
docker-compose -f deploy/docker-compose.yml --project-directory . up --build
```

If you want to develop in docker with autoreload add `-f deploy/docker-compose.dev.yml` to your docker command.
Like this:

```bash
docker-compose -f deploy/docker-compose.yml -f deploy/docker-compose.dev.yml --project-directory . up --build
```

This command exposes the web application on port 8000, mounts current directory and enables autoreload.

But you have to rebuild image every time you modify `poetry.lock` or `pyproject.toml` with this command:

```bash
docker-compose -f deploy/docker-compose.yml --project-directory . build
```

## Project structure

```bash
$ tree "rezai"
rezai
├── conftest.py  # Fixtures for all tests.
├── db  # module contains db configurations
│   ├── dao  # Data Access Objects. Contains different classes to interact with database.
│   └── models  # Package contains different models for ORMs.
├── __main__.py  # Startup script. Starts uvicorn.
├── services  # Package for different external services such as rabbit or redis etc.
├── settings.py  # Main configuration settings for project.
├── static  # Static content.
├── tests  # Tests for project.
└── web  # Package contains web server. Handlers, startup config.
    ├── api  # Package with all handlers.
    │   └── router.py  # Main router.
    ├── application.py  # FastAPI application configuration.
    └── lifetime.py  # Contains actions to perform on startup and shutdown.
```

## Configuration

This application can be configured with environment variables.

You can create `.env` file in the root directory and place all
environment variables here.

All environment variables should start with "REZAI_" prefix.

For example if you see in your "rezai/settings.py" a variable named like
`random_parameter`, you should provide the "REZAI_RANDOM_PARAMETER"
variable to configure the value. This behaviour can be changed by overriding `env_prefix` property
in `rezai.settings.Settings.Config`.

An example of .env file:
```bash
REZAI_RELOAD="True"
REZAI_PORT="8000"
REZAI_ENVIRONMENT="dev"
```

You can read more about BaseSettings class here: https://pydantic-docs.helpmanual.io/usage/settings/

## Pre-commit

To install pre-commit simply run inside the shell:
```bash
pre-commit install
```

pre-commit is very useful to check your code before publishing it.
It's configured using .pre-commit-config.yaml file.

By default it runs:
* black (formats your code);
* mypy (validates types);
* isort (sorts imports in all files);
* flake8 (spots possible bugs);


You can read more about pre-commit here: https://pre-commit.com/

## Migrations

If you want to migrate your database, you should run following commands:
```bash
# To run all migrations until the migration with revision_id.
alembic upgrade "<revision_id>"

# To perform all pending migrations.
alembic upgrade "head"
```

### Reverting migrations

If you want to revert migrations, you should run:
```bash
# revert all migrations up to: revision_id.
alembic downgrade <revision_id>

# Revert everything.
 alembic downgrade base
```

### Migration generation

To generate migrations you should run:
```bash
# For automatic change detection.
alembic revision --autogenerate

# For empty file generation.
alembic revision
```


## Running tests

If you want to run it in docker, simply run:

```bash
docker-compose -f deploy/docker-compose.yml -f deploy/docker-compose.dev.yml --project-directory . run --build --rm api pytest -vv .
docker-compose -f deploy/docker-compose.yml -f deploy/docker-compose.dev.yml --project-directory . down
```

For running tests on your local machine.
1. you need to start a database.

I prefer doing it with docker:
```
docker run -p "5432:5432" -e "POSTGRES_PASSWORD=rezai" -e "POSTGRES_USER=rezai" -e "POSTGRES_DB=rezai" postgres:13.8-bullseye
```


2. Run the pytest.
```bash
pytest -vv .
```
```

## File: steven-haddix_rezai/pyproject.toml (Size: 2.57 KB)

```
[tool.poetry]
name = "rezai"
version = "0.1.0"
description = ""
authors = [

]
maintainers = [

]
readme = "README.md"

[tool.poetry.dependencies]
python = ">=3.9,<3.9.7 || >3.9.7,<4.0"
fastapi = "^0.100.0"
uvicorn = { version = "^0.22.0", extras = ["standard"] }
gunicorn = "^21.2.0"
pydantic = "^2"
pydantic-settings = "^2"
yarl = "^1.9.2"
ujson = "^5.8.0"
SQLAlchemy = {version = "^2.0.18", extras = ["asyncio"]}
alembic = "^1.11.1"
asyncpg = {version = "^0.28.0", extras = ["sa"]}
redis = {version = "^4.6.0", extras = ["hiredis"]}
aiofiles = "^23.1.0"
httptools = "^0.6.0"
loguru = "^0.7.0"
langchain-anthropic = "^0.1.19"
openai = "^1.35.13"
langchain-core = "^0.2.13"
langgraph = "^0.1.7"
twilio = "^9.2.3"
tavily-python = "^0.3.3"
langchain-community = "^0.2.7"
importlib_metadata = "^4.0.0"
setuptools = "^71.0.1"
streamlit = "^1.37.0"


[tool.poetry.dev-dependencies]
pytest = "^7.2.1"
flake8 = "~7"
isort = "^5.11.4"
pre-commit = "^3.0.1"
wemake-python-styleguide = "^0.19.0"
black = "^22.12.0"
autoflake = "^1.6.1"
pytest-cov = "^4.0.0"
anyio = "^3.6.2"
pytest-env = "^0.8.1"
fakeredis = "^2.5.0"
httpx = "^0.23.3"
setuptools = "^71.0.1"

[tool.poetry.group.dev.dependencies]
pyright = "^1.1.374"

[tool.isort]
profile = "black"
multi_line_output = 3
src_paths = ["rezai",]

[tool.mypy]
strict = true
ignore_missing_imports = true
allow_subclassing_any = true
allow_untyped_calls = true
pretty = true
show_error_codes = true
implicit_reexport = true
allow_untyped_decorators = true
warn_unused_ignores = false
warn_return_any = false
namespace_packages = true

# Remove this and add `types-redis`
# when the issue https://github.com/python/typeshed/issues/8242 is resolved.
[[tool.mypy.overrides]]
module = [
    'redis.asyncio'
]
ignore_missing_imports = true

[tool.pytest.ini_options]
filterwarnings = [
    "error",
    "ignore::DeprecationWarning",
    "ignore:.*unclosed.*:ResourceWarning",
]
env = [
    "REZAI_ENVIRONMENT=pytest",
    "REZAI_DB_BASE=rezai_test",
]

[fastapi-template.options]
project_name = "rezai"
api_type = "rest"
enable_redis = "True"
enable_rmq = "None"
ci_type = "github"
enable_migrations = "True"
enable_taskiq = "None"
enable_kube = "None"
kube_name = "rezai"
enable_routers = "True"
enable_kafka = "None"
enable_loguru = "True"
traefik_labels = "None"
add_dummy = "True"
orm = "sqlalchemy"
self_hosted_swagger = "True"
prometheus_enabled = "None"
sentry_enabled = "None"
otlp_enabled = "None"
pydanticv1 = "None"
gunicorn = "True"
add_users = "None"
cookie_auth = "None"
jwt_auth = "None"

[build-system]
requires = ["poetry-core>=1.0.0"]
build-backend = "poetry.core.masonry.api"
```

## File: steven-haddix_rezai/source.json (Size: 0.22 KB)

```
{
  "url": "https://github.com/steven-haddix/rezai/tree/master",
  "owner": "steven-haddix",
  "repo": "rezai",
  "branch": "master",
  "path": "",
  "content_type": "REPOSITORY",
  "download_time": "2025-02-25 01:17:23"
}
```

## File: steven-haddix_rezai_rezai_services_youcom_service.py/service.py (Size: 1.21 KB)

```
# rezai/services/youcom/service.py
from typing import Any

import httpx

from rezai.settings import settings


class YouComService:
    def __init__(self, api_key: str = settings.youcom_api_key):
        self.api_key = api_key
        self.base_url = "https://api.ydc-index.io"

    async def query_web_llm(self, query: str) -> Any:
        headers = {"X-API-Key": self.api_key}
        params = {"query": query}
        async with httpx.AsyncClient() as client:
            response = await client.get(
                f"{self.base_url}/rag",
                params=params,
                headers=headers,
                timeout=30,
            )
            response.raise_for_status()
            data = response.json()
            return data

    async def get_ai_snippets_for_query(self, query: str) -> Any:
        headers = {"X-API-Key": self.api_key}
        params = {"query": query}
        async with httpx.AsyncClient() as client:
            response = await client.get(
                f"{self.base_url}/search",
                params=params,
                headers=headers,
                timeout=30,
            )
            response.raise_for_status()
            data = response.json()
            return data
```

## File: steven-haddix_rezai_rezai_services_youcom_service.py/source.json (Size: 0.27 KB)

```
{
  "url": "https://github.com/steven-haddix/rezai/blob/main/rezai/services/youcom/service.py",
  "owner": "steven-haddix",
  "repo": "rezai",
  "branch": "main",
  "path": "rezai/services/youcom/service.py",
  "content_type": "FILE",
  "download_time": "2025-02-25 01:17:28"
}
```

## File: tom-doerr_perplexity_search/.gitignore (Size: 3.07 KB)

```
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/latest/usage/project/#working-with-version-control
.pdm.toml
.pdm-python
.pdm-build/

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/
.aider*
```

## File: tom-doerr_perplexity_search/LICENSE (Size: 1.04 KB)

```
MIT License

Copyright (c) 2024 Tom Doerr

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

## File: tom-doerr_perplexity_search/README.md (Size: 5.68 KB)

```
<div align="center">

# 🔍 Perplexity Search

[![Python 3.x](https://img.shields.io/badge/python-3.x-blue?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green?style=for-the-badge&logo=opensourceinitiative&logoColor=white)](https://opensource.org/licenses/MIT)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen?style=for-the-badge&logo=github&logoColor=white)](https://github.com/tom-doerr/perplexity_search/pulls)
[![GitHub Issues](https://img.shields.io/github/issues/tom-doerr/perplexity_search?style=for-the-badge&logo=github&logoColor=white)](https://github.com/tom-doerr/perplexity_search/issues)
[![GitHub Stars](https://img.shields.io/github/stars/tom-doerr/perplexity_search?style=for-the-badge&logo=github&logoColor=white)](https://github.com/tom-doerr/perplexity_search/stargazers)

A powerful Python tool for performing technical searches using the Perplexity API, optimized for retrieving precise facts, code examples, and numerical data.

<img src="assets/images/screenshot_2.png" alt="Perplexity Search Demo" width="600"/>

</div>

<div align="center">

## 📋 Table of Contents

[Overview](#overview) •
[Features](#features) •
[Installation](#installation) •
[Usage](#usage) •
[Configuration](#configuration) •
[Requirements](#requirements) •
[Error Handling](#error-handling) •
[Contributing](#contributing) •
[FAQ](#faq) •
[License](#license)

</div>

## 🌟 Overview

Perplexity Search is a command-line tool and Python library that leverages the power of Perplexity AI to provide accurate, technical search results. It's designed for developers, researchers, and technical users who need quick access to precise information, code examples, and technical documentation. It also includes an interactive mode for multi-turn conversations.

## ✨ Features

- **Interactive Mode:** Engage in a conversational interface where you can ask multiple queries in sequence.
- **Conversation Context:** Maintain context across multiple turns in interactive mode.
- **Markdown Output:** Save conversation history to a markdown file.
- **Perform searches using different LLaMA models (small, large, huge)**
- **Configurable API key support via environment variable or direct input**
- **Command-line interface for easy usage**
- **Focused on retrieving technical information with code examples**
- **Returns responses formatted in markdown**
- **Optimized for factual and numerical data**
- **Debug logging**

## Installation

```bash
pip install plexsearch
```

## Usage

### As a Python Module

```python
from perplexity_search import perform_search

# Using environment variable for API key
result = perform_search("What is Python's time complexity for list operations?")

# Or passing API key directly
result = perform_search("What are the differences between Python 3.11 and 3.12?", api_key="your-api-key")

# Specify a different model
result = perform_search("Show me example code for Python async/await", model="llama-3.1-sonar-huge-128k-online")
```

### Command Line Interface

#### Interactive Mode

To enter interactive mode, simply run the command without any query arguments:

```bash
plexsearch
```

In interactive mode, you can type your queries one by one. Type `exit` or press `Ctrl-D` to quit the interactive session.

```bash
# Basic search
plexsearch "What is Python's time complexity for list operations?"

# Specify model
plexsearch --model llama-3.1-sonar-huge-128k-online "What are the differences between Python 3.11 and 3.12?"

# Use specific API key
plexsearch --api-key your-api-key "Show me example code for Python async/await"

# Multi-word queries work naturally
plexsearch tell me about frogs

# Disable streaming output
plexsearch --no-stream "tell me about frogs"

# Show numbered citations at the bottom of the response
plexsearch --citations "tell me about Python's GIL"

Note: Streaming is automatically disabled when running inside Aider to prevent
filling up the context window.
```

## Configuration

### API Key

Set your Perplexity API key in one of these ways:
1. **Environment variable:**
   ```bash
   export PERPLEXITY_API_KEY=your-api-key
   # Or add to your ~/.bashrc or ~/.zshrc for persistence
   echo 'export PERPLEXITY_API_KEY=your-api-key' >> ~/.bashrc
   ```
2. **Pass directly in code or CLI:** `--api-key your-api-key`

### Available Models

The following models can be specified using the `--model` parameter:

- `llama-3.1-sonar-small-128k-online` (Faster, lighter model)
- `llama-3.1-sonar-large-128k-online` (Default, balanced model)
- `llama-3.1-sonar-huge-128k-online` (Most capable model)

### Conversation Logging

You can log your conversation to a file using the `--log-file` parameter.

### Markdown Output

You can save your conversation to a markdown file using the `--markdown-file` parameter.

## Requirements

- **Python 3.x**
- **requests library**
- **rich library**
- **feedparser library**
- **Perplexity API key** (obtain from [Perplexity API](https://docs.perplexity.ai/))

## Error Handling

The tool includes error handling for:
- **Missing API keys**
- **Invalid API responses**
- **Network issues**
- **Invalid model selections**

## Contributing

We welcome contributions! Please see our [CONTRIBUTING.md](CONTRIBUTING.md) for more details on how to contribute to this project. Check our [CHANGELOG.md](CHANGELOG.md) for recent updates and changes.

## FAQ

**Q:** How do I get an API key for Perplexity?

**A:** You can obtain an API key by signing up on the [Perplexity API](https://docs.perplexity.ai/) website.

**Q:** What models are available for search?

**A:** The available models are `small`, `large`, and `huge`.

## License

MIT License - see the [LICENSE](LICENSE) file for details
```

## File: tom-doerr_perplexity_search/pyproject.toml (Size: 1.46 KB)

```
[tool.poetry]
name = "plexsearch"
version = "0.2.2"
description = "A powerful Python tool for performing technical searches using the Perplexity API"
authors = ["Tom Doerr"]
license = "MIT"
readme = "README.md"
repository = "https://github.com/tom-doerr/perplexity_search"
documentation = "https://github.com/tom-doerr/perplexity_search#readme"
packages = [{include = "plexsearch"}]
keywords = ["search", "perplexity", "ai", "llm"]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.7",
    "Programming Language :: Python :: 3.8",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
]

[tool.poetry.dependencies]
python = ">=3.7"
requests = "^2.25.0"
rich = "^13.7.0"
feedparser = "^6.0.0"
packaging = "^23.0"

[tool.poetry.group.dev.dependencies]
pytest = "^7.4.0"

[tool.poetry.scripts]
plexsearch = "plexsearch.core:main"

[tool.poetry.urls]
"Homepage" = "https://github.com/tom-doerr/perplexity_search"
"Bug Tracker" = "https://github.com/tom-doerr/perplexity_search/issues"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

[tool.pytest.ini_options]
markers = [
    "integration: marks tests as integration tests (deselect with '-m \"not integration\"')"
]
```

## File: tom-doerr_perplexity_search/requirements.txt (Size: 0.05 KB)

```
feedparser
pytest-mock>=3.0.0
pytest-mock>=3.0.0
```

## File: tom-doerr_perplexity_search/source.json (Size: 0.23 KB)

```
{
  "url": "https://github.com/tom-doerr/perplexity_search/tree/master",
  "owner": "tom-doerr",
  "repo": "perplexity_search",
  "branch": "master",
  "path": "",
  "content_type": "REPOSITORY",
  "download_time": "2025-02-25 01:17:28"
}
```
```

